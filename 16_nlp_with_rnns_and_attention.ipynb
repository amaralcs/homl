{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Shakesperian Text using a Character RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start exploring NLP by creating an RNN that takes in a sequence of characters and tries to predict the next character in the sequence. Check this [cool article](https://karpathy.github.io/2015/05/21/rnn-effectiveness/) that contains examples (including Algebraic Geometry)!\n",
    "\n",
    "We'll train our model on Shakespeare's text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "shakespeare_url = 'https://homl.info/shakespeare'\n",
    "filepath = keras.utils.get_file('shakespeare.txt', shakespeare_url)\n",
    "with open(filepath) as f:\n",
    "    shakespeare_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'First Citizen:\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shakespeare_text[:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To process the text, we'll encode each character as an integer by using Keras' ```Tokenizer``` class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = keras.preprocessing.text.Tokenizer(char_level=True)\n",
    "tokenizer.fit_on_texts(shakespeare_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can give the tokenizer a string and get back its encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[20, 6, 9, 8, 3, 1, 19, 6, 3, 6, 36, 2, 10, 24, 11]]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.texts_to_sequences([shakespeare_text[:15]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_id = len(tokenizer.word_index)  # number of distinct characters\n",
    "max_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1115394"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# total number of characters\n",
    "dataset_size = tokenizer.document_count\n",
    "dataset_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encode the text (subtracting 1 to get ranges from 0 to 38)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "[encoded] = np.array(tokenizer.texts_to_sequences([shakespeare_text])) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([19,  5,  8, ..., 20, 26, 10])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting a sequential Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to split into training, validation and test set. But how we do it with characters? We can't just shuffle all the characters in the text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With time series, we gnerally split on time: for example if we might take years 2000 to 2012 for training, 2013 to 2015 for validation and 2016 to 2018 for test.\n",
    "\n",
    "In some other case we can split among other dimensions such as an industry type or country.\n",
    "\n",
    "Splitting accross time is safe, however it implicitly assumes that patterns learned in the past will still exist in the future. I.e. we assume the time series is *stationary*. To make sure the time series is sufficiently stationary, we can plot the model's error on the validation set across time: if the model performes better on the first part of the validation set than on the last part, then the time series might not be stationary enough. Training the model on a shorter time span might be better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the Shakespeare example  we'll take the first 90% of the text for the training set keeping the rest for validation and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = dataset_size *90 // 100\n",
    "dataset = tf.data.Dataset.from_tensor_slices(encoded[:train_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chopping data into windows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use ```dataset.window``` to convert the one long instance we have into many smaller windows of text. Every instance in training will be a fairly short substring of the full text and the RNN will be unrolled over the length of the substrings. This is called **truncated backpropagation through time**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 100 # tune-able parameter\n",
    "window_length = n_steps + 1 # target will be the input shifted 1 character ahead\n",
    "dataset = dataset.window(window_length, shift=1, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using ```shift=1``` we create overlapping windows: the first window will contain characters 0 to 100, the second 1 to 101 and so on...\n",
    "\n",
    "```drop_remainder``` ensures all windows are 101 characters long and we don't have to do any padding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```window()``` creates a dataset containing windows, each of which is also a dataset. It is a *nested dataset*. This is useful when we want to transform each window by calling dataset methods but it cannot be used directly for training. The model expects tensors, not datasets. \n",
    "\n",
    "The ```flat_map``` dataset converts a nested dataset into a *flat dataset* (i.e. without nesting). It also takes a function as an argument which allows you to transform each dataset _before_ flattening.\n",
    "\n",
    "For example passing ```lambda ds: ds.batch(2)``` to ```flat_map``` the dataset $\\{\\{1, 2\\}, \\{3,4, 5,6\\}\\}$ becomes $\\{[1,2], [3,4], [5,6]\\}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.flat_map(lambda window: window.batch(window_length))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By calling passing ```window_length``` to batch, we get a single tensor for each batch (all windows have that length). That is, the dataset contains windows of 101 characters each. Now we can shuffle the tensors and separate inputs from targets. See Figure 16-1 for a pictorial representation of the process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "dataset = dataset.shuffle(10000).batch(batch_size)\n",
    "dataset = dataset.map(lambda windows: (windows[:, :-1], windows[:, 1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[11 22 26 ... 11 14  7]\n",
      " [ 1 27  7 ... 29  0 16]\n",
      " [10  8  3 ... 10 14  4]\n",
      " ...\n",
      " [11 12  0 ...  9 29 10]\n",
      " [ 1  0 16 ... 13  8  2]\n",
      " [18  3 13 ... 16  3 13]], shape=(32, 100), dtype=int64)\n",
      "tf.Tensor(\n",
      "[[22 26  0 ... 14  7  0]\n",
      " [27  7  0 ...  0 16  6]\n",
      " [ 8  3 14 ... 14  4  8]\n",
      " ...\n",
      " [12  0 21 ... 29 10 10]\n",
      " [ 0 16  6 ...  8  2 17]\n",
      " [ 3 13  9 ...  3 13 11]], shape=(32, 100), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "for X, y in dataset.take(1):\n",
    "    print(X)\n",
    "    print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll do one-hot-encoding since there are fairly few features (only 39 characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth=max_id), Y_batch))\n",
    "dataset = dataset.prefetch(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bulding and Training the Char-RNN model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll create a model with GRU units that are fed into a Time-distributed Dense layer of 39 unites (max_id), since we want to output character probabilities for each possible character"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Running the code below takes a good while..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "31368/31368 [==============================] - 7643s 244ms/step - loss: 1.4618\n",
      "Epoch 2/3\n",
      "31368/31368 [==============================] - 7633s 243ms/step - loss: 1.3606\n",
      "Epoch 3/3\n",
      "31368/31368 [==============================] - 7425s 237ms/step - loss: 1.3407\n"
     ]
    }
   ],
   "source": [
    "# model = keras.models.Sequential([\n",
    "#     keras.layers.GRU(128, return_sequences=True, input_shape=[None, max_id],\n",
    "#                      dropout=0.2, recurrent_dropout=0.2),\n",
    "#     keras.layers.GRU(128, return_sequences=True,\n",
    "#                      dropout=0.2, recurrent_dropout=0.2),\n",
    "#     keras.layers.TimeDistributed(keras.layers.Dense(max_id, activation='softmax'))\n",
    "# ])\n",
    "# model.compile(loss='sparse_categorical_crossentropy', optimizer='adam')\n",
    "# history = model.fit(dataset, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('saved_models/16_nlp_with_rnns/shakespeare.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model('saved_models/16_nlp_with_rnns/shakespeare.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using the Char-RNN model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To pass some inputs to our model, we need to apply the same preprocessing we did for training it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(texts):\n",
    "    X = np.array(tokenizer.texts_to_sequences(texts)) - 1\n",
    "    return tf.one_hot(X, max_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And a test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2, 12,  1,  6, 10,  2,  1, 16,  4, 14]])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new = preprocess(['How are yo'])\n",
    "y_pred = np.argmax(model.predict(X_new), axis=-1) + 1 # Account for the 1 we removed during preprocessing\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model generate one predicion for each character in the input. Since we're only interested in the prediction for the last character we simply pick up the last prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'u'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.sequences_to_texts(y_pred)[0][-1] # 1st sentence, last char"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating new text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a model, we could start using it by feeding in some text, generate a prediction, amend the text, feed it in, get a new prediction, etc...\n",
    "\n",
    "While this approach does work, it often leads to the same words being predicted over and over again. Instead, we can sample from the distribution of possible characters using ```tf.random.categorical()```, this will generate more interesting text.\n",
    "\n",
    "```categorical``` samples random class indices, given class log probabilities. We can also introduce a notion of *temperature*, which will control how wild our model will deviate from the logits. We divide the logits by the temperature, when temperature is close to 0 it will favour high probability characters, when it is close to 1, it gives characters an equal probability.\n",
    "\n",
    "We'll define the ```next_char``` function to do this for us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_char(text, model, temperature=1):\n",
    "    X_new = preprocess([text])\n",
    "    y_proba = model.predict(X_new)[0, -1:, :]\n",
    "    rescaled_logits = tf.math.log(y_proba) / temperature\n",
    "    char_id = tf.random.categorical(rescaled_logits, num_samples=1) + 1\n",
    "    return tokenizer.sequences_to_texts(char_id.numpy())[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And a helper function that will generate the text and fill it in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete_text(text, model, n_chars=50, temperature=1):\n",
    "    for _ in range(n_chars):\n",
    "        text+= next_char(text, model, temperature)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the raiment servant,\n",
      "that she is a rest daughter wh\n"
     ]
    }
   ],
   "source": [
    "print(complete_text('t', model, temperature=0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test this gate,\n",
      "and what you be gone, 'tis not whom th\n"
     ]
    }
   ],
   "source": [
    "print(complete_text('test', model, temperature=0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adam, if you, understand me! i am all begot.\n",
      "\n",
      "bianca:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(complete_text('adam', model, temperature=0.8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "urst:\n",
      "and she's better with mind.\n",
      "\n",
      "petruchio:\n",
      "sir, \n"
     ]
    }
   ],
   "source": [
    "print(complete_text('u', model, temperature=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k exock, read up grew i\n",
      "schreadt, upon, knavoliut u\n"
     ]
    }
   ],
   "source": [
    "print(complete_text('k', model, temperature=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get better text we can add more layers, train for longer, add regularization. The limitation of this model is that it is incapable of learning patterns longer than n_steps=100 characters. Stateful RNNs can help with that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stateful RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RNNs discussed so far only used hidden states at each epoch. If we preserve the state after the end of each step and use that as the starting point for the next step, the RNN willl learn long-term patterns despite only backpropagating through short sequences. This is called a *stateful RNN*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It has a catch though: training a stateful RNN only makes sense if each input sequence in a batch starts exactly where the corresponding sequence in the previous batch left off.\n",
    "\n",
    "Instead of using shuffled data, we will use sequential, non-overlapping sequences. To do this we'll create the dataset using ```shift=n_steps``` and naturally we won't ```shuffle``` the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batching is where things get a bit harder. If we were to use ```batch(32)``` then the first batch would contain windows 1 through 32 and the next batch would be 33 to 64, so the first window of each batch are not consecutive. One approach to this is to use 'batches' containing a single window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices(encoded[:train_size])\n",
    "dataset = dataset.window(window_length, shift=n_steps, drop_remainder=True)\n",
    "dataset = dataset.flat_map(lambda window: window.batch(window_length))\n",
    "dataset = dataset.batch(1)\n",
    "dataset = dataset.map(lambda windows: (windows[:, :-1], windows[:, 1:]))\n",
    "dataset = dataset.map(\n",
    "    lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth=max_id), Y_batch))\n",
    "dataset = dataset.prefetch(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another approach would be to divide the corpus into 32 texts of equal length, create one dataset of consecutive input sequences for them and then use ```tf.data.Dataset.zip(dataset).map(lambda *windows: tf.stack(windows))``` to create proper batches, where the nth input sequence in a batch start off exactly where the nth input sequence ended in the previous batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "encoded_parts = np.array_split(encoded[:train_size], batch_size)\n",
    "datasets = []\n",
    "for encoded_part in encoded_parts:\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(encoded_part)\n",
    "    dataset = dataset.window(window_length, shift=n_steps, drop_remainder=True)\n",
    "    dataset = dataset.flat_map(lambda window: window.batch(window_length))\n",
    "    datasets.append(dataset)\n",
    "dataset = tf.data.Dataset.zip(tuple(datasets)).map(lambda *windows: tf.stack(windows))\n",
    "dataset = dataset.map(lambda windows: (windows[:, :-1], windows[:, 1:]))\n",
    "dataset = dataset.map(\n",
    "    lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth=max_id), Y_batch))\n",
    "dataset = dataset.prefetch(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to create the RNN we must set its ```stateful``` parameter as wel as specify the batch_input_shape for the first layer. We leave the second dimension unspecified since the inputs can have any length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.GRU(128, return_sequences=True, stateful=True,\n",
    "                     dropout=0.2, recurrent_dropout=0.2,\n",
    "                     batch_input_shape=[batch_size, None, max_id]),\n",
    "    keras.layers.GRU(128, return_sequences=True, stateful=True,\n",
    "                     dropout=0.2, recurrent_dropout=0.2),\n",
    "    keras.layers.TimeDistributed(keras.layers.Dense(max_id, activation='softmax'))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to add a little callback to ensure we reset the states at the end of each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResetStatesCallback(keras.callbacks.Callback):\n",
    "    def on_epoch_begin(self, epoch, logs):\n",
    "        self.model.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "313/313 [==============================] - 63s 200ms/step - loss: 1.6431\n",
      "Epoch 2/50\n",
      "313/313 [==============================] - 62s 199ms/step - loss: 1.6372\n",
      "Epoch 3/50\n",
      "313/313 [==============================] - 63s 201ms/step - loss: 1.6314\n",
      "Epoch 4/50\n",
      "313/313 [==============================] - 63s 202ms/step - loss: 1.6261\n",
      "Epoch 5/50\n",
      "313/313 [==============================] - 63s 202ms/step - loss: 1.6208\n",
      "Epoch 6/50\n",
      "313/313 [==============================] - 63s 202ms/step - loss: 1.6164\n",
      "Epoch 7/50\n",
      "313/313 [==============================] - 63s 202ms/step - loss: 1.6123\n",
      "Epoch 8/50\n",
      "313/313 [==============================] - 63s 202ms/step - loss: 1.6097\n",
      "Epoch 9/50\n",
      "313/313 [==============================] - 66s 211ms/step - loss: 1.6059\n",
      "Epoch 10/50\n",
      "313/313 [==============================] - 65s 209ms/step - loss: 1.6021\n",
      "Epoch 11/50\n",
      "313/313 [==============================] - 64s 203ms/step - loss: 1.5980\n",
      "Epoch 12/50\n",
      "313/313 [==============================] - 64s 203ms/step - loss: 1.5962\n",
      "Epoch 13/50\n",
      "313/313 [==============================] - 64s 203ms/step - loss: 1.5931\n",
      "Epoch 14/50\n",
      "313/313 [==============================] - 64s 205ms/step - loss: 1.5902\n",
      "Epoch 15/50\n",
      "313/313 [==============================] - 64s 204ms/step - loss: 1.5876\n",
      "Epoch 16/50\n",
      "313/313 [==============================] - 64s 205ms/step - loss: 1.5848\n",
      "Epoch 17/50\n",
      "313/313 [==============================] - 64s 204ms/step - loss: 1.5835\n",
      "Epoch 18/50\n",
      "313/313 [==============================] - 65s 208ms/step - loss: 1.5805\n",
      "Epoch 19/50\n",
      "313/313 [==============================] - 58s 186ms/step - loss: 1.5796\n",
      "Epoch 20/50\n",
      "313/313 [==============================] - 50s 161ms/step - loss: 1.5775\n",
      "Epoch 21/50\n",
      "313/313 [==============================] - 50s 161ms/step - loss: 1.5750\n",
      "Epoch 22/50\n",
      "313/313 [==============================] - 50s 161ms/step - loss: 1.5729\n",
      "Epoch 23/50\n",
      "313/313 [==============================] - 51s 161ms/step - loss: 1.5718\n",
      "Epoch 24/50\n",
      "313/313 [==============================] - 51s 162ms/step - loss: 1.5691\n",
      "Epoch 25/50\n",
      "313/313 [==============================] - 51s 163ms/step - loss: 1.5666\n",
      "Epoch 26/50\n",
      "313/313 [==============================] - 51s 162ms/step - loss: 1.5657\n",
      "Epoch 27/50\n",
      "313/313 [==============================] - 52s 165ms/step - loss: 1.5631\n",
      "Epoch 28/50\n",
      "313/313 [==============================] - 50s 160ms/step - loss: 1.5629\n",
      "Epoch 29/50\n",
      "313/313 [==============================] - 50s 161ms/step - loss: 1.5606\n",
      "Epoch 30/50\n",
      "313/313 [==============================] - 50s 161ms/step - loss: 1.5603\n",
      "Epoch 31/50\n",
      "313/313 [==============================] - 50s 161ms/step - loss: 1.5589\n",
      "Epoch 32/50\n",
      "313/313 [==============================] - 50s 161ms/step - loss: 1.5570\n",
      "Epoch 33/50\n",
      "313/313 [==============================] - 50s 161ms/step - loss: 1.5555\n",
      "Epoch 34/50\n",
      "313/313 [==============================] - 50s 161ms/step - loss: 1.5547\n",
      "Epoch 35/50\n",
      "313/313 [==============================] - 50s 160ms/step - loss: 1.5531\n",
      "Epoch 36/50\n",
      "313/313 [==============================] - 50s 160ms/step - loss: 1.5523\n",
      "Epoch 37/50\n",
      "313/313 [==============================] - 51s 162ms/step - loss: 1.5520\n",
      "Epoch 38/50\n",
      "313/313 [==============================] - 50s 161ms/step - loss: 1.5500\n",
      "Epoch 39/50\n",
      "313/313 [==============================] - 50s 160ms/step - loss: 1.5481\n",
      "Epoch 40/50\n",
      "313/313 [==============================] - 50s 161ms/step - loss: 1.5474\n",
      "Epoch 41/50\n",
      "313/313 [==============================] - 50s 161ms/step - loss: 1.5450\n",
      "Epoch 42/50\n",
      "313/313 [==============================] - 51s 162ms/step - loss: 1.5463\n",
      "Epoch 43/50\n",
      "313/313 [==============================] - 50s 161ms/step - loss: 1.5436\n",
      "Epoch 44/50\n",
      "313/313 [==============================] - 50s 161ms/step - loss: 1.5421\n",
      "Epoch 45/50\n",
      "313/313 [==============================] - 50s 161ms/step - loss: 1.5416\n",
      "Epoch 46/50\n",
      "313/313 [==============================] - 50s 160ms/step - loss: 1.5397\n",
      "Epoch 47/50\n",
      "313/313 [==============================] - 50s 161ms/step - loss: 1.5398\n",
      "Epoch 48/50\n",
      "313/313 [==============================] - 52s 165ms/step - loss: 1.5389\n",
      "Epoch 49/50\n",
      "313/313 [==============================] - 51s 163ms/step - loss: 1.5381\n",
      "Epoch 50/50\n",
      "313/313 [==============================] - 51s 162ms/step - loss: 1.5368\n"
     ]
    }
   ],
   "source": [
    "# model.compile(loss='sparse_categorical_crossentropy', optimizer='adam')\n",
    "# history = model.fit(dataset, epochs=50, callbacks=[ResetStatesCallback()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('saved_models/16_nlp_with_rnns/stateful_shake.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model('saved_models/16_nlp_with_rnns/stateful_shake.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that this model is trained, it can only make predictions of the same size as the batch size used during training. To avoid this restriction, we can create a statelss model and copy the stateful model weights' to this model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "stateless_model = keras.models.Sequential([\n",
    "    keras.layers.GRU(128, return_sequences=True, input_shape=[None, max_id]),\n",
    "    keras.layers.GRU(128, return_sequences=True),\n",
    "    keras.layers.TimeDistributed(keras.layers.Dense(max_id, activation='softmax'))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "stateless_model.set_weights(model.get_weights())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then use it for some predictions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "umber:\n",
      "too this some his brother clarence.\n",
      "\n",
      "duke vi\n"
     ]
    }
   ],
   "source": [
    "print(complete_text('u', stateless_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
