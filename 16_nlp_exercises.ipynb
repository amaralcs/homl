{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.data import Dataset\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Dense, SimpleRNN, GRU, LSTM, TimeDistributed\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras.optimizers import Adam, Nadam\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\"\"\" Avoid error with Blas:GEMM not initializing when using GPU:\n",
    "See: https://stackoverflow.com/questions/43990046/tensorflow-blas-gemm-launch-failed\n",
    "\"\"\"\n",
    "physical_devices = tf.config.list_physical_devices('GPU') \n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "\n",
    "tf.random.set_seed(98)\n",
    "np.random.seed(99)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embedded Reber grammars were used by Hochreiter and Schmidhuber in their paper about LSTMs. They are artificial grammars that produce strings such as “BPBTSXXVPSEPE.” Check out [Jenny Orr’s nice introduction to this topic](https://www.willamette.edu/~gorr/classes/cs449/reber.html). Choose a particular embedded Reber grammar (such as the one represented on Jenny Orr’s page), then train an RNN to identify whether a string respects that grammar or not. You will first need to write a function capable of generating a training batch containing about 50% strings that respect the grammar, and 50% that don’t."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start creating a Reber grammar generator. I'll follow the same structure provided in the link above will allow for tokens to be passed to the class to generate the strings.\n",
    "\n",
    "Each state will be labelled as follows and has two possible transitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Reber Grammar](images\\reber.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "reber_transitions = {\n",
    "    0: [(1, 'B')],\n",
    "    1: [(2, 'T'), (3, 'P')],\n",
    "    2: [(2, 'S'), (4, 'X')],\n",
    "    3: [(3, 'T'), (5, 'V')],\n",
    "    4: [(6, 'S'), (3, 'X')],\n",
    "    5: [(4, 'P'), (6, 'V')],\n",
    "    6: [(7, 'E')]}\n",
    "\n",
    "def move_state(cur_state, transitions=reber_transitions):\n",
    "    \"\"\"Finds possible next moves in transition table and picks one at a random\"\"\"\n",
    "    paths = transitions[cur_state]\n",
    "    return paths[np.random.choice(len(transitions[cur_state]), size=1)[0]]\n",
    "    \n",
    "def generate_string(string=''):\n",
    "    \"\"\"Iterates over possible paths until final state is reached\"\"\"\n",
    "    state = 0\n",
    "    while state != 7:\n",
    "        state, char = move_state(state)\n",
    "        string += char\n",
    "    return string\n",
    "\n",
    "def find_next_state(cur_state, char, transitions=reber_transitions):\n",
    "    \"\"\"Given a current state and a character in the next state, searches transitions for a state\n",
    "    with the corresponding next_char, if it exists.\"\"\"\n",
    "    for (next_state, next_char) in transitions[cur_state]:\n",
    "        if next_char == char:\n",
    "            return next_state, next_char\n",
    "    return -1, -1\n",
    "\n",
    "def validate_string(string, transitions=reber_transitions, verbose=False):\n",
    "    \"\"\"Iterates through a given string and checks whether the string was generated by some grammar with\n",
    "    given transitions. \n",
    "    \n",
    "    Probably can be improved\n",
    "    \"\"\"\n",
    "    next_state, next_char = move_state(0, transitions)\n",
    "    for idx, char in enumerate(string):\n",
    "        if verbose: print(f\"Next State: {next_state}; Testing {idx} : {char} vs {next_char}\")\n",
    "        if char != next_char:\n",
    "            return 0\n",
    "        try:\n",
    "            if verbose: print(f\"\\tGoing to find next state by accessing {string[idx+1]}\")\n",
    "            next_state, next_char = find_next_state(next_state, string[idx+1])\n",
    "            if verbose: print(f\"\\tReturned ({next_state}, {next_char})\")\n",
    "            if next_state == -1:\n",
    "                return 0\n",
    "        except IndexError:\n",
    "            pass # Trying to access out of bounds value, meaning we reached the end of the checks\n",
    "        if next_state == 7:\n",
    "            break\n",
    "    return 1\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's generate positive classes for our dataset and check they are valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_size = 10_000\n",
    "reber_strings = [generate_string() for _ in range(dataset_size)]\n",
    "reber_labels = [1 for _ in range(dataset_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert all([validate_string(reber_string) for reber_string in reber_strings]), \"Some generated string is NOT REBER!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That some sample non-reber strings are invalid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_reber_tests = ['BTSSPXSE', 'BTXXVVSE', 'BPVSPSE', 'BTSSSE', 'BPTVVB']\n",
    "assert not any([validate_string(not_reber) for not_reber in not_reber_tests]), \"One of the test strings was identified as reber\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's generate a bunch of random strings and use the function above to mark them as not reber strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = 'BEPSTVX'\n",
    "min_length = len(min(reber_strings, key=len))\n",
    "max_length = len(max(reber_strings, key=len))\n",
    "\n",
    "# Generate N random strings with the vocab. Each time strings will have different lengths \n",
    "# that are bounded by the min/max size of the reber_lengths\n",
    "randomly_generated = [''.join(np.random.choice(list(vocab), size=np.random.randint(min_length, max_length)))\n",
    "                      for _ in range(dataset_size)]\n",
    "randomly_gen_labels = [validate_string(random_str) for random_str in randomly_generated]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're ready to create the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000,) (20000,)\n"
     ]
    }
   ],
   "source": [
    "X = np.concatenate((reber_strings, randomly_generated))\n",
    "y = np.concatenate((reber_labels, randomly_gen_labels))\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1]), array([ 9998, 10002], dtype=int64))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y, return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data looks balanced enough! Now let's convert it to a Tensorflow Dataset and do preprocessing\n",
    "\n",
    "First we start with tokenizing at character level, converting the characters into numbers and creating a dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_datasets(X, y, batch_size=32):\n",
    "    tokenizer = Tokenizer(char_level=True, lower=False)\n",
    "    tokenizer.fit_on_texts(X)\n",
    "    \n",
    "    encoded = tokenizer.texts_to_sequences(X)\n",
    "    padded = pad_sequences(encoded, maxlen=max_length, padding='post', value=0) # pad with zeros\n",
    "    \n",
    "    # Recall RNN inputs have shape [batch_size, time_steps, dimensionality]\n",
    "    # Need to reshape the data to an appropriate format\n",
    "    X_full, y_full = padded[..., np.newaxis], y.reshape(-1,1)\n",
    "    X_train_full, X_test, y_train_full, y_test = train_test_split(X_full, y_full, test_size=0.05)\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full, test_size=0.05)\n",
    "        \n",
    "    train_set = Dataset.from_tensor_slices((X_train, y_train)).shuffle(dataset_size).batch(batch_size).prefetch(1)\n",
    "    valid_set = Dataset.from_tensor_slices((X_valid, y_valid)).shuffle(dataset_size).batch(batch_size)\n",
    "    test_set = Dataset.from_tensor_slices((X_test, y_test)).shuffle(dataset_size)\n",
    "    return train_set, valid_set, test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, valid_set, test_set = create_datasets(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Input(shape=[None, 1], name='Input'),\n",
    "    SimpleRNN(32, return_sequences=True),\n",
    "    SimpleRNN(32),\n",
    "    Dense(1, activation=\"sigmoid\")])\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=Adam(), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "565/565 [==============================] - 41s 70ms/step - loss: 0.3868 - accuracy: 0.8351 - val_loss: 0.0863 - val_accuracy: 0.9747\n",
      "Epoch 2/5\n",
      "565/565 [==============================] - 39s 70ms/step - loss: 0.0749 - accuracy: 0.9786 - val_loss: 0.0480 - val_accuracy: 0.9874\n",
      "Epoch 3/5\n",
      "565/565 [==============================] - 41s 72ms/step - loss: 0.0495 - accuracy: 0.9869 - val_loss: 0.0480 - val_accuracy: 0.9884\n",
      "Epoch 4/5\n",
      "565/565 [==============================] - 40s 72ms/step - loss: 0.0367 - accuracy: 0.9907 - val_loss: 0.0195 - val_accuracy: 0.9958\n",
      "Epoch 5/5\n",
      "565/565 [==============================] - 40s 71ms/step - loss: 0.0372 - accuracy: 0.9899 - val_loss: 0.0165 - val_accuracy: 0.9958\n"
     ]
    }
   ],
   "source": [
    "with tf.device('GPU:0'):\n",
    "    model.fit(train_set, epochs=5, validation_data=valid_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "homl",
   "language": "python",
   "name": "homl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
