{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.data import Dataset\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Input, Dense, SimpleRNN, GRU, LSTM, LSTMCell, Embedding\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.metrics import Precision, Recall\n",
    "from tensorflow.keras.optimizers import Adam, Nadam\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datetime import datetime\n",
    "\n",
    "\"\"\" Avoid error with Blas:GEMM not initializing when using GPU:\n",
    "See: https://stackoverflow.com/questions/43990046/tensorflow-blas-gemm-launch-failed\n",
    "\"\"\"\n",
    "physical_devices = tf.config.list_physical_devices('GPU') \n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "\n",
    "tf.random.set_seed(98)\n",
    "np.random.seed(99)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embedded Reber grammars were used by Hochreiter and Schmidhuber in their paper about LSTMs. They are artificial grammars that produce strings such as “BPBTSXXVPSEPE.” Check out [Jenny Orr’s nice introduction to this topic](https://www.willamette.edu/~gorr/classes/cs449/reber.html). Choose a particular embedded Reber grammar (such as the one represented on Jenny Orr’s page), then train an RNN to identify whether a string respects that grammar or not. You will first need to write a function capable of generating a training batch containing about 50% strings that respect the grammar, and 50% that don’t."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Reber grammars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start creating a Reber grammar generator. I'll follow the same structure provided in the link above will allow for tokens to be passed to the class to generate the strings.\n",
    "\n",
    "Each state will be labelled as follows and has two possible transitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Reber Grammar](images\\reber.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "reber_transitions = {\n",
    "    0: [(1, 'B')],\n",
    "    1: [(2, 'T'), (3, 'P')],\n",
    "    2: [(2, 'S'), (4, 'X')],\n",
    "    3: [(3, 'T'), (5, 'V')],\n",
    "    4: [(6, 'S'), (3, 'X')],\n",
    "    5: [(4, 'P'), (6, 'V')],\n",
    "    6: [(7, 'E')]}\n",
    "\n",
    "def move_state(cur_state, transitions=reber_transitions):\n",
    "    \"\"\"Finds possible next moves in transition table and picks one at a random\"\"\"\n",
    "    paths = transitions[cur_state]\n",
    "    return paths[np.random.choice(len(transitions[cur_state]), size=1)[0]]\n",
    "    \n",
    "def generate_string(string=''):\n",
    "    \"\"\"Iterates over possible paths until final state is reached\"\"\"\n",
    "    state = 0\n",
    "    while state != 7:\n",
    "        state, char = move_state(state)\n",
    "        string += char\n",
    "    return string\n",
    "\n",
    "def find_next_state(cur_state, char, transitions=reber_transitions):\n",
    "    \"\"\"Given a current state and a character in the next state, searches transitions for a state\n",
    "    with the corresponding next_char, if it exists.\"\"\"\n",
    "    for (next_state, next_char) in transitions[cur_state]:\n",
    "        if next_char == char:\n",
    "            return next_state, next_char\n",
    "    return -1, -1\n",
    "\n",
    "def validate_string(string, transitions=reber_transitions, verbose=False):\n",
    "    \"\"\"Iterates through a given string and checks whether the string was generated by some grammar with\n",
    "    given transitions. \n",
    "    \n",
    "    Probably can be improved\n",
    "    \"\"\"\n",
    "    next_state, next_char = move_state(0, transitions)\n",
    "    for idx, char in enumerate(string):\n",
    "        if verbose: print(f\"Next State: {next_state}; Testing {idx} : {char} vs {next_char}\")\n",
    "        if char != next_char:\n",
    "            return 0\n",
    "        try:\n",
    "            if verbose: print(f\"\\tGoing to find next state by accessing {string[idx+1]}\")\n",
    "            next_state, next_char = find_next_state(next_state, string[idx+1])\n",
    "            if verbose: print(f\"\\tReturned ({next_state}, {next_char})\")\n",
    "            if next_state == -1:\n",
    "                return 0\n",
    "        except IndexError:\n",
    "            pass # Trying to access out of bounds value, meaning we reached the end of the checks\n",
    "        if next_state == 7:\n",
    "            break\n",
    "    return 1\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's generate positive classes for our dataset and check they are valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_size = 10_000\n",
    "reber_strings = [generate_string() for _ in range(dataset_size)]\n",
    "reber_labels = [1 for _ in range(dataset_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert all([validate_string(reber_string) for reber_string in reber_strings]), \"Some generated string is NOT REBER!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That some sample non-reber strings are invalid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_reber_tests = ['BTSSPXSE', 'BTXXVVSE', 'BPVSPSE', 'BTSSSE', 'BPTVVB']\n",
    "assert not any([validate_string(not_reber) for not_reber in not_reber_tests]), \"One of the test strings was identified as reber\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's generate a bunch of random strings and use the function above to mark them as not reber strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = 'BEPSTVX'\n",
    "min_length = len(min(reber_strings, key=len))\n",
    "max_length = len(max(reber_strings, key=len))\n",
    "\n",
    "# Generate N random strings with the vocab. Each time strings will have different lengths \n",
    "# that are bounded by the min/max size of the reber_lengths\n",
    "randomly_generated = [''.join(np.random.choice(list(vocab), size=np.random.randint(min_length, max_length)))\n",
    "                      for _ in range(dataset_size)]\n",
    "randomly_gen_labels = [validate_string(random_str) for random_str in randomly_generated]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000,) (20000,)\n"
     ]
    }
   ],
   "source": [
    "X = np.concatenate((reber_strings, randomly_generated))\n",
    "y = np.concatenate((reber_labels, randomly_gen_labels))\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1]), array([ 9998, 10002], dtype=int64))"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y, return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data looks balanced enough! Now let's convert it to a Tensorflow Dataset and do preprocessing\n",
    "\n",
    "First we start with tokenizing at character level, converting the characters into numbers and creating a dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_datasets(X, y, batch_size=32):\n",
    "    tokenizer = Tokenizer(char_level=True, lower=False)\n",
    "    tokenizer.fit_on_texts(X)\n",
    "    \n",
    "    encoded = tokenizer.texts_to_sequences(X)\n",
    "    padded = pad_sequences(encoded, maxlen=max_length, padding='post', value=0) # pad with zeros\n",
    "    \n",
    "    # Recall RNN inputs have shape [batch_size, time_steps, dimensionality]\n",
    "    # Need to reshape the data to an appropriate format\n",
    "    X_full, y_full = padded[..., np.newaxis], y.reshape(-1,1)\n",
    "    X_train_full, X_test, y_train_full, y_test = train_test_split(X_full, y_full, test_size=0.05)\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full, test_size=0.05)\n",
    "        \n",
    "    train_set = Dataset.from_tensor_slices((X_train, y_train)).shuffle(dataset_size).batch(batch_size).prefetch(1)\n",
    "    valid_set = Dataset.from_tensor_slices((X_valid, y_valid)).shuffle(dataset_size).batch(batch_size)\n",
    "    test_set = Dataset.from_tensor_slices((X_test, y_test)).shuffle(dataset_size).batch(batch_size)\n",
    "    return train_set, valid_set, test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, valid_set, test_set = create_datasets(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define and train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Input(shape=[None, 1], name='Input'),\n",
    "    LSTM(32, return_sequences=True),\n",
    "    LSTM(32),\n",
    "    Dense(1, activation=\"sigmoid\")])\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=Adam(), metrics=['accuracy', Precision(), Recall()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "565/565 [==============================] - 10s 12ms/step - loss: 0.4544 - accuracy: 0.7941 - precision_2: 0.7607 - recall_2: 0.8523 - val_loss: 0.3566 - val_accuracy: 0.8505 - val_precision_2: 0.8307 - val_recall_2: 0.8828\n",
      "Epoch 2/15\n",
      "565/565 [==============================] - 6s 10ms/step - loss: 0.3806 - accuracy: 0.8277 - precision_2: 0.7829 - recall_2: 0.9020 - val_loss: 0.2004 - val_accuracy: 0.9326 - val_precision_2: 0.8966 - val_recall_2: 0.9791\n",
      "Epoch 3/15\n",
      "565/565 [==============================] - 6s 11ms/step - loss: 0.2347 - accuracy: 0.9103 - precision_2: 0.8704 - recall_2: 0.9624 - val_loss: 0.1610 - val_accuracy: 0.9516 - val_precision_2: 0.9303 - val_recall_2: 0.9770\n",
      "Epoch 4/15\n",
      "565/565 [==============================] - 6s 11ms/step - loss: 0.1466 - accuracy: 0.9560 - precision_2: 0.9276 - recall_2: 0.9901 - val_loss: 0.1188 - val_accuracy: 0.9674 - val_precision_2: 0.9443 - val_recall_2: 0.9937\n",
      "Epoch 5/15\n",
      "565/565 [==============================] - 6s 11ms/step - loss: 0.1758 - accuracy: 0.9427 - precision_2: 0.9061 - recall_2: 0.9876 - val_loss: 0.1572 - val_accuracy: 0.9568 - val_precision_2: 0.9226 - val_recall_2: 0.9979\n",
      "Epoch 6/15\n",
      "565/565 [==============================] - 6s 11ms/step - loss: 0.1077 - accuracy: 0.9709 - precision_2: 0.9489 - recall_2: 0.9955 - val_loss: 0.1174 - val_accuracy: 0.9663 - val_precision_2: 0.9407 - val_recall_2: 0.9958\n",
      "Epoch 7/15\n",
      "565/565 [==============================] - 6s 11ms/step - loss: 0.0875 - accuracy: 0.9772 - precision_2: 0.9597 - recall_2: 0.9966 - val_loss: 0.0635 - val_accuracy: 0.9863 - val_precision_2: 0.9755 - val_recall_2: 0.9979\n",
      "Epoch 8/15\n",
      "565/565 [==============================] - 6s 11ms/step - loss: 0.0698 - accuracy: 0.9836 - precision_2: 0.9709 - recall_2: 0.9971 - val_loss: 0.0548 - val_accuracy: 0.9874 - val_precision_2: 0.9775 - val_recall_2: 0.9979\n",
      "Epoch 9/15\n",
      "565/565 [==============================] - 6s 11ms/step - loss: 0.0817 - accuracy: 0.9788 - precision_2: 0.9628 - recall_2: 0.9953 - val_loss: 0.0768 - val_accuracy: 0.9832 - val_precision_2: 0.9695 - val_recall_2: 0.9979\n",
      "Epoch 10/15\n",
      "565/565 [==============================] - 6s 11ms/step - loss: 0.0487 - accuracy: 0.9880 - precision_2: 0.9790 - recall_2: 0.9975 - val_loss: 0.0518 - val_accuracy: 0.9895 - val_precision_2: 0.9815 - val_recall_2: 0.9979\n",
      "Epoch 11/15\n",
      "565/565 [==============================] - 6s 11ms/step - loss: 0.0439 - accuracy: 0.9896 - precision_2: 0.9819 - recall_2: 0.9977 - val_loss: 0.0778 - val_accuracy: 0.9800 - val_precision_2: 0.9636 - val_recall_2: 0.9979\n",
      "Epoch 12/15\n",
      "565/565 [==============================] - 6s 11ms/step - loss: 0.0415 - accuracy: 0.9894 - precision_2: 0.9823 - recall_2: 0.9965 - val_loss: 0.0229 - val_accuracy: 0.9968 - val_precision_2: 0.9958 - val_recall_2: 0.9979\n",
      "Epoch 13/15\n",
      "565/565 [==============================] - 6s 11ms/step - loss: 0.0328 - accuracy: 0.9922 - precision_2: 0.9867 - recall_2: 0.9976 - val_loss: 0.0310 - val_accuracy: 0.9937 - val_precision_2: 0.9896 - val_recall_2: 0.9979\n",
      "Epoch 14/15\n",
      "565/565 [==============================] - 6s 11ms/step - loss: 0.0288 - accuracy: 0.9922 - precision_2: 0.9888 - recall_2: 0.9955 - val_loss: 0.0313 - val_accuracy: 0.9937 - val_precision_2: 0.9896 - val_recall_2: 0.9979\n",
      "Epoch 15/15\n",
      "565/565 [==============================] - 6s 11ms/step - loss: 0.0164 - accuracy: 0.9966 - precision_2: 0.9942 - recall_2: 0.9990 - val_loss: 0.0146 - val_accuracy: 0.9989 - val_precision_2: 1.0000 - val_recall_2: 0.9979\n"
     ]
    }
   ],
   "source": [
    "with tf.device('GPU:0'):\n",
    "    model.fit(train_set, epochs=15, validation_data=valid_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 0s 6ms/step - loss: 0.0092 - accuracy: 0.9990 - precision_2: 0.9980 - recall_2: 1.0000\n"
     ]
    }
   ],
   "source": [
    "loss, acc, pre, rec = model.evaluate(test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train an Encoder–Decoder model that can convert a date string from one format to another (e.g., from “April 22, 2019” to “2019-04-22”).\n",
    "\n",
    "Let's first generate the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>X</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>34536</th>\n",
       "      <td>1994-07-23</td>\n",
       "      <td>July 23, 1994</td>\n",
       "      <td>1994-07-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42697</th>\n",
       "      <td>2016-11-25</td>\n",
       "      <td>November 25, 2016</td>\n",
       "      <td>2016-11-25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36008</th>\n",
       "      <td>1998-08-03</td>\n",
       "      <td>August 03, 1998</td>\n",
       "      <td>1998-08-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23587</th>\n",
       "      <td>1964-07-31</td>\n",
       "      <td>July 31, 1964</td>\n",
       "      <td>1964-07-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29313</th>\n",
       "      <td>1980-04-04</td>\n",
       "      <td>April 04, 1980</td>\n",
       "      <td>1980-04-04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          source                  X           y\n",
       "34536 1994-07-23      July 23, 1994  1994-07-23\n",
       "42697 2016-11-25  November 25, 2016  2016-11-25\n",
       "36008 1998-08-03    August 03, 1998  1998-08-03\n",
       "23587 1964-07-31      July 31, 1964  1964-07-31\n",
       "29313 1980-04-04     April 04, 1980  1980-04-04"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({\"source\":np.arange('1900-01-01', '2021-01-01', dtype='datetime64[D]')})\n",
    "df[\"X\"] = df[\"source\"].dt.strftime(\"%B %d, %Y\")\n",
    "df[\"y\"] = df[\"source\"].dt.strftime(\"%Y-%m-%d\")\n",
    "df = df.sample(frac=1.)  # Resample the dataframe to shuffle the dates\n",
    "\n",
    "X = df[\"X\"].to_numpy()\n",
    "y = df[\"y\"].to_numpy()\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the data already cleansed, we can tokenize both X and y with the same object. The `<sos>` and `<eos>` tokens will have ids 0 and 1 respectively.\n",
    "\n",
    "Since we'll be outputting probabilities and our targets are words, we have no way of calculating metrics during training (maybe?). Thus I'll use a very small test size\n",
    "\n",
    "*We also have to shift the decoder inputs by 1 so the words we give as inputs are the words that it **should** have output at the previous step* - Not so sure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_vocab = sorted(list(set(word for entry in X for word in entry)))\n",
    "output_vocab = sorted(list(set(word for entry in y for word in entry)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_sent(sent, vocab):\n",
    "    return [vocab.index(char) for char in sent]\n",
    "\n",
    "def encode_data(data, vocab):\n",
    "    ids = [tokenize_sent(sent, vocab) for sent in data]\n",
    "    data = tf.ragged.constant(ids, ragged_rank=1)\n",
    "    return (data + 1).to_tensor() # will be padding token id\n",
    "\n",
    "def decode_sequence(seq, vocab):\n",
    "    return ''.join(vocab[char_id - 1] for char_id in seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_encoded = encode_data(X, input_vocab)\n",
    "y_encoded = encode_data(y, output_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = len(X_encoded) * 80 // 100\n",
    "test_size = (len(X_encoded) - train_size) // 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = X_encoded[:train_size], y_encoded[:train_size]\n",
    "X_valid, y_valid = X_encoded[train_size:train_size + test_size], y_encoded[train_size:train_size + test_size]\n",
    "X_test, y_test = X_encoded[train_size + test_size:], y_encoded[train_size + test_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "EMBED_SIZE = 8\n",
    "sos_id = len(output_vocab) + 1 # adding a start of sequence token for the shifted decoder inputs\n",
    "\n",
    "def get_sequence_lengths(y):\n",
    "    return np.full([y.shape[0]], y.shape[1])\n",
    "\n",
    "def shift_sequence(y, fill=sos_id):\n",
    "    \"\"\"Shifts target out by 1 to generate decoder inputs with a start of sequence token\"\"\"\n",
    "    return np.c_[np.full((len(y), 1), fill), y[:, :-1]]\n",
    "\n",
    "def create_dataset(X, y, batch_size=BATCH_SIZE):\n",
    "    input_ = Dataset.zip((\n",
    "        Dataset.from_tensor_slices(X), # Encoder inputs\n",
    "        Dataset.from_tensor_slices(shift_sequence(y)), # Decoder inputs\n",
    "        Dataset.from_tensor_slices(get_sequence_lengths(y)))) # Target sequence lengths\n",
    "    target = Dataset.from_tensor_slices(y)\n",
    "    return Dataset.zip((input_, target)).shuffle(len(X)).batch(batch_size)\n",
    "    \n",
    "train_set = create_dataset(X_train, y_train).prefetch(1)\n",
    "valid_set = create_dataset(X_valid, y_valid)\n",
    "test_set = create_dataset(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this Encoder-Decoder Network, we'll be using [tensorflow-addons](https://www.tensorflow.org/addons/tutorials/networks_seq2seq_nmt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"date_translator\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_inputs (InputLayer)     [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "decoder_inputs (InputLayer)     [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "encoder_embedding (Embedding)   (None, None, 8)      312         encoder_inputs[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "decoder_embedding (Embedding)   (None, None, 8)      320         decoder_inputs[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     [(None, 64), (None,  18688       encoder_embedding[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "sequence_lengths (InputLayer)   [(None,)]            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "basic_decoder (BasicDecoder)    (BasicDecoderOutput( 19468       decoder_embedding[0][0]          \n",
      "                                                                 lstm[0][1]                       \n",
      "                                                                 lstm[0][2]                       \n",
      "                                                                 sequence_lengths[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.softmax (TFOpLambda)      (None, None, 12)     0           basic_decoder[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 38,788\n",
      "Trainable params: 38,788\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder_inputs = Input(shape=[None], dtype=np.int32, name='encoder_inputs')\n",
    "decoder_inputs = Input(shape=[None], dtype=np.int32, name='decoder_inputs')\n",
    "sequence_lengths = Input(shape=[], dtype=np.int32, name='sequence_lengths')\n",
    "\n",
    "# Add 1 to the Embed dimesion due to using 0 padding\n",
    "encoder_embeddings = Embedding(len(input_vocab) + 1, EMBED_SIZE, name='encoder_embedding')\n",
    "# And an extra 1 for the SoS token\n",
    "decoder_embeddings = Embedding(len(input_vocab) + 2, EMBED_SIZE, name='decoder_embedding')\n",
    "\n",
    "encoder_embeddings = encoder_embeddings(encoder_inputs)\n",
    "decoder_embeddings = decoder_embeddings(decoder_inputs)\n",
    "\n",
    "encoder = LSTM(64, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_embeddings)\n",
    "encoder_state = [state_h, state_c]\n",
    "\n",
    "sampler = tfa.seq2seq.sampler.TrainingSampler()\n",
    "\n",
    "decoder_cell = LSTMCell(64)\n",
    "output_layer = Dense(len(output_vocab) + 1) # Zero padding\n",
    "decoder = tfa.seq2seq.basic_decoder.BasicDecoder(decoder_cell, sampler, output_layer=output_layer)\n",
    "final_outputs, final_state, final_sequence_lengths = decoder(decoder_embeddings,\n",
    "                                                             initial_state=encoder_state,\n",
    "                                                             sequence_length=sequence_lengths)\n",
    "Y_proba = tf.nn.softmax(final_outputs.rnn_output)\n",
    "\n",
    "model = Model(inputs=[encoder_inputs, decoder_inputs, sequence_lengths],\n",
    "              outputs=[Y_proba],\n",
    "              name='date_translator')\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer='nadam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup for tensorboard\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "fpath = './my_logs/date_translator'\n",
    "Path(fpath).mkdir(parents=True, exist_ok=True)\n",
    "root_logdir = os.path.join(os.curdir, fpath)\n",
    "\n",
    "def get_run_logdir():\n",
    "    import time\n",
    "    run_id = f\"char_model_batch_size_{BATCH_SIZE}_embed_dim_{EMBED_SIZE}\"\n",
    "    run_id += time.strftime(\"_%Y_%m_%d-%H\")\n",
    "    return os.path.join(root_logdir, run_id)\n",
    "\n",
    "callbacks = [TensorBoard(get_run_logdir())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tensorboard.plugins import projector\n",
    "\n",
    "def get_last_modified_folder(loc='./my_logs/date_translator/'):\n",
    "    files = os.listdir(loc)\n",
    "    stats = [(loc+fname, os.stat(loc+fname).st_mtime) for fname in files]\n",
    "    return max(stats, key=lambda x: x[1])[0]\n",
    "\n",
    "def save_embeddings(embed_layer, name, vocab):\n",
    "    logdir = get_last_modified_folder()\n",
    "    with open(os.path.join(logdir, f'{name}_metadata.tsv'), \"w\") as f:\n",
    "        for word in vocab:\n",
    "            f.write(f\"{word}\\n\")\n",
    "    \n",
    "    weights = tf.Variable(embed_layer.get_weights()[0])\n",
    "    checkpoint = tf.train.Checkpoint(embedding=weights)\n",
    "    checkpoint.save(os.path.join(logdir, f\"{name}.ckpt\"))\n",
    "    \n",
    "    config = projector.ProjectorConfig()\n",
    "    embedding = config.embeddings.add()\n",
    "    embedding.tensor_name = f\"{name}/.ATTRIBUTES/TEST_VALUE\"\n",
    "    embedding.metadata_path = f'{name}_metadata.tsv'\n",
    "    projector.visualize_embeddings(logdir, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1105/1105 [==============================] - 23s 18ms/step - loss: 1.3994 - accuracy: 0.5026 - val_loss: 0.8578 - val_accuracy: 0.6653\n",
      "Epoch 2/5\n",
      "1105/1105 [==============================] - 18s 16ms/step - loss: 0.7715 - accuracy: 0.7139 - val_loss: 0.5667 - val_accuracy: 0.7920\n",
      "Epoch 3/5\n",
      "1105/1105 [==============================] - 18s 16ms/step - loss: 0.4343 - accuracy: 0.8548 - val_loss: 0.1963 - val_accuracy: 0.9432\n",
      "Epoch 4/5\n",
      "1105/1105 [==============================] - 18s 16ms/step - loss: 0.1545 - accuracy: 0.9628 - val_loss: 0.0601 - val_accuracy: 0.9942\n",
      "Epoch 5/5\n",
      "1105/1105 [==============================] - 18s 16ms/step - loss: 0.0375 - accuracy: 0.9981 - val_loss: 0.0143 - val_accuracy: 0.9998\n"
     ]
    }
   ],
   "source": [
    "with tf.device('CPU:0'):\n",
    "    model.fit(train_set, epochs=5, validation_data=valid_set,callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the embeddings to examine in TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_layer = model.layers[2]\n",
    "save_embeddings(embed_layer, embed_layer.name, input_vocab)\n",
    "\n",
    "embed_layer = model.layers[3]\n",
    "save_embeddings(embed_layer, embed_layer.name, output_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3, 11,  9, ...,  1,  3,  5],\n",
       "       [ 3, 11,  6, ...,  1,  4,  5],\n",
       "       [ 3, 11,  6, ...,  1,  5,  2],\n",
       "       ...,\n",
       "       [ 3, 11,  9, ...,  1,  4, 11],\n",
       "       [ 4,  2,  2, ...,  1,  4,  9],\n",
       "       [ 3, 11,  3, ...,  1,  4,  3]], dtype=int64)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(y_pred_probas, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_probas = model.predict(test_set)\n",
    "y_preds = np.argmax(y_pred_probas, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual = [decode_sequence(sent, output_vocab) for sent in y_test]\n",
    "translated = [decode_sequence(sent , output_vocab) for sent in y_preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "139/139 [==============================] - 3s 22ms/step - loss: 0.0140 - accuracy: 0.9999\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.014049885794520378, 0.9998868703842163]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actual</th>\n",
       "      <th>translation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-05-20</td>\n",
       "      <td>1975-12-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2009-10-28</td>\n",
       "      <td>1946-07-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1936-06-16</td>\n",
       "      <td>1948-12-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1954-09-22</td>\n",
       "      <td>1920-12-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1984-06-21</td>\n",
       "      <td>1908-01-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1949-07-06</td>\n",
       "      <td>1982-12-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1921-09-08</td>\n",
       "      <td>2006-01-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1943-11-23</td>\n",
       "      <td>1906-01-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1970-12-08</td>\n",
       "      <td>1923-12-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1911-01-21</td>\n",
       "      <td>1949-04-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1966-10-04</td>\n",
       "      <td>1977-06-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1947-05-19</td>\n",
       "      <td>1941-01-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1902-11-04</td>\n",
       "      <td>1973-10-24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1960-01-01</td>\n",
       "      <td>1990-12-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2017-05-11</td>\n",
       "      <td>1960-02-24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1951-12-20</td>\n",
       "      <td>1924-12-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1997-06-18</td>\n",
       "      <td>1900-08-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1922-08-08</td>\n",
       "      <td>1903-11-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1922-08-21</td>\n",
       "      <td>1901-06-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1947-02-20</td>\n",
       "      <td>2000-01-09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        actual translation\n",
       "0   2018-05-20  1975-12-13\n",
       "1   2009-10-28  1946-07-23\n",
       "2   1936-06-16  1948-12-30\n",
       "3   1954-09-22  1920-12-29\n",
       "4   1984-06-21  1908-01-23\n",
       "5   1949-07-06  1982-12-07\n",
       "6   1921-09-08  2006-01-30\n",
       "7   1943-11-23  1906-01-03\n",
       "8   1970-12-08  1923-12-02\n",
       "9   1911-01-21  1949-04-10\n",
       "10  1966-10-04  1977-06-03\n",
       "11  1947-05-19  1941-01-15\n",
       "12  1902-11-04  1973-10-24\n",
       "13  1960-01-01  1990-12-31\n",
       "14  2017-05-11  1960-02-24\n",
       "15  1951-12-20  1924-12-13\n",
       "16  1997-06-18  1900-08-12\n",
       "17  1922-08-08  1903-11-10\n",
       "18  1922-08-21  1901-06-04\n",
       "19  1947-02-20  2000-01-09"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df = pd.DataFrame({\"actual\": actual, \"translation\": translated})\n",
    "result_df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not sure why the translations are so off even though we have 100% accuracy. Must double check this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "homl",
   "language": "python",
   "name": "homl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
