{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.data import Dataset\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Input, Dense, SimpleRNN, GRU, LSTM, LSTMCell, Embedding\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.metrics import Precision, Recall\n",
    "from tensorflow.keras.optimizers import Adam, Nadam\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datetime import datetime\n",
    "\n",
    "\"\"\" Avoid error with Blas:GEMM not initializing when using GPU:\n",
    "See: https://stackoverflow.com/questions/43990046/tensorflow-blas-gemm-launch-failed\n",
    "\"\"\"\n",
    "physical_devices = tf.config.list_physical_devices('GPU') \n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "\n",
    "tf.random.set_seed(98)\n",
    "np.random.seed(99)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embedded Reber grammars were used by Hochreiter and Schmidhuber in their paper about LSTMs. They are artificial grammars that produce strings such as “BPBTSXXVPSEPE.” Check out [Jenny Orr’s nice introduction to this topic](https://www.willamette.edu/~gorr/classes/cs449/reber.html). Choose a particular embedded Reber grammar (such as the one represented on Jenny Orr’s page), then train an RNN to identify whether a string respects that grammar or not. You will first need to write a function capable of generating a training batch containing about 50% strings that respect the grammar, and 50% that don’t."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Reber grammars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start creating a Reber grammar generator. I'll follow the same structure provided in the link above will allow for tokens to be passed to the class to generate the strings.\n",
    "\n",
    "Each state will be labelled as follows and has two possible transitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Reber Grammar](images\\reber.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "reber_transitions = {\n",
    "    0: [(1, 'B')],\n",
    "    1: [(2, 'T'), (3, 'P')],\n",
    "    2: [(2, 'S'), (4, 'X')],\n",
    "    3: [(3, 'T'), (5, 'V')],\n",
    "    4: [(6, 'S'), (3, 'X')],\n",
    "    5: [(4, 'P'), (6, 'V')],\n",
    "    6: [(7, 'E')]}\n",
    "\n",
    "def move_state(cur_state, transitions=reber_transitions):\n",
    "    \"\"\"Finds possible next moves in transition table and picks one at a random\"\"\"\n",
    "    paths = transitions[cur_state]\n",
    "    return paths[np.random.choice(len(transitions[cur_state]), size=1)[0]]\n",
    "    \n",
    "def generate_string(string=''):\n",
    "    \"\"\"Iterates over possible paths until final state is reached\"\"\"\n",
    "    state = 0\n",
    "    while state != 7:\n",
    "        state, char = move_state(state)\n",
    "        string += char\n",
    "    return string\n",
    "\n",
    "def find_next_state(cur_state, char, transitions=reber_transitions):\n",
    "    \"\"\"Given a current state and a character in the next state, searches transitions for a state\n",
    "    with the corresponding next_char, if it exists.\"\"\"\n",
    "    for (next_state, next_char) in transitions[cur_state]:\n",
    "        if next_char == char:\n",
    "            return next_state, next_char\n",
    "    return -1, -1\n",
    "\n",
    "def validate_string(string, transitions=reber_transitions, verbose=False):\n",
    "    \"\"\"Iterates through a given string and checks whether the string was generated by some grammar with\n",
    "    given transitions. \n",
    "    \n",
    "    Probably can be improved\n",
    "    \"\"\"\n",
    "    next_state, next_char = move_state(0, transitions)\n",
    "    for idx, char in enumerate(string):\n",
    "        if verbose: print(f\"Next State: {next_state}; Testing {idx} : {char} vs {next_char}\")\n",
    "        if char != next_char:\n",
    "            return 0\n",
    "        try:\n",
    "            if verbose: print(f\"\\tGoing to find next state by accessing {string[idx+1]}\")\n",
    "            next_state, next_char = find_next_state(next_state, string[idx+1])\n",
    "            if verbose: print(f\"\\tReturned ({next_state}, {next_char})\")\n",
    "            if next_state == -1:\n",
    "                return 0\n",
    "        except IndexError:\n",
    "            pass # Trying to access out of bounds value, meaning we reached the end of the checks\n",
    "        if next_state == 7:\n",
    "            break\n",
    "    return 1\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's generate positive classes for our dataset and check they are valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_size = 10_000\n",
    "reber_strings = [generate_string() for _ in range(dataset_size)]\n",
    "reber_labels = [1 for _ in range(dataset_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert all([validate_string(reber_string) for reber_string in reber_strings]), \"Some generated string is NOT REBER!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That some sample non-reber strings are invalid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_reber_tests = ['BTSSPXSE', 'BTXXVVSE', 'BPVSPSE', 'BTSSSE', 'BPTVVB']\n",
    "assert not any([validate_string(not_reber) for not_reber in not_reber_tests]), \"One of the test strings was identified as reber\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's generate a bunch of random strings and use the function above to mark them as not reber strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = 'BEPSTVX'\n",
    "min_length = len(min(reber_strings, key=len))\n",
    "max_length = len(max(reber_strings, key=len))\n",
    "\n",
    "# Generate N random strings with the vocab. Each time strings will have different lengths \n",
    "# that are bounded by the min/max size of the reber_lengths\n",
    "randomly_generated = [''.join(np.random.choice(list(vocab), size=np.random.randint(min_length, max_length)))\n",
    "                      for _ in range(dataset_size)]\n",
    "randomly_gen_labels = [validate_string(random_str) for random_str in randomly_generated]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000,) (20000,)\n"
     ]
    }
   ],
   "source": [
    "X = np.concatenate((reber_strings, randomly_generated))\n",
    "y = np.concatenate((reber_labels, randomly_gen_labels))\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1]), array([ 9998, 10002], dtype=int64))"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y, return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data looks balanced enough! Now let's convert it to a Tensorflow Dataset and do preprocessing\n",
    "\n",
    "First we start with tokenizing at character level, converting the characters into numbers and creating a dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_datasets(X, y, batch_size=32):\n",
    "    tokenizer = Tokenizer(char_level=True, lower=False)\n",
    "    tokenizer.fit_on_texts(X)\n",
    "    \n",
    "    encoded = tokenizer.texts_to_sequences(X)\n",
    "    padded = pad_sequences(encoded, maxlen=max_length, padding='post', value=0) # pad with zeros\n",
    "    \n",
    "    # Recall RNN inputs have shape [batch_size, time_steps, dimensionality]\n",
    "    # Need to reshape the data to an appropriate format\n",
    "    X_full, y_full = padded[..., np.newaxis], y.reshape(-1,1)\n",
    "    X_train_full, X_test, y_train_full, y_test = train_test_split(X_full, y_full, test_size=0.05)\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full, test_size=0.05)\n",
    "        \n",
    "    train_set = Dataset.from_tensor_slices((X_train, y_train)).shuffle(dataset_size).batch(batch_size).prefetch(1)\n",
    "    valid_set = Dataset.from_tensor_slices((X_valid, y_valid)).shuffle(dataset_size).batch(batch_size)\n",
    "    test_set = Dataset.from_tensor_slices((X_test, y_test)).shuffle(dataset_size).batch(batch_size)\n",
    "    return train_set, valid_set, test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, valid_set, test_set = create_datasets(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define and train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Input(shape=[None, 1], name='Input'),\n",
    "    LSTM(32, return_sequences=True),\n",
    "    LSTM(32),\n",
    "    Dense(1, activation=\"sigmoid\")])\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=Adam(), metrics=['accuracy', Precision(), Recall()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "565/565 [==============================] - 10s 12ms/step - loss: 0.4544 - accuracy: 0.7941 - precision_2: 0.7607 - recall_2: 0.8523 - val_loss: 0.3566 - val_accuracy: 0.8505 - val_precision_2: 0.8307 - val_recall_2: 0.8828\n",
      "Epoch 2/15\n",
      "565/565 [==============================] - 6s 10ms/step - loss: 0.3806 - accuracy: 0.8277 - precision_2: 0.7829 - recall_2: 0.9020 - val_loss: 0.2004 - val_accuracy: 0.9326 - val_precision_2: 0.8966 - val_recall_2: 0.9791\n",
      "Epoch 3/15\n",
      "565/565 [==============================] - 6s 11ms/step - loss: 0.2347 - accuracy: 0.9103 - precision_2: 0.8704 - recall_2: 0.9624 - val_loss: 0.1610 - val_accuracy: 0.9516 - val_precision_2: 0.9303 - val_recall_2: 0.9770\n",
      "Epoch 4/15\n",
      "565/565 [==============================] - 6s 11ms/step - loss: 0.1466 - accuracy: 0.9560 - precision_2: 0.9276 - recall_2: 0.9901 - val_loss: 0.1188 - val_accuracy: 0.9674 - val_precision_2: 0.9443 - val_recall_2: 0.9937\n",
      "Epoch 5/15\n",
      "565/565 [==============================] - 6s 11ms/step - loss: 0.1758 - accuracy: 0.9427 - precision_2: 0.9061 - recall_2: 0.9876 - val_loss: 0.1572 - val_accuracy: 0.9568 - val_precision_2: 0.9226 - val_recall_2: 0.9979\n",
      "Epoch 6/15\n",
      "565/565 [==============================] - 6s 11ms/step - loss: 0.1077 - accuracy: 0.9709 - precision_2: 0.9489 - recall_2: 0.9955 - val_loss: 0.1174 - val_accuracy: 0.9663 - val_precision_2: 0.9407 - val_recall_2: 0.9958\n",
      "Epoch 7/15\n",
      "565/565 [==============================] - 6s 11ms/step - loss: 0.0875 - accuracy: 0.9772 - precision_2: 0.9597 - recall_2: 0.9966 - val_loss: 0.0635 - val_accuracy: 0.9863 - val_precision_2: 0.9755 - val_recall_2: 0.9979\n",
      "Epoch 8/15\n",
      "565/565 [==============================] - 6s 11ms/step - loss: 0.0698 - accuracy: 0.9836 - precision_2: 0.9709 - recall_2: 0.9971 - val_loss: 0.0548 - val_accuracy: 0.9874 - val_precision_2: 0.9775 - val_recall_2: 0.9979\n",
      "Epoch 9/15\n",
      "565/565 [==============================] - 6s 11ms/step - loss: 0.0817 - accuracy: 0.9788 - precision_2: 0.9628 - recall_2: 0.9953 - val_loss: 0.0768 - val_accuracy: 0.9832 - val_precision_2: 0.9695 - val_recall_2: 0.9979\n",
      "Epoch 10/15\n",
      "565/565 [==============================] - 6s 11ms/step - loss: 0.0487 - accuracy: 0.9880 - precision_2: 0.9790 - recall_2: 0.9975 - val_loss: 0.0518 - val_accuracy: 0.9895 - val_precision_2: 0.9815 - val_recall_2: 0.9979\n",
      "Epoch 11/15\n",
      "565/565 [==============================] - 6s 11ms/step - loss: 0.0439 - accuracy: 0.9896 - precision_2: 0.9819 - recall_2: 0.9977 - val_loss: 0.0778 - val_accuracy: 0.9800 - val_precision_2: 0.9636 - val_recall_2: 0.9979\n",
      "Epoch 12/15\n",
      "565/565 [==============================] - 6s 11ms/step - loss: 0.0415 - accuracy: 0.9894 - precision_2: 0.9823 - recall_2: 0.9965 - val_loss: 0.0229 - val_accuracy: 0.9968 - val_precision_2: 0.9958 - val_recall_2: 0.9979\n",
      "Epoch 13/15\n",
      "565/565 [==============================] - 6s 11ms/step - loss: 0.0328 - accuracy: 0.9922 - precision_2: 0.9867 - recall_2: 0.9976 - val_loss: 0.0310 - val_accuracy: 0.9937 - val_precision_2: 0.9896 - val_recall_2: 0.9979\n",
      "Epoch 14/15\n",
      "565/565 [==============================] - 6s 11ms/step - loss: 0.0288 - accuracy: 0.9922 - precision_2: 0.9888 - recall_2: 0.9955 - val_loss: 0.0313 - val_accuracy: 0.9937 - val_precision_2: 0.9896 - val_recall_2: 0.9979\n",
      "Epoch 15/15\n",
      "565/565 [==============================] - 6s 11ms/step - loss: 0.0164 - accuracy: 0.9966 - precision_2: 0.9942 - recall_2: 0.9990 - val_loss: 0.0146 - val_accuracy: 0.9989 - val_precision_2: 1.0000 - val_recall_2: 0.9979\n"
     ]
    }
   ],
   "source": [
    "with tf.device('GPU:0'):\n",
    "    model.fit(train_set, epochs=15, validation_data=valid_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 0s 6ms/step - loss: 0.0092 - accuracy: 0.9990 - precision_2: 0.9980 - recall_2: 1.0000\n"
     ]
    }
   ],
   "source": [
    "loss, acc, pre, rec = model.evaluate(test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train an Encoder–Decoder model that can convert a date string from one format to another (e.g., from “April 22, 2019” to “2019-04-22”).\n",
    "\n",
    "Let's first generate the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>X</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>34536</th>\n",
       "      <td>1994-07-23</td>\n",
       "      <td>July 23, 1994</td>\n",
       "      <td>1994-07-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42697</th>\n",
       "      <td>2016-11-25</td>\n",
       "      <td>November 25, 2016</td>\n",
       "      <td>2016-11-25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36008</th>\n",
       "      <td>1998-08-03</td>\n",
       "      <td>August 03, 1998</td>\n",
       "      <td>1998-08-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23587</th>\n",
       "      <td>1964-07-31</td>\n",
       "      <td>July 31, 1964</td>\n",
       "      <td>1964-07-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29313</th>\n",
       "      <td>1980-04-04</td>\n",
       "      <td>April 04, 1980</td>\n",
       "      <td>1980-04-04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          source                  X           y\n",
       "34536 1994-07-23      July 23, 1994  1994-07-23\n",
       "42697 2016-11-25  November 25, 2016  2016-11-25\n",
       "36008 1998-08-03    August 03, 1998  1998-08-03\n",
       "23587 1964-07-31      July 31, 1964  1964-07-31\n",
       "29313 1980-04-04     April 04, 1980  1980-04-04"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({\"source\":np.arange('1900-01-01', '2021-01-01', dtype='datetime64[D]')})\n",
    "df[\"X\"] = df[\"source\"].dt.strftime(\"%B %d, %Y\")\n",
    "df[\"y\"] = df[\"source\"].dt.strftime(\"%Y-%m-%d\")\n",
    "df = df.sample(frac=1.)  # Resample the dataframe to shuffle the dates\n",
    "\n",
    "X = df[\"X\"].to_numpy()\n",
    "y = df[\"y\"].to_numpy()\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the data already cleansed, we can tokenize both X and y with the same object. The `<sos>` and `<eos>` tokens will have ids 0 and 1 respectively.\n",
    "\n",
    "Since we'll be outputting probabilities and our targets are words, we have no way of calculating metrics during training (maybe?). Thus I'll use a very small test size\n",
    "\n",
    "*We also have to shift the decoder inputs by 1 so the words we give as inputs are the words that it **should** have output at the previous step* - Not so sure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_vocab = sorted(list(set(word for entry in X for word in entry)))\n",
    "output_vocab = sorted(list(set(word for entry in y for word in entry)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_sent(sent, vocab):\n",
    "    \"\"\"Converts characters to ints by using char's index \"\"\"\n",
    "    return [vocab.index(char) for char in sent]\n",
    "\n",
    "def encode_data(data, vocab):\n",
    "    \"\"\"Tokenize list of sentence and converts them to a dense tensor padded\n",
    "    with zeroes.\"\"\"\n",
    "    ids = [tokenize_sent(sent, vocab) for sent in data]\n",
    "    data = tf.ragged.constant(ids, ragged_rank=1)\n",
    "    return (data + 1).to_tensor() # Padding token will have id 0\n",
    "\n",
    "def decode_sequence(seq, vocab):\n",
    "    return ''.join(vocab[char_id - 1] for char_id in seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_encoded = encode_data(X, input_vocab)\n",
    "y_encoded = encode_data(y, output_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X :  ['July 31, 1939']\n",
      "X encoded:  tf.Tensor([[16 36 28 38  1  6  4  2  1  4 12  6 12  0  0  0  0  0]], shape=(1, 18), dtype=int32)\n",
      "X decoded:  July 31, 1939yyyyy\n"
     ]
    }
   ],
   "source": [
    "print(\"X : \", X[:1])\n",
    "print(\"X encoded: \", X_encoded[:1])\n",
    "print(\"X decoded: \", decode_sequence(X_encoded[:1].numpy()[0], input_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train, validation, test split\n",
    "train_size = len(X_encoded) * 80 // 100\n",
    "test_size = (len(X_encoded) - train_size) // 2\n",
    "\n",
    "X_train, y_train = X_encoded[:train_size], y_encoded[:train_size]\n",
    "X_valid, y_valid = X_encoded[train_size:train_size + test_size], y_encoded[train_size:train_size + test_size]\n",
    "X_test, y_test = X_encoded[train_size + test_size:], y_encoded[train_size + test_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "EMBED_SIZE = 8\n",
    "MAX_OUTPUT_LEN = y_train.shape[1]\n",
    "sos_id = len(output_vocab) + 1 # adding a start of sequence token for the shifted decoder inputs\n",
    "\n",
    "def get_sequence_lengths(y):\n",
    "    return np.full([y.shape[0]], y_train.shape[1])\n",
    "\n",
    "def shift_sequence(y, fill=sos_id):\n",
    "    \"\"\"Shifts target out by 1 to generate decoder inputs with a start of sequence token\"\"\"\n",
    "    return np.c_[np.full((len(y), 1), fill), y[:, :-1]]\n",
    "\n",
    "def create_dataset(X, y, batch_size=BATCH_SIZE):\n",
    "    input_ = Dataset.zip((\n",
    "        Dataset.from_tensor_slices(X), # Encoder inputs\n",
    "        Dataset.from_tensor_slices(shift_sequence(y)))) # Decoder inputs\n",
    "    target = Dataset.from_tensor_slices(y)\n",
    "    return Dataset.zip((input_, target)).shuffle(len(X)).batch(batch_size)\n",
    "    \n",
    "train_set = create_dataset(X_train, y_train).prefetch(1)\n",
    "valid_set = create_dataset(X_valid, y_valid)\n",
    "test_set = create_dataset(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this Encoder-Decoder Network, we'll be using [tensorflow-addons](https://www.tensorflow.org/addons/tutorials/networks_seq2seq_nmt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"date_translator\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_inputs (InputLayer)     [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "decoder_inputs (InputLayer)     [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "encoder_embedding (Embedding)   (None, None, 8)      312         encoder_inputs[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "decoder_embedding (Embedding)   (None, None, 8)      320         decoder_inputs[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   [(None, 64), (None,  18688       encoder_embedding[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "basic_decoder_2 (BasicDecoder)  (BasicDecoderOutput( 19468       decoder_embedding[0][0]          \n",
      "                                                                 lstm_2[0][1]                     \n",
      "                                                                 lstm_2[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.softmax_2 (TFOpLambda)    (None, None, 12)     0           basic_decoder_2[0][0]            \n",
      "==================================================================================================\n",
      "Total params: 38,788\n",
      "Trainable params: 38,788\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder_inputs = Input(shape=[None], dtype=np.int32, name='encoder_inputs')\n",
    "decoder_inputs = Input(shape=[None], dtype=np.int32, name='decoder_inputs')\n",
    "sequence_lengths = Input(shape=[], dtype=np.int32, name='sequence_lengths')\n",
    "\n",
    "# Add 1 to the Embed dimension due to using 0 padding\n",
    "encoder_embedding_layer = Embedding(len(input_vocab) + 1, EMBED_SIZE, name='encoder_embedding')\n",
    "# And an extra 1 for the SoS token\n",
    "decoder_embedding_layer = Embedding(len(input_vocab) + 2, EMBED_SIZE, name='decoder_embedding')\n",
    "\n",
    "encoder_embeddings = encoder_embedding_layer(encoder_inputs)\n",
    "decoder_embeddings = decoder_embedding_layer(decoder_inputs)\n",
    "\n",
    "encoder = LSTM(64, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_embeddings)\n",
    "encoder_state = [state_h, state_c]\n",
    "\n",
    "sampler = tfa.seq2seq.sampler.TrainingSampler()\n",
    "\n",
    "decoder_cell = LSTMCell(64)\n",
    "output_layer = Dense(len(output_vocab) + 1) # Zero padding\n",
    "decoder = tfa.seq2seq.basic_decoder.BasicDecoder(decoder_cell, sampler, output_layer=output_layer)\n",
    "final_outputs, final_state, final_sequence_lengths = decoder(decoder_embeddings,\n",
    "                                                             initial_state=encoder_state)\n",
    "Y_proba = tf.nn.softmax(final_outputs.rnn_output)\n",
    "\n",
    "model = Model(inputs=[encoder_inputs, decoder_inputs],\n",
    "              outputs=[Y_proba],\n",
    "              name='date_translator')\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer='nadam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup for tensorboard\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "fpath = './my_logs/date_translator'\n",
    "Path(fpath).mkdir(parents=True, exist_ok=True)\n",
    "root_logdir = os.path.join(os.curdir, fpath)\n",
    "\n",
    "def get_run_logdir():\n",
    "    import time\n",
    "    run_id = f\"char_model_batch_size_{BATCH_SIZE}_embed_dim_{EMBED_SIZE}\"\n",
    "    run_id += time.strftime(\"_%Y_%m_%d-%H\")\n",
    "    return os.path.join(root_logdir, run_id)\n",
    "\n",
    "callbacks = [TensorBoard(get_run_logdir())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tensorboard.plugins import projector\n",
    "\n",
    "def get_last_modified_folder(loc='./my_logs/date_translator/'):\n",
    "    files = os.listdir(loc)\n",
    "    stats = [(loc+fname, os.stat(loc+fname).st_mtime) for fname in files]\n",
    "    return max(stats, key=lambda x: x[1])[0]\n",
    "\n",
    "def save_embeddings(embed_layer, name, vocab):\n",
    "    logdir = get_last_modified_folder()\n",
    "    with open(os.path.join(logdir, f'{name}_metadata.tsv'), \"w\") as f:\n",
    "        for word in vocab:\n",
    "            f.write(f\"{word}\\n\")\n",
    "    \n",
    "    weights = tf.Variable(embed_layer.get_weights()[0])\n",
    "    checkpoint = tf.train.Checkpoint(embedding=weights)\n",
    "    checkpoint.save(os.path.join(logdir, f\"{name}.ckpt\"))\n",
    "    \n",
    "    config = projector.ProjectorConfig()\n",
    "    embedding = config.embeddings.add()\n",
    "    embedding.tensor_name = f\"{name}/.ATTRIBUTES/TEST_VALUE\"\n",
    "    embedding.metadata_path = f'{name}_metadata.tsv'\n",
    "    projector.visualize_embeddings(logdir, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1105/1105 [==============================] - 29s 23ms/step - loss: 1.4171 - accuracy: 0.4975 - val_loss: 0.8316 - val_accuracy: 0.6834\n",
      "Epoch 2/5\n",
      "1105/1105 [==============================] - 18s 16ms/step - loss: 0.7613 - accuracy: 0.7098 - val_loss: 0.5143 - val_accuracy: 0.8024\n",
      "Epoch 3/5\n",
      "1105/1105 [==============================] - 18s 17ms/step - loss: 0.4084 - accuracy: 0.8552 - val_loss: 0.1634 - val_accuracy: 0.9625\n",
      "Epoch 4/5\n",
      "1105/1105 [==============================] - 18s 17ms/step - loss: 0.1260 - accuracy: 0.9730 - val_loss: 0.0427 - val_accuracy: 0.9948\n",
      "Epoch 5/5\n",
      "1105/1105 [==============================] - 18s 17ms/step - loss: 0.0326 - accuracy: 0.9969 - val_loss: 0.0129 - val_accuracy: 0.9999\n"
     ]
    }
   ],
   "source": [
    "with tf.device('CPU:0'):\n",
    "    model.fit(train_set, epochs=5, validation_data=valid_set, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the embeddings to examine in TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_layer = model.layers[2]\n",
    "save_embeddings(embed_layer, embed_layer.name, input_vocab)\n",
    "\n",
    "embed_layer = model.layers[3]\n",
    "save_embeddings(embed_layer, embed_layer.name, output_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Earlier I was trying to generate predictions by simply passing the encoder input (X) shifted to the decoder. Naturally, this was not working as intended. The approach below does the prediction step by step by changing the decoder inputs at each iteration and using that to generate the next character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predictions(dates, sos_id=len(output_vocab)+1):\n",
    "    \"\"\"Generates predictions, one character at a time. \n",
    "    \n",
    "    We start with passing SoS character to the decoder, generating a prediction,\n",
    "    appending that to decoder input and so on, until MAX_LEN is reached. \n",
    "    \"\"\"\n",
    "    X = encode_data(dates, input_vocab)\n",
    "    y = tf.fill((len(X), 1), sos_id) \n",
    "    for index in range(MAX_OUTPUT_LEN):\n",
    "        pad_size = MAX_OUTPUT_LEN - y.shape[1]\n",
    "        X_decoder = tf.pad(y, [[0, 0], [0, pad_size]])\n",
    "        y_pred_probas = model.predict((X, X_decoder))[:, index:index+1]\n",
    "        y_pred = tf.argmax(y_pred_probas, axis=-1, output_type=tf.int32)\n",
    "        y = tf.concat([y, y_pred], axis=1)\n",
    "    return [decode_sequence(translated, output_vocab) for translated in y[:, 1:].numpy()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actual</th>\n",
       "      <th>translation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>May 20, 2018</td>\n",
       "      <td>2018-05-20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>October 28, 2009</td>\n",
       "      <td>2009-10-28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>June 16, 1936</td>\n",
       "      <td>1936-06-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>September 22, 1954</td>\n",
       "      <td>1954-09-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>June 21, 1984</td>\n",
       "      <td>1984-06-21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>July 06, 1949</td>\n",
       "      <td>1949-07-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>September 08, 1921</td>\n",
       "      <td>1921-09-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>November 23, 1943</td>\n",
       "      <td>1943-11-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>December 08, 1970</td>\n",
       "      <td>1970-12-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>January 21, 1911</td>\n",
       "      <td>1911-01-21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>October 04, 1966</td>\n",
       "      <td>1966-10-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>May 19, 1947</td>\n",
       "      <td>1947-05-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>November 04, 1902</td>\n",
       "      <td>1902-11-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>January 01, 1960</td>\n",
       "      <td>1960-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>May 11, 2017</td>\n",
       "      <td>2017-05-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>December 20, 1951</td>\n",
       "      <td>1951-12-20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>June 18, 1997</td>\n",
       "      <td>1997-06-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>August 08, 1922</td>\n",
       "      <td>1922-08-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>August 21, 1922</td>\n",
       "      <td>1922-08-21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>February 20, 1947</td>\n",
       "      <td>1947-02-20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                actual translation\n",
       "0         May 20, 2018  2018-05-20\n",
       "1     October 28, 2009  2009-10-28\n",
       "2        June 16, 1936  1936-06-16\n",
       "3   September 22, 1954  1954-09-22\n",
       "4        June 21, 1984  1984-06-21\n",
       "5        July 06, 1949  1949-07-06\n",
       "6   September 08, 1921  1921-09-08\n",
       "7    November 23, 1943  1943-11-23\n",
       "8    December 08, 1970  1970-12-08\n",
       "9     January 21, 1911  1911-01-21\n",
       "10    October 04, 1966  1966-10-04\n",
       "11        May 19, 1947  1947-05-19\n",
       "12   November 04, 1902  1902-11-04\n",
       "13    January 01, 1960  1960-01-01\n",
       "14        May 11, 2017  2017-05-11\n",
       "15   December 20, 1951  1951-12-20\n",
       "16       June 18, 1997  1997-06-18\n",
       "17     August 08, 1922  1922-08-08\n",
       "18     August 21, 1922  1922-08-21\n",
       "19   February 20, 1947  1947-02-20"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actual = X[train_size + test_size:]\n",
    "translated = make_predictions(actual)\n",
    "result_df = pd.DataFrame({\"actual\": actual, \"translation\": translated})\n",
    "result_df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results look good! There's an issue however; Because the model was trained on data ranging from years ranging from 1900 to 2021, it does not perform well when given an year out of that range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2014-07-11', '2013-17-12']"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_predictions(['July 18, 1444', 'September 12, 1234'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using GreedyEmbeddingSampler\n",
    "The [notebook](https://github.com/ageron/handson-ml2/blob/master/16_nlp_with_rnns_and_attention.ipynb) points out that by using `GreedyEmbeddingSampler` as sampler, we can speed up prediction time. \n",
    "\n",
    "It does this by calculating the decoder outputs argmax, running the resulting token IDs through the decoder embedding layer then feeding these results to the decoder cell at the next time step. This also eliminates the need for us to pass the shifted decoder inputs as well. \n",
    "\n",
    "Below is an implementation of this model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_sampler = tfa.seq2seq.sampler.GreedyEmbeddingSampler(embedding_fn=decoder_embedding_layer)\n",
    "inference_decoder = tfa.seq2seq.basic_decoder.BasicDecoder(\n",
    "    decoder_cell, inference_sampler, output_layer=output_layer,\n",
    "    maximum_iterations=MAX_OUTPUT_LEN) # Avoids infinite loops\n",
    "\n",
    "batch_size = tf.shape(encoder_inputs)[:1]\n",
    "start_tokens = tf.fill(dims=batch_size, value=sos_id)\n",
    "final_outputs, final_state, final_sequence_lengths = inference_decoder(\n",
    "    start_tokens,\n",
    "    initial_state=encoder_state,\n",
    "    start_tokens=start_tokens,\n",
    "    end_token=0)\n",
    "\n",
    "inference_model = Model(inputs=[encoder_inputs], \n",
    "                        outputs=[final_outputs.sample_id]) # Use final_outputs.rnn_output if we want logits\n",
    "inference_model.compile(loss=\"sparse_categorical_crossentropy\", optimizer='nadam', metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "def speedy_predictions(dates):\n",
    "    X = encode_data(dates, input_vocab)\n",
    "    y = inference_model.predict(X)\n",
    "    return [decode_sequence(translated, output_vocab) for translated in y]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare the two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30.2 s ± 122 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit make_predictions(actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.4 s ± 34 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit speedy_predictions(actual)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "homl",
   "language": "python",
   "name": "homl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
