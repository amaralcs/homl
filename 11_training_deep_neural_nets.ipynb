{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Deep Neural Networks\n",
    "The NNs developed so far have been shallow, with only a few layers. What if we're tackling a much more complex problem, such as detecting hundreds of types of objexts in high-res images?\n",
    "\n",
    "Training deep NNs can be problematic, for example:\n",
    "- You may face the *vanishing/exploding gradients* problem. This is when the gradients grow smaller and small, or larger and larger, when flowing backwards through the DNN during training. This makes it difficult to train lower layers\n",
    "- You might not have enough training data, or it may be too costly to label\n",
    "- Training may be extremely slow\n",
    "- A model with millions of parameters would severely risk overfitting the training set, especially if there's not enough training instances or the dataset is too noisy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The vanishing/exploding gradients problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall the backpropagation algorithm used to train Neural nets. At each step, the gradient often gets smaller and smaller as the algorithm progresses to the lower layers. As a result, the Gradient Descent update leaves the lower layer's connection weights virtually unchanged and training never converges to a good solution. \n",
    "\n",
    "The opposite can also happen, the gradients can grow bigger and bigger until layers get insanely large weight updates and the algorithm diverges. This is the *exploding gradients* problem, which surfaces in recurrent NNs. In general, deep networks suffer from unstable gradients, different layers learn at widely different speeds.\n",
    "\n",
    "In a [2010 paper](https://homl.info/47) the authors found a few suspects to why gradients can be so unstable, including a combination of the popular logistic sigmoid activation function and the weight initialization technique that was popular at the time (normal distribution centered around 0 with deviation of 1). They showed that with this activation function and this initialization scheme, the variance of the outputs of each layer is much greater than the variance of its inputs. Going forward in the network, the variance keeps increasing until the activation function saturates at the top layers. (fig 11-1 on pg 333 exemplifies this)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Glorot and He initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The authors of the paper Xavier Gloror and Yoshua Bengio propose a way to mitigate the unstable gradients problem. They point out that we need the signal to flow in both directions: forwards when making predictions and in the reverse direction when backpropagating gradients. We don't want the signal to die out, not to explode and saturate. They argue that we need the variance of the outputs of each layer to be equal to the variance of the inputs, and we need the gradients to have equal variance before and after flowing through a layer in the reverse direction.\n",
    "\n",
    "It is not actually possible to guarantee both, unless a layer has an equal number of inputs and neurons (these numbers are called *fan-in* and *fan-out* of the layer), but the authors proposed a good compromise: the connection weights of each layer must be initialized randomly as described by the equation below:\n",
    "\n",
    "$$\\text{Normal distributions with mean 0 and variance }\\sigma^2 = \\frac{1}{fan_{\\text{avg}}}$$\n",
    "or\n",
    "$$\\text{Uniform distribution between -r and +r with }r = \\sqrt{\\frac{3}{fan_{\\text{avg}}}}$$\n",
    "\n",
    "where $fan_{\\text{avg}} = (fan_{in} + fan_{out})/2$. This strategy is called *Xavier* or *Glorot initialization*. Using Glorot initialization can speed up training considerably."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we replace $fan_{\\text{avg}}$ with $fan_{\\text{in}}$ we get *LeCun initialization*, which was proposed in the 90s. \n",
    "\n",
    "Some papers have provided different strategies for initialization for various activations functions. They differ only by the scale of the variance and whether they use $fan_{\\text{avg}}$ or $fan_{\\text{in}}$\n",
    "\n",
    "| Initialization | Activation Functions           | $\\sigma^2$ (Normal)     |\n",
    "| -------------- | ------------------------------ | ----------------------- |\n",
    "| Glorot         | None, tanh, logistic, softmax  | 1/$fan_{\\text{avg}}$    |\n",
    "| He             | ReLU and variants              | 2/$fan_{\\text{avg}}$    |\n",
    "| LeCun          | SELU                           | 1/$fan_{\\text{avg}}$    |\n",
    "\n",
    "For the uniform distribution just compute $r=\\sqrt{3\\sigma^2}$. Note that for ReLU and its variants, the initialization is called *He initialization*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, Keras uses Glorot with a uniform distribution. When creating a layer we can pass in the initialization by setting ```kernel_initializer=\"he_uniform\"``` or ```kernel_initializer=\"he_normal\"```, for example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want He initialization with uniform distribution but based on $fan_\\text{avg}$ rather than $fan_\\text{in}$ you can use ```VarianceScaling``` initializer as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.layers.core.Dense at 0x7f3b740e7ee0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "from keras.layers import Dense\n",
    "\n",
    "he_avg_init = keras.initializers.VarianceScaling(scale=2., mode='fan_avg', distribution='uniform')\n",
    "Dense(10, activation='sigmoid', kernel_initializer=he_avg_init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nonsaturating Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ReLU is a great choice of activation function for NNs because it doesn;t saturate for positive values (unlike the sigmoid function) and it is fast to compute. It suffers however, from the *dying ReLU* problem: during training some neurons *'die'* and stop outputting anything other than 0. A neuron dies when its weights get tweaked in such a way that the weighted sum of its inputs are negative for all instances in the training set. When this happens, it just keeps outputting zeros and Gradient Descent does not affect it anymore because the gradient of the ReLU function is zero when its output is negative.\n",
    "\n",
    "A variant of ReLU, *leaky ReLU* can help solve this problem.\n",
    "\n",
    "$$ \\text{LeakyReLU}_a(z) = \\max(\\alpha z, z) $$\n",
    "\n",
    "The $\\alpha$ hyperparameter defines how much 'leaks': it is the slope of the function for z<0 and is typically set to 0.01. This small slope ensures the leaky ReLU never dies; they can go into a ,long coma but they have a chance to eventually wake up.\n",
    "\n",
    "A [2015 paper](https://homl.info/49) compared several variants of the ReLU function and one of its conclusions was that leaky variants alwayas outperformed the strict ReLU. Setting $\\alpha=0.2$ (a huge leak) seemd to result in a better performance than $\\alpha=0.01$ (a small leak). The paper also evaluated *randomized leaky ReLU* (RReLU), where $\\alpha$ is picked randomly in a given range during training and is fixed to an average value during testing. It performemed well and acted as a regularizer. Finally it evaluated *parametric leaky ReLU* (PReLU), where $\\alpha$ is authorized to be learned during training (i.e. becimoing a parameter that can be modified by backpropagation). PReLU was reported to strongly outperform ReLU on large image datasets, but on smaller datasets it runs the risk of overfitting the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In 2015 the [*exponential linear unit (ELU)*](https://homl.info/50) was introduced and outperformed all other ReLU variants in the author's experiments: training time reduced, and the neural network performed better on the test set. \n",
    "\n",
    "$$ \\text{ELU}_\\alpha(z) = \\begin{cases}\n",
    "                          \\alpha(\\exp(z) - 1) & \\text{if } z<0\\\\\n",
    "                          z & \\text{if } z\\geq0\n",
    "                          \\end{cases} $$\n",
    "                          \n",
    "Where $\\alpha$ is the hyperparameter that defines the value the ELU function takes when $z$ is a large negative number. The ELU function looks like the ELU (fig 11-3 on pg 336) with a few major differences:\n",
    "- It takes on negative values when z<0; allowing units to have an average output closer to zero, which alleviates the vanishing gradients problem\n",
    "- It has non-zero gradient for $z<0$, which avoids the dead neurons problem\n",
    "- if $\\alpha=1$ then the function is smooth everywhere, which helps speed up Gradient Descent, since it does not bounce as much\n",
    "\n",
    "The main drawback of ELU is the that it is slower to compute than ReLU and its variants. Its faster convergence rate compensates for that slow computation, but still at test time an ELU network will be slower than a ReLU network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In 2017 the [Scaled ELU (SELU)](https://homl.info/selu) was introduced. The authors showed that if you build a neural network composed exclusively of a stack of dense layers, and if all hidden layers use the SELU function, then the network will *self-normalize*: the output of each layer will tend to preserve a mean of 0 and standard deviation of 1 during training. As a result SELU significantly outperforms other activation functions. However there are certain conditions for self normalization to happen:\n",
    "- Input features must be standardized\n",
    "- Every hidden layer myst be initialized with LeCun normal initialization\n",
    "- The network's architecture must be sequential*\n",
    "- The paper only guarantees self-normalization if all layers are dense, but some researchers have noted that the SELU activation function can improve performance in convolutional neural nets as well\n",
    "\n",
    "Note: For non-sequential architectures such as recurrent networks or networks with *skip-connections*, self-normalization is not guaranteed, however some researchers noted SELU to perform well in convolutional Neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general \n",
    "$$ \\text{SELU} > \\text{ELU} >\\text{leaky ReELU (and variants)} > \\text{ReLU} > \\text{tanh} > \\text{sigmoid}$$\n",
    "\n",
    "Architecture might prevent you from using SELU, in which case you switch to ELU. If you care about runtime latency then use leaky ReLU instead. If you don't want to tweak $\\alpha$, use the keras defaults. If you have spare time and computing power, use cross validation to evaluate other activation functions such as RReLU and PReLU. that said, because ReLU is the most common function, many libraries and hardware accelerators provide ReLU-specific optimizations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use the leaky ReLU function, create leaky ReLU layer and add it to model just after the layer you want to apply to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(10, kernel_initializer='he_normal'),\n",
    "    keras.layers.LeakyReLU(alpha=0.2)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For PReLU replace LeakyReLU with ```PReLU()```. There's currently no implementation of RReLU in keras but you can easily implement your own.\n",
    "\n",
    "For SELU set ```activation='selu'``` and ```kernel_initizalizer='lecun_normal'``` when creating a layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While He normalization along with ELU (and ReLU variants) can help with the exploding gradients problem at the beginning of training, it doesn't guarantee it won't come back during trainig. [*Batch normalization*](https://homl.info/51) was introduced in 2017 to address these problems.\n",
    "\n",
    "It consists of zero-centering and normalizing each input, then scaling and shifting the results using two neu parameter vectors per layer; one for scaling and the other for shifting. This way the model is allowed to learn the optimal scale and mean of each of the layer's inputs.\n",
    "In many cases, adding a BN layer as the very first input means you don't need to standardize your training set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm computes the mean and standard deviation of the input over the current mini-batch. The operation is summarized below\n",
    "\n",
    "1. $$\\boldsymbol{\\mu}_B = \\frac{1}{m_B}\\sum_{i=1}^{m_B}\\textbf{x}^{(i)} $$\n",
    "2. $$\\boldsymbol{\\sigma}^2 = \\frac{1}{m_B}\\sum_{i=1}^{m_B}(\\textbf{x}^{(i)} - \\mu_B)^2 $$\n",
    "3. $$\\hat{\\textbf{x}}^{(i)} = \\frac{\\textbf{x}^{(i)} - \\boldsymbol{\\mu_B}}{\\sqrt{\\boldsymbol{\\sigma}^2+\\epsilon}} $$\n",
    "4. $$\\textbf{z}^{(i)} = \\boldsymbol{\\gamma}\\otimes\\hat{\\textbf{x}}^{(i)}+ \\boldsymbol{\\beta}$$\n",
    "\n",
    "Where \n",
    "- $\\boldsymbol{\\mu}_B$ is the vector of input means, evaluated over the whole mini-batch $B$\n",
    "- $\\boldsymbol\\sigma_B$ is the vector of input standard deviations over mini-batch $B$\n",
    "- $m_B$ is the number of instances in the mini batch\n",
    "- $\\hat{\\textbf{x}}^{(i)}$ is the vector of zero centered and normalized inputs for instance $i$\n",
    "- $\\boldsymbol\\gamma$ is the output scale parameter vector for the layer\n",
    "- $\\otimes$ is element-wise multiplication (each input is multiplied by its correspoding scale parameter)\n",
    "- $\\boldsymbol\\beta$ is the output shift parameter vector for the layer. Each input is offser by its corresponding shift parameter\n",
    "- $\\epsilon$ is a tiny number that avoids division by zero, called a *smoothing term*\n",
    "- $\\textbf{z}^{(i)}$ is the output of the Batch Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might ask *'but what mean and deviation do I use at test time?'*. you might have only one test instance or even if we have a test batch, the samples might not be I.I.D.. \n",
    "\n",
    "One solution would be to wait until end of training, then run the whole training set through the NN to compute the mean and deviation of each input of the BN layer. These \"final\" input means and deviations could then be used instead of the batch means/deviation when making predictions. \n",
    "\n",
    "However, most implementations of BN, estimate these final statistics by using a moving average of the layer's input means and standard deviations. Keras does this automatically.\n",
    "\n",
    "Batch Normalization also acts as a regularizer, reducing the needs for other normalization techniques. It does however, add some complexity to the model. It also makes slower predictions due to the extra computations required at each layer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batch Normalization with keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The folllowing model implements a BN layer after every hidden layer and as the first layer in the model (after flattening the input images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 784)               3136      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 300)               235500    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 300)               1200      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 100)               30100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 271,346\n",
      "Trainable params: 268,978\n",
      "Non-trainable params: 2,368\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten, BatchNormalization\n",
    "\n",
    "model = Sequential([\n",
    "    Flatten(input_shape=[28, 28]),\n",
    "    BatchNormalization(),\n",
    "    Dense(300, activation='elu', kernel_initializer='he_normal'),\n",
    "    BatchNormalization(),\n",
    "    Dense(100, activation='elu', kernel_initializer='he_normal'),\n",
    "    BatchNormalization(),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For such a small model, it is unlikely BN will have a very positive impact, but for deeper networks it can make a tremendous difference. \n",
    "\n",
    "For each layer, BN adds 4 parameters per input: $\\boldsymbol{\\gamma, \\beta, \\mu, \\sigma}$. E.g. the first BN layer adds 3,136 parameters, which is $4\\times784$. Since $\\boldsymbol{\\mu, \\sigma}$ are the moving averages, they are not affected by backpropagation, so Keras calls them \"non-trainable\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The authors of the BN paper argued in favor of adding the BN layes before the activation functions, rather than after (as we did). It is a good idea to experiment to see which option works best for your data. To do this, we need to remove the activation function from the hidden layers. Moreover, since a  BN layer includes one offset paramete per input, we can remove the bias term from the previous layer passing ```use_bias=False```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_2 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 784)               3136      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 300)               235200    \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 300)               1200      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 100)               30000     \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 270,946\n",
      "Trainable params: 268,578\n",
      "Non-trainable params: 2,368\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Activation\n",
    "\n",
    "model = Sequential([\n",
    "    Flatten(input_shape=[28, 28]),\n",
    "    BatchNormalization(),\n",
    "    Dense(300, kernel_initializer='he_normal', use_bias=False),\n",
    "    BatchNormalization(),\n",
    "    Activation('elu'),\n",
    "    Dense(100, activation='elu', kernel_initializer='he_normal', use_bias=False),\n",
    "    BatchNormalization(),\n",
    "    Activation('elu'),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually the default values of the BN layer hyperparameters are fine, but you may ocasionally need to tweak the ```momentum```. Momentum is used by the BN layer when it updates the exponential moving averages, given a new value $\\textbf{v}$, the layer updates the running average $\\hat{\\textbf{v}}$ using \n",
    "$$ \\hat{\\textbf{v}} \\leftarrow \\hat{\\textbf{v}} \\times \\text{momentum} + \\textbf{v}\\times (1 - \\text{momentum})$$\n",
    "Typical values are close to 1: 0.9, 0.99, 0.999 (adding more 9s for larger datasets and smaller mini-batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another important hyperparameter is the ```axis```. It determines which axis should be normalized, with default -1, i.e. normalizing the last axis. \n",
    "\n",
    "When the input batch is 2D (i.e. batch shape is [batch size, features]) this means each input feature will be normalized based on the mean and standard deviation computed across all the instances in the batch. For ecample the first BN layer in the previous example will independetly normalize, rescale and shift each of the 784 input features.\n",
    "\n",
    "If we move the first BN layer before the Flatten layer, then the input batches will be 3D with shape [batch size, height, width]: thus the BN layer will comput 28 means and 28 standard deviations and will normalize, rescale and shift all pixels in a given column using the same mean and standard deviation. If instead you want to treat each of the 784 pixels independently you should set ```axis=[1,2]```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BN is so popular that it is often omitted in model diagrams as it is assumed BN is added after every layer. A recent [paper](https://homl.info/fixup) however, used a novel *fixed-update* weight initialization technique to train a very deep neural network (10,000 layers) withouth BN. This is bleeding edge research, so wait for additional results before dropping BN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Clipping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another [tecnique](https://homl.info) for mitigating the exploding gradient problem. It is most often used in recurrent neural nets, as Batch Normalization is tricky to use in RNNs. In Keras, adding Gradient Clipping is simple a matter of setting the ```clipvalue``` or ```clipnorm``` argument when creating an optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.SGD(clipvalue=1.0)\n",
    "model.compile(loss='mse', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This optimizer clips every component of the gradient vector to a value between -1 and 1. The threshold is a hyperparameter you can tune. \n",
    "\n",
    "Note it may change the orientation of the gradient vector. For instance, if the original gradient vector is [0.9, 100], it points mostly in the direction of the second axis. Hoerver clipping it gives [0.9, 1.0] which points roughly in the diagonal of the two axes. If you want to ensure clipping doesn't change direction of the gradient vector, you should clip by norm by setting ```clipnorm```. This will clip the whole gradient if its $l_2$ norm is greater than the threshold you picked.\n",
    "For example, with ```clipnorm=1.0``` the vector [0.9, 100] becomes [0.00899964, 0.9999595] preserving orientation but almost eliminating the first component. You cant track the size of gradients using TensorBoard and you may want to try both clipping by value and norm with different thresholds to see which option performs best on the validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reusing Pretrained Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because very deep Neural nets take so long to train and run, it is often a good idea to find one that was built for a problem similar to yours and re-use it. This technique is called *transfer learning* and it speeds up training and requires less data. See diagram on pg. 346"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use Fashion MNIST as an example: Suppose the dataset only contained eight classes (e.g. all but sandal and shirt). Someone built a Keras model that achieved good performance (>90% accuracy). Call this model A.\n",
    "\n",
    "Our task is to train a binary classifier that differentiates between sandals and shirts (positive=shirt, negative=sandal). We only have 200 labeled images. We train a model (call it model B) and we get 97.2% accuracy. \n",
    "\n",
    "We then realise the two tasks are quite similar and we can use transfer learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
    "X_train_full = X_train_full / 255.0\n",
    "X_test = X_test / 255.0\n",
    "X_valid, X_train = X_train_full[:5000], X_train_full[5000:]\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def split_dataset(X, y):\n",
    "    y_5_or_6 = (y == 5) | (y == 6) # sandals or shirts\n",
    "    y_A = y[~y_5_or_6]\n",
    "    y_A[y_A > 6] -= 2 # class indices 7, 8, 9 should be moved to 5, 6, 7\n",
    "    y_B = (y[y_5_or_6] == 6).astype(np.float32) # binary classification task: is it a shirt (class 6)?\n",
    "    return ((X[~y_5_or_6], y_A),\n",
    "            (X[y_5_or_6], y_B))\n",
    "\n",
    "(X_train_A, y_train_A), (X_train_B, y_train_B) = split_dataset(X_train, y_train)\n",
    "(X_valid_A, y_valid_A), (X_valid_B, y_valid_B) = split_dataset(X_valid, y_valid)\n",
    "(X_test_A, y_test_A), (X_test_B, y_test_B) = split_dataset(X_test, y_test)\n",
    "X_train_B = X_train_B[:200]\n",
    "y_train_B = y_train_B[:200]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train model A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 43986 samples, validate on 4014 samples\n",
      "Epoch 1/20\n",
      "43986/43986 [==============================] - 5s 108us/step - loss: 0.5678 - accuracy: 0.8144 - val_loss: 0.3796 - val_accuracy: 0.8717\n",
      "Epoch 2/20\n",
      "43986/43986 [==============================] - 4s 100us/step - loss: 0.3538 - accuracy: 0.8798 - val_loss: 0.3218 - val_accuracy: 0.8951\n",
      "Epoch 3/20\n",
      "43986/43986 [==============================] - 5s 105us/step - loss: 0.3147 - accuracy: 0.8919 - val_loss: 0.3006 - val_accuracy: 0.9016\n",
      "Epoch 4/20\n",
      "43986/43986 [==============================] - 5s 105us/step - loss: 0.2947 - accuracy: 0.8989 - val_loss: 0.2885 - val_accuracy: 0.9033\n",
      "Epoch 5/20\n",
      "43986/43986 [==============================] - 4s 101us/step - loss: 0.2817 - accuracy: 0.9028 - val_loss: 0.2784 - val_accuracy: 0.9061\n",
      "Epoch 6/20\n",
      "43986/43986 [==============================] - 4s 95us/step - loss: 0.2720 - accuracy: 0.9071 - val_loss: 0.2692 - val_accuracy: 0.9096\n",
      "Epoch 7/20\n",
      "43986/43986 [==============================] - 4s 99us/step - loss: 0.2644 - accuracy: 0.9100 - val_loss: 0.2655 - val_accuracy: 0.9096\n",
      "Epoch 8/20\n",
      "43986/43986 [==============================] - 5s 103us/step - loss: 0.2581 - accuracy: 0.9115 - val_loss: 0.2589 - val_accuracy: 0.9150\n",
      "Epoch 9/20\n",
      "43986/43986 [==============================] - 5s 102us/step - loss: 0.2526 - accuracy: 0.9136 - val_loss: 0.2588 - val_accuracy: 0.9126\n",
      "Epoch 10/20\n",
      "43986/43986 [==============================] - 4s 97us/step - loss: 0.2481 - accuracy: 0.9154 - val_loss: 0.2558 - val_accuracy: 0.9145\n",
      "Epoch 11/20\n",
      "43986/43986 [==============================] - 4s 102us/step - loss: 0.2439 - accuracy: 0.9171 - val_loss: 0.2504 - val_accuracy: 0.9158\n",
      "Epoch 12/20\n",
      "43986/43986 [==============================] - 4s 98us/step - loss: 0.2401 - accuracy: 0.9185 - val_loss: 0.2622 - val_accuracy: 0.9056\n",
      "Epoch 13/20\n",
      "43986/43986 [==============================] - 4s 95us/step - loss: 0.2370 - accuracy: 0.9196 - val_loss: 0.2501 - val_accuracy: 0.9153\n",
      "Epoch 14/20\n",
      "43986/43986 [==============================] - 4s 95us/step - loss: 0.2335 - accuracy: 0.9206 - val_loss: 0.2451 - val_accuracy: 0.9175\n",
      "Epoch 15/20\n",
      "43986/43986 [==============================] - 4s 96us/step - loss: 0.2306 - accuracy: 0.9216 - val_loss: 0.2453 - val_accuracy: 0.9173\n",
      "Epoch 16/20\n",
      "43986/43986 [==============================] - 4s 96us/step - loss: 0.2279 - accuracy: 0.9218 - val_loss: 0.2439 - val_accuracy: 0.9138\n",
      "Epoch 17/20\n",
      "43986/43986 [==============================] - 4s 100us/step - loss: 0.2253 - accuracy: 0.9231 - val_loss: 0.2384 - val_accuracy: 0.9183\n",
      "Epoch 18/20\n",
      "43986/43986 [==============================] - 4s 97us/step - loss: 0.2231 - accuracy: 0.9239 - val_loss: 0.2355 - val_accuracy: 0.9183\n",
      "Epoch 19/20\n",
      "43986/43986 [==============================] - 4s 98us/step - loss: 0.2203 - accuracy: 0.9248 - val_loss: 0.2444 - val_accuracy: 0.9123\n",
      "Epoch 20/20\n",
      "43986/43986 [==============================] - 4s 96us/step - loss: 0.2179 - accuracy: 0.9255 - val_loss: 0.2398 - val_accuracy: 0.9170\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "model_A = keras.models.Sequential()\n",
    "model_A.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
    "for n_hidden in (300, 100, 50, 50, 50):\n",
    "    model_A.add(keras.layers.Dense(n_hidden, activation=\"selu\"))\n",
    "model_A.add(keras.layers.Dense(8, activation=\"softmax\"))\n",
    "\n",
    "model_A.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "                optimizer=keras.optimizers.SGD(lr=1e-3),\n",
    "                metrics=[\"accuracy\"])\n",
    "\n",
    "history = model_A.fit(X_train_A, y_train_A, epochs=20,\n",
    "                    validation_data=(X_valid_A, y_valid_A))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_A.save('saved_models/11_training_dnns/model_A.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we prepare model B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 200 samples, validate on 986 samples\n",
      "Epoch 1/20\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.5307 - accuracy: 0.7350 - val_loss: 0.4757 - val_accuracy: 0.7921\n",
      "Epoch 2/20\n",
      "200/200 [==============================] - 0s 412us/step - loss: 0.4328 - accuracy: 0.8200 - val_loss: 0.4016 - val_accuracy: 0.8458\n",
      "Epoch 3/20\n",
      "200/200 [==============================] - 0s 412us/step - loss: 0.3612 - accuracy: 0.8750 - val_loss: 0.3459 - val_accuracy: 0.8915\n",
      "Epoch 4/20\n",
      "200/200 [==============================] - 0s 343us/step - loss: 0.3075 - accuracy: 0.9200 - val_loss: 0.3030 - val_accuracy: 0.9118\n",
      "Epoch 5/20\n",
      "200/200 [==============================] - 0s 350us/step - loss: 0.2669 - accuracy: 0.9500 - val_loss: 0.2698 - val_accuracy: 0.9320\n",
      "Epoch 6/20\n",
      "200/200 [==============================] - 0s 393us/step - loss: 0.2357 - accuracy: 0.9500 - val_loss: 0.2429 - val_accuracy: 0.9473\n",
      "Epoch 7/20\n",
      "200/200 [==============================] - 0s 376us/step - loss: 0.2099 - accuracy: 0.9500 - val_loss: 0.2220 - val_accuracy: 0.9564\n",
      "Epoch 8/20\n",
      "200/200 [==============================] - 0s 381us/step - loss: 0.1899 - accuracy: 0.9500 - val_loss: 0.2028 - val_accuracy: 0.9594\n",
      "Epoch 9/20\n",
      "200/200 [==============================] - 0s 429us/step - loss: 0.1708 - accuracy: 0.9650 - val_loss: 0.1874 - val_accuracy: 0.9655\n",
      "Epoch 10/20\n",
      "200/200 [==============================] - 0s 339us/step - loss: 0.1565 - accuracy: 0.9650 - val_loss: 0.1741 - val_accuracy: 0.9665\n",
      "Epoch 11/20\n",
      "200/200 [==============================] - 0s 491us/step - loss: 0.1440 - accuracy: 0.9700 - val_loss: 0.1636 - val_accuracy: 0.9665\n",
      "Epoch 12/20\n",
      "200/200 [==============================] - 0s 382us/step - loss: 0.1336 - accuracy: 0.9750 - val_loss: 0.1544 - val_accuracy: 0.9706\n",
      "Epoch 13/20\n",
      "200/200 [==============================] - 0s 361us/step - loss: 0.1249 - accuracy: 0.9800 - val_loss: 0.1460 - val_accuracy: 0.9726\n",
      "Epoch 14/20\n",
      "200/200 [==============================] - 0s 375us/step - loss: 0.1168 - accuracy: 0.9850 - val_loss: 0.1382 - val_accuracy: 0.9746\n",
      "Epoch 15/20\n",
      "200/200 [==============================] - 0s 327us/step - loss: 0.1096 - accuracy: 0.9800 - val_loss: 0.1322 - val_accuracy: 0.9767\n",
      "Epoch 16/20\n",
      "200/200 [==============================] - 0s 307us/step - loss: 0.1035 - accuracy: 0.9850 - val_loss: 0.1264 - val_accuracy: 0.9787\n",
      "Epoch 17/20\n",
      "200/200 [==============================] - 0s 306us/step - loss: 0.0980 - accuracy: 0.9850 - val_loss: 0.1216 - val_accuracy: 0.9797\n",
      "Epoch 18/20\n",
      "200/200 [==============================] - 0s 303us/step - loss: 0.0933 - accuracy: 0.9850 - val_loss: 0.1170 - val_accuracy: 0.9777\n",
      "Epoch 19/20\n",
      "200/200 [==============================] - 0s 342us/step - loss: 0.0885 - accuracy: 0.9850 - val_loss: 0.1129 - val_accuracy: 0.9787\n",
      "Epoch 20/20\n",
      "200/200 [==============================] - 0s 374us/step - loss: 0.0846 - accuracy: 0.9850 - val_loss: 0.1091 - val_accuracy: 0.9797\n"
     ]
    }
   ],
   "source": [
    "model_B = keras.models.Sequential()\n",
    "model_B.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
    "for n_hidden in (300, 100, 50, 50, 50):\n",
    "    model_B.add(keras.layers.Dense(n_hidden, activation=\"selu\"))\n",
    "model_B.add(keras.layers.Dense(1, activation=\"sigmoid\"))\n",
    "model_B.compile(loss=\"binary_crossentropy\",\n",
    "                optimizer=keras.optimizers.SGD(lr=1e-3),\n",
    "                metrics=[\"accuracy\"])\n",
    "history = model_B.fit(X_train_B, y_train_B, epochs=20,\n",
    "                      validation_data=(X_valid_B, y_valid_B))\n",
    "model_B.save('saved_models/11_training_dnns/model_B.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - 0s 40us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.10593942695856094, 0.9825000166893005]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_B.evaluate(X_test_B, y_test_B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To re-use model A, we need to load it and create a new model based on that model's layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_A = keras.models.load_model('saved_models/11_training_dnns/model_A.h5')\n",
    "model_B_on_A = keras.models.Sequential(model_A.layers[:-1])\n",
    "model_B_on_A.add(keras.layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that now model_A and model_B_on_A will share layers, so training model_B_on_A will affect model_A. We need to clone model_A and its weights before re-using its layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_A_clone = keras.models.clone_model(model_A)\n",
    "model_A_clone.set_weights(model_A.get_weights())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the output layer for model_B_on_A was randomly initialized, training it now would cause it to make large errors and wreck the pre-trained weights. To avoid this, we'll freeze the reused layers for a few epochs giving the output layer time to learn reasonable weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze layers and re-compile\n",
    "for layer in model_B_on_A.layers[:-1]:\n",
    "    layer.trainable = False\n",
    "    \n",
    "model_B_on_A.compile(loss='binary_crossentropy', optimizer='SGD', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 200 samples, validate on 986 samples\n",
      "Epoch 1/4\n",
      "200/200 [==============================] - 0s 791us/step - loss: 1.4455 - accuracy: 0.1600 - val_loss: 0.7714 - val_accuracy: 0.4878\n",
      "Epoch 2/4\n",
      "200/200 [==============================] - 0s 359us/step - loss: 0.5890 - accuracy: 0.6950 - val_loss: 0.3846 - val_accuracy: 0.8935\n",
      "Epoch 3/4\n",
      "200/200 [==============================] - 0s 422us/step - loss: 0.3126 - accuracy: 0.9250 - val_loss: 0.2510 - val_accuracy: 0.9574\n",
      "Epoch 4/4\n",
      "200/200 [==============================] - 0s 297us/step - loss: 0.2088 - accuracy: 0.9700 - val_loss: 0.1914 - val_accuracy: 0.9726\n"
     ]
    }
   ],
   "source": [
    "# Train for a few epochs\n",
    "history = model_B_on_A.fit(X_train_B, y_train_B, epochs=4, \n",
    "                           validation_data=(X_valid_B, y_valid_B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 200 samples, validate on 986 samples\n",
      "Epoch 1/16\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1728 - accuracy: 0.9850 - val_loss: 0.1887 - val_accuracy: 0.9736\n",
      "Epoch 2/16\n",
      "200/200 [==============================] - 0s 395us/step - loss: 0.1700 - accuracy: 0.9850 - val_loss: 0.1861 - val_accuracy: 0.9736\n",
      "Epoch 3/16\n",
      "200/200 [==============================] - 0s 373us/step - loss: 0.1673 - accuracy: 0.9850 - val_loss: 0.1833 - val_accuracy: 0.9736\n",
      "Epoch 4/16\n",
      "200/200 [==============================] - 0s 468us/step - loss: 0.1645 - accuracy: 0.9900 - val_loss: 0.1807 - val_accuracy: 0.9746\n",
      "Epoch 5/16\n",
      "200/200 [==============================] - 0s 470us/step - loss: 0.1619 - accuracy: 0.9900 - val_loss: 0.1783 - val_accuracy: 0.9746\n",
      "Epoch 6/16\n",
      "200/200 [==============================] - 0s 379us/step - loss: 0.1594 - accuracy: 0.9900 - val_loss: 0.1759 - val_accuracy: 0.9746\n",
      "Epoch 7/16\n",
      "200/200 [==============================] - 0s 359us/step - loss: 0.1570 - accuracy: 0.9900 - val_loss: 0.1736 - val_accuracy: 0.9746\n",
      "Epoch 8/16\n",
      "200/200 [==============================] - 0s 418us/step - loss: 0.1546 - accuracy: 0.9900 - val_loss: 0.1712 - val_accuracy: 0.9757\n",
      "Epoch 9/16\n",
      "200/200 [==============================] - 0s 408us/step - loss: 0.1522 - accuracy: 0.9900 - val_loss: 0.1689 - val_accuracy: 0.9767\n",
      "Epoch 10/16\n",
      "200/200 [==============================] - 0s 350us/step - loss: 0.1499 - accuracy: 0.9900 - val_loss: 0.1668 - val_accuracy: 0.9767\n",
      "Epoch 11/16\n",
      "200/200 [==============================] - 0s 366us/step - loss: 0.1478 - accuracy: 0.9900 - val_loss: 0.1647 - val_accuracy: 0.9767\n",
      "Epoch 12/16\n",
      "200/200 [==============================] - 0s 372us/step - loss: 0.1456 - accuracy: 0.9900 - val_loss: 0.1627 - val_accuracy: 0.9767\n",
      "Epoch 13/16\n",
      "200/200 [==============================] - 0s 377us/step - loss: 0.1436 - accuracy: 0.9900 - val_loss: 0.1607 - val_accuracy: 0.9767\n",
      "Epoch 14/16\n",
      "200/200 [==============================] - 0s 367us/step - loss: 0.1416 - accuracy: 0.9900 - val_loss: 0.1588 - val_accuracy: 0.9777\n",
      "Epoch 15/16\n",
      "200/200 [==============================] - 0s 365us/step - loss: 0.1397 - accuracy: 0.9900 - val_loss: 0.1570 - val_accuracy: 0.9777\n",
      "Epoch 16/16\n",
      "200/200 [==============================] - 0s 573us/step - loss: 0.1380 - accuracy: 0.9900 - val_loss: 0.1553 - val_accuracy: 0.9777\n"
     ]
    }
   ],
   "source": [
    "# Unfreeze the layers\n",
    "for layer in model_B_on_A.layers[:-1]:\n",
    "    layer.trainable = True\n",
    "    \n",
    "# Freezing/unfreezing requires the model to be re-compiled\n",
    "optimizer = keras.optimizers.SGD(lr=1e-4)\n",
    "model_B_on_A.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "history = model_B_on_A.fit(X_train_B, y_train_B, epochs=16, \n",
    "                           validation_data=(X_valid_B, y_valid_B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - 0s 53us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.15868764865398408, 0.972000002861023]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_B_on_A.evaluate(X_test_B, y_test_B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results are not as good. Why? Transfer learning does not work very well with small dense networks, presumably because they learn very few patterns.\n",
    "\n",
    "Transfer learning works best with Deep Convolutional Neural Networks which tend to learn feature detectors that are much more general. Transfer learning will be revisited in chapter 14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsupervised pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The books explains this concept on page 349. It talks about not having enough labelled data for a supervised problem and using other DNNs such as Autoencoders or GANs to pre-train a model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretraining on Auxiliary task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another alternative for when we don't have enough labeled training data, is to train a first neural net on an auxiliary task for which we can obtain labeled data. Explanation on pg 350"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Faster Optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We present some popular algorithms for optimizers that are faster than SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Momentum Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that Gradient Descent updates the weights by subtracting the gradient of the cost function with regards to the weights multiplied by the learning rate. It does not care about what previous gradients were.\n",
    "\n",
    "$$ \\boldsymbol{\\theta} \\leftarrow \\boldsymbol{\\theta} - \\eta \\nabla_{\\boldsymbol{\\theta}}J(\\boldsymbol{\\theta}) \\leftarrow \\boldsymbol{\\theta} - \\eta \\nabla_{\\boldsymbol{\\theta}}J(\\boldsymbol{\\theta}) $$\n",
    "\n",
    "In [momentum optimization](https://homl.info/54), we introduct the *momentum vector* $\\textbf{m}$ and *momentum hyperparameter* $\\beta$ which takes into account previous gradients. At each iteration, the local gradient is subtracted from $\\textbf{m}$ and updates the weights accordingly.  $\\beta$ stops the momentum from growing too large, and is set between 0(high friction) and 1(no friction).\n",
    "\n",
    "1. $$ \\textbf{m} \\leftarrow \\beta\\textbf{m} - \\eta \\nabla_{\\boldsymbol{\\theta}}J(\\boldsymbol{\\theta}) $$\n",
    "2. $$ \\boldsymbol{\\theta} \\leftarrow \\boldsymbol{\\theta} + \\textbf{m} $$\n",
    "\n",
    "Note: Recall that the idea behind GD is to take steps towards the bottom of a hill. For momentum optimization imagine a ball rolling down the hill instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In deep Neural nets that don't use Batch Normalization, upper layers have inputs with very different scales so using momentum optimization helps a lot.\n",
    "\n",
    "In keras we can use the SGD optimizer and set its ```momentum``` hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.SGD(lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The one drawback of Momentum optmization is that it add yet another hyperparameter to be tuned. In practice, ```momentum=0.9``` usually works well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nesterov Accelerated Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A slight variant of momentum optimization. The [Nesterov Accelerated Gradient (NAG)](https://homl.info/55), measures the gradient of the cost function not a the local position $\\boldsymbol{\\theta}$ but slightly ahead in the direction of the momentum at $\\boldsymbol{\\theta} + \\beta\\textbf{m}$\n",
    "\n",
    "1. $$ \\textbf{m} \\leftarrow \\beta\\textbf{m} - \\eta \\nabla_{\\boldsymbol{\\theta}}J(\\boldsymbol{\\theta} + \\beta\\textbf{m}) $$\n",
    "2. $$ \\boldsymbol{\\theta} \\leftarrow \\boldsymbol{\\theta} + \\textbf{m} $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This small tweak works because in general the momentum vector will be pointing in the right direction (towards optimum). By using the momentum a bit farther ahead, our push will be slightly more accurate. (see Figure 11-6 pg 343). \n",
    "\n",
    "After many iterations, the slight improvements add up and NAG becomes significantly faster than regular momentum optimization. Moreover, when the momentum pushes weights across a valley, NAG pushes back towards the bottom of the valley, helping reducing oscillation and thus it converges faster.\n",
    "\n",
    "With keras, we simply add the ```nesterov=True``` hyperparameter\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.SGD(lr=0.001, momentum=0.9, nesterov=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaGrad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient descent points towards the steepest slope (in an elongated bowl), which does not point down to the global optimum. \n",
    "\n",
    "[AdaGrad](https://homl.info/56) corrects the direction of descent  to point a bit more towards the global optimum by scaling down the gradient vector along the steepest dimensions.\n",
    "\n",
    "1. $$ \\textbf{s} \\leftarrow \\textbf{s} + \\nabla_{\\boldsymbol{\\theta}}J(\\boldsymbol{\\theta})\\otimes \\nabla_{\\boldsymbol{\\theta}}J(\\boldsymbol{\\theta})$$\n",
    "2. $$ \\boldsymbol{\\theta} \\leftarrow \\boldsymbol{\\theta} - \\eta\\nabla_{\\boldsymbol{\\theta}}J(\\boldsymbol{\\theta})\\oslash \\sqrt{\\textbf{s} + \\epsilon} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1 accumulates the square of the gradients into vector $\\textbf{s}$ (recall $\\otimes$ is element-wise multiplication). \n",
    "\n",
    "Step 2 scales the gradient vector by a factor of $\\sqrt{\\textbf{s} + \\epsilon}$ ($\\otimes$ represents element-wise division).\n",
    "\n",
    "In short the algorithm decays the learning rate, doing it faster for steeper dimensions than for dimensions with gentler slopes. This is called *adaptive learning rate*. An additional benefit is that it requires less tuning of the learning rate $\\eta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adagrad works well for simple quadratic problems, but often stops too early when training neural nets. While keras has an Adagrad optimizer, it is not recommended to use it for training deep neural networks (it may be efficient for simpler tasks such as Linear Regression though)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RMSProp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RMSProp works similarly to AdaGrad but it only accumulates the gradients from the most recent iterations of training. It does so by using exponential decay in the first step.\n",
    "\n",
    "1. $$ \\textbf{s} \\leftarrow \\beta\\textbf{s} + (1-\\beta)\\nabla_{\\boldsymbol{\\theta}}J(\\boldsymbol{\\theta})\\otimes \\nabla_{\\boldsymbol{\\theta}}J(\\boldsymbol{\\theta})$$\n",
    "2. $$ \\boldsymbol{\\theta} \\leftarrow \\boldsymbol{\\theta} - \\eta\\nabla_{\\boldsymbol{\\theta}}J(\\boldsymbol{\\theta})\\oslash \\sqrt{\\textbf{s} + \\epsilon} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A typical value for decay ($\\beta$) is 0.9 and this default usually works well. Except for very simple problems, this optimizer is faster than AdaGrad. Keras has the ```RMSprop``` optimizer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.RMSprop(lr=0.001, rho=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adam and Nadam Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Adam](https://homl.info/59) stands for *Adaptive moment estimation* and combines ideas of momentum optimization and RMSprop: It keeps track of an exponentially decaying average of past gradients; and keeps track of an exponentially decaying average of past squared gradients\n",
    "\n",
    "1. $$ \\textbf{m} \\leftarrow \\beta_1\\textbf{m} - (1-\\beta_1)\\nabla_{\\boldsymbol{\\theta}}J(\\boldsymbol{\\theta}) $$\n",
    "2. $$ \\textbf{s} \\leftarrow \\beta_2\\textbf{s} + (1-\\beta_2)\\nabla_{\\boldsymbol{\\theta}}J(\\boldsymbol{\\theta})\\otimes \\nabla_{\\boldsymbol{\\theta}}J(\\boldsymbol{\\theta})$$\n",
    "3. $$\\hat{\\textbf{m}} \\leftarrow \\frac{\\textbf{m}}{1 - \\beta_2^{t}}$$\n",
    "4. $$ \\hat{\\textbf{s}} \\leftarrow \\frac{s}{1-\\beta_2^{t}} $$\n",
    "5. $$ \\boldsymbol{\\theta} \\leftarrow \\boldsymbol{\\theta} + \\eta\\hat{\\textbf{m}}\\oslash\\sqrt{\\hat{\\textbf{s}} + \\epsilon} $$\n",
    "\n",
    "In this equation, $t$ represents the iteration number (starting at 1).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps 1, 2 and 5 are familiar and look like momentum optimization and RMSprop, with the only difference being that step 1 computes an exponentially decaying average, rather than an exponentially moving sum.\n",
    "\n",
    "Steps 3 and 4 are a technical detail: since **m** and **s** are intialized at 0, they will be biased toward 0 at the beginning of training, so these steps help boost **m** and **s**. \n",
    "\n",
    "The momentum decay hyperparameter $\\beta_1$ is typically initialized at 0.9, while the scaling decay hyperparameter $\\beta_2$ is often initialized to 0.999."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Adam(lr=0.01, beta_1=0.9, beta_2=0.999)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since Adam it is an adaptive learning algorithm, it requires less tuning of the learning rate $\\eta$. Using the value of $\\eta=0.001$ is generally a good choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One modification of Adam, is *Adamax*, which is described in pg 357. Adam is typically better than Adamax, so you can try if you experience problems with Adam. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nadam is Adam optimization + the Nesterov trick. It often converges slightly faster than Adam. The paper that introduced it, it was found that Nadam generally outperforms Adam, but is sometimes outperformed by RMSprop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Adaptive optimization methods are great and often converge fast. However a [2017 paper](https://homl.info/60) showed they can lead to solutions that generalize poorly on some datasets. If this is the case for you, try using NAG instead. Also, keep an eye on the latest research as it is moving fast."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See note on pg 358 on Jacobians and Hessians.\n",
    "\n",
    "Pg 359 for Note on Sparse models and a round-up of optimizers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Rate Scheduling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A good learning rate is very important. Set it too high, training may diverge. Set it too low, converge will happen, but it will take a very long time. Learning Schedules are techniques used for training a model with a variable learning rate. We present the most commonly used schedules."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Power Scheduling\n",
    "Set the learning rate to a function of the iteration number $t$: $\\eta(t) = \\eta_0 / (1 +t/s)^c$. The initial learning rate $\\eta_0$, the power $c$ (usually set to 1) and the steps $s$ are hyperparameters.\n",
    "This method drops the learning rate quickly at first, then more and more slowly. Requires tuning of $\\eta_0, s$ and possibly $c$.\n",
    "\n",
    "In Keras we set the decay hyperparameter, which is the inverse of $s$. Keras assumes $c=1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.SGD(lr=0.01, decay=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exponential Scheduling\n",
    "Set the learnining rate to $\\eta(n) = \\eta_0 0.1^{t/s}$. This way $\\eta$ drops gradually by a factor of 10 every $s$ steps. \n",
    "\n",
    "In keras, we define the exponentialy decay function and use the ```LearningRateScheduler``` callback to pass it when fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exponential_decay(lr0, s):\n",
    "    def exponential_decay_fn(epoch):\n",
    "        return lr*0.1**(epoch/s)\n",
    "    return exponential_decay_fn\n",
    "\n",
    "exponential_decay_fn = exponential_decay(lr0=0.01, s=20)\n",
    "lr_scheduler = keras.callbacks.LearningRateScheduler(exponential_decay_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you save a model, the optimizer and the learning rate get saved along with it. Meaning we can load the model and continue training from where it left off.\n",
    "\n",
    "However the epoch argument does not get saved, it gets reset to 0 every time we call ```fit()```. One solution will be to set the ```fit()``` method's ```initial_epoch``` argument so that the epoch starts from where we left off."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Piecewise constant scheduling\n",
    "Use a constant learning rate $\\eta_0$ for $e_0$ epochs, then use another learning rate $\\eta_1$ for some $e_1$ epochs, with $\\eta_0 > \\eta_1 > ... > \\eta_n$. Although this can work very well, it requires fiddling with the right sequence of learning rates and how long to use them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def piecewise_constant_fn(epoch):\n",
    "    if epoch < 5:\n",
    "        return 0.01\n",
    "    elif epoch < 15:\n",
    "        return 0.005\n",
    "    else:\n",
    "        return 0.001\n",
    "\n",
    "lr_scheduler = keras.callbacks.LearningRateScheduler(piecewise_constant_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performance Scheduling\n",
    "Measure the validation error every $N$ steps and reduce the learning rate by a factor of $\\lambda$ when error stops dropping.\n",
    "\n",
    "We can use the ```ReduceLROnPlateau``` callback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiply lr by 0.5 if loss does not improve for 5  consecutive epochs\n",
    "lr_scheduler = keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1cycle scheduling\n",
    "See book pg 361"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A [2013 paper by Andey Senior et al](https://homl.info/63) compared performance of popular learning rates to train DNNs for speech recognition. Authors concluded that performance scheduling and exponential scheduling performed well, favouring exponential scheduling for simplicity. and slightly faster performance.\n",
    "\n",
    "Still, it seems that 1cycle scheduling is a better approach. See [notebook](https://github.com/ageron/handson-ml2/blob/master/11_training_deep_neural_networks.ipynb) for an implementation which uses the same approach for finding the optimal learning rate for the starting learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tf.keras\n",
    "```tf.keras``` offeras an alternative way to implement LR scheduling: define the LR using one of the schedules available in ```keras.optimizers.schedules``` then pass this learning rate to any optimizer. This approach updates the learning rate at each step, instead of each epoch. For example the exponential_decay_fn is implemented below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 20 * len(X_train) // 32 # number of steps in 20 epochs (batch_size=32)\n",
    "learning_rate = keras.optimizers.schedules.ExponentialDecay(0.01, s, 0.1)\n",
    "optimizer = keras.optimizers.SGD(learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this approach, the schedule and its stae gets saved as well. (Note, this is specific to tf.keras, i.e. the tensorflow backend of keras)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the best techniques for regularization is early stopping. Moreover, even though batch normalization was designed to solve the exploding gradients problem, it also does a really good job of regularizing a model. Below we'll explore other techniques for regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $l_1$ and $l_2$ regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$l_2$ can be used for constraining a NN connection weights and/or $l_1$ cna be used if you want a sparse model (i.e. many weights equal to 0). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of l2 regularization with a factor of 0.01\n",
    "layer = keras.layers.Dense(100, activation='elu', \n",
    "                           kernel_initializer='he_normal',\n",
    "                           kernel_regularizer=keras.regularizers.l2(0.01))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This returns a regularizer that is applied at the end of each step during training, this is then added to the final loss. For $l_1$ use ```keras.regularizers.l1()``` and if you want both use ```keras.regularizers.l1_l2()``` specifying both regularization factors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For implementing this in practice, we can use ```functools.partial()``` to create a thin wrapper around a callable. This makes the code easier to read and less error-prone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "RegularizedDense = partial(keras.layers.Dense,\n",
    "                           activation='elu',\n",
    "                           kernel_initializer='he_normal',\n",
    "                           kernel_regularizer=keras.regularizers.l2(0.01))\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    RegularizedDense(300),\n",
    "    RegularizedDense(100),\n",
    "    RegularizedDense(10, activation='softmax', \n",
    "                     kernel_initializer='glorot_uniform')    \n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dropout\n",
    "A very popular and effective regularization technique introduced in [2012](https://homl.info/64) and further detailed in [2014](https://homl.info/65).\n",
    "\n",
    "At every training step, every neuron (including input neurons, but always excluding output neurons), has a probability $p$ of being dropped out. Meaning it will be completely ignored during this training step, but may be active in the next. The *dropout rate* $p$ is typically set between 10% and 50% (20-30% for recurrent neural nets, 40-50% for convolutional neural nets). After training, no neurons are dropped. That's it!\n",
    "\n",
    "Neurons trained with dropout cannot co-adapt with their neighbouring neurons; they have to be as useful as possible on their own. They cannot rely excessively on just a few input neurons; they must pay attention to each of their input neurons. They end up being less sensitive to slight changes to the inputs and in the end we get a more robust network that generalizes better.\n",
    "\n",
    "Another way to understand the power of droput is to realizet that a unique neural net is generated at each training ste. There are $2^N$ possible networks (where $N$ is the number of droppable neurons), which is such a large number that is extremely unlikely the same networks will be sampled twice. Once we have run 10,000 training steps we have essentially trained 10,000 neural nets. They are not independent, but they are all different. The resulting NN can be seen as an averaging ensemble of all these smaller networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One technical detail is that for testing, we need to multiply each input connection weight by the *keep probability* $(1-p)$. If we don't do this, we'll be getting an input signal much larger than the network was trained on and will be unlikely to perform well. Alternatively, we can divide each neuron's input by the keep probability during training (these alternatives are not perfectly equivalent, but work equally well).\n",
    "\n",
    "In keras, we can use ```keras.layers.Droput``` which will randomly drop some inputs (setting them to 0) and divide the remaining inputs by the keep probability. This only applies during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 0.2\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28,28]),\n",
    "    keras.layers.Dropout(rate=p),\n",
    "    keras.layers.Dense(300, activation='elu', kernel_initializer='he_normal'),\n",
    "    keras.layers.Dropout(rate=p),\n",
    "    keras.layers.Dense(100, activation='elu', kernel_initializer='he_normal'),\n",
    "    keras.layers.Dropout(rate=p),\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note** on overfitting, underfitting and using dropout with SeLU on pg. 367"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Monte Carlo (MC) Dropout\n",
    "An improvement to dropout was made in a [2016 paper](https://homl.info/mcdroput) with two main points:\n",
    "- The paper established a connection between dropout networks and approximate Bayesian inference, giving dropout a solid mathematical justification\n",
    "- *MC Dropout* was introduced, which can boost the performance of a model without retraining, or modify it at all. It provides a much better measure of the uncertainty of the model\n",
    "\n",
    "Below is its implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_probas = np.stack([model(X_test_scaled, training=True)\n",
    "                           for sample in range(100)])\n",
    "y_proba = y_probas.mean(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We just make 100 predictions over the test set (setting training=True to ensure Dropout is active) and stack the predictions. Page 369 has a lenghty comparison of Dropout model vs MC dropout model. It also discussed the uncertainty in a modela's probability estimates.\n",
    "\n",
    "Note that the number of Monte Carlo samples you take (100 above) is a hyperparameter you can tweak. The more, the better. However inference time will also be increased and above a certain number of samples, we will get little improvement. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your model contains other layers that behave in a special way during training (e.g. batch normalization), you should not force training, like above. Instead we should replace the Dropout Layer with the following MCDropout class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCDropout(keras.layers.Dropout):\n",
    "    def call(self, inputs):\n",
    "        return super().call(inputs, training=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Max-Norm Regularization\n",
    "\n",
    "For each neuron, Max Norm constrains the weights $\\textbf{w}$ of incoming connections such that $||\\textbf{w}||_2 \\leq r$, where $r$ is the max-norm hyperparameter and $||. ||_2$ is the $l_2$ norm. This does not add a regularization term ot the overall loss function, but instead is tipically implemented by computing $||\\textbf{w}||_2$ after each training step and rescaling $\\textbf{w}$ if needed ($\\textbf{w} \\leftarrow \\textbf{w}\\frac{2}{||\\textbf{w}||_2 }$). Reducing $r$ increases regularization and helps reduce overfitting. This can also help alleviate vanishing gradients (if not using BN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.layers.core.Dense at 0x7f3ab03e5af0>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.layers.Dense(100, activation='elu', kernel_initializer='he_normal',\n",
    "                   kernel_constraint=keras.constraints.max_norm(1.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 8\n",
    "We'll train various networks on the [CIFAR10](https://keras.io/api/datasets/cifar10/#load_data-function) dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(50000, 32, 32, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "tf.random.set_seed(58)\n",
    "np.random.seed(58)\n",
    "\n",
    "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "X_train_full.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is compose of 50,000 32x32 images with RGB channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y_test.flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 10 target classes, labeles 1 through 10. The labels for the classes are: airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 32, 32, 3)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_full_size = X_train_full.shape[0]\n",
    "val_size = int(train_full_size*0.1)\n",
    "\n",
    "X_train, y_train = X_train_full[:-val_size], y_train_full[:-val_size]\n",
    "X_val, y_val = X_train_full[-val_size:], y_train_full[-val_size:]\n",
    "X_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## a / b)\n",
    "Build a DNN with 20 hidden layers of 100 neurons each. Use He initialization and ELU activation function. Using Nadam optimization and earlystopping train the network on the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-34c1935ec502cdf0\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-34c1935ec502cdf0\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          url.port = 6006;\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# setup for tensorboard\n",
    "import os\n",
    "root_logdir = os.path.join(os.curdir, 'my_logs')\n",
    "\n",
    "def get_run_logdir():\n",
    "    import time\n",
    "    run_id = time.strftime(\"run_%Y_%m_%d-%H_%M_%S\")\n",
    "    return os.path.join(root_logdir, run_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run ```tensorboard --logdir=./my_logs --port=6006``` on terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense\n",
    "\n",
    "def build_hidden_layers(inputs, units, n_layers,\n",
    "                       kernel_initializer='he_normal', activation='elu'):\n",
    "    h = Dense(units, kernel_initializer=kernel_initializer, activation=activation)(inputs)\n",
    "    for idx in range(1, n_layers):\n",
    "        h = Dense(units, kernel_initializer=kernel_initializer, activation=activation)(h)\n",
    "    return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_14 (InputLayer)        (None, 32, 32, 3)         0         \n",
      "_________________________________________________________________\n",
      "flatten_14 (Flatten)         (None, 3072)              0         \n",
      "_________________________________________________________________\n",
      "dense_140 (Dense)            (None, 100)               307300    \n",
      "_________________________________________________________________\n",
      "dense_141 (Dense)            (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_142 (Dense)            (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_143 (Dense)            (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_144 (Dense)            (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_145 (Dense)            (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_146 (Dense)            (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_147 (Dense)            (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_148 (Dense)            (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_149 (Dense)            (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_150 (Dense)            (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_151 (Dense)            (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_152 (Dense)            (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_153 (Dense)            (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_154 (Dense)            (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_155 (Dense)            (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_156 (Dense)            (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_157 (Dense)            (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_158 (Dense)            (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_159 (Dense)            (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 500,210\n",
      "Trainable params: 500,210\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Flatten\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping, TensorBoard\n",
    "\n",
    "units = 100\n",
    "n_layers = 20\n",
    "epochs = 50\n",
    "batch_size = 32\n",
    "learning_rate = 3e-5\n",
    "\n",
    "inputs = Input(shape=X_train.shape[1:])\n",
    "flatten = Flatten()(inputs)\n",
    "hidden = build_hidden_layers(flatten, units=units, n_layers=n_layers)\n",
    "outputs = Dense(10, activation='softmax', name='output')(hidden)\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "optimizer = Adam(learning_rate=learning_rate)\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/50\n",
      "45000/45000 [==============================] - 11s 251us/step - loss: 4.2571 - accuracy: 0.1517 - val_loss: 2.1785 - val_accuracy: 0.2090\n",
      "Epoch 2/50\n",
      "45000/45000 [==============================] - 11s 235us/step - loss: 2.1086 - accuracy: 0.2278 - val_loss: 2.0555 - val_accuracy: 0.2430\n",
      "Epoch 3/50\n",
      "45000/45000 [==============================] - 13s 290us/step - loss: 1.9948 - accuracy: 0.2687 - val_loss: 1.9973 - val_accuracy: 0.2764\n",
      "Epoch 4/50\n",
      "45000/45000 [==============================] - 12s 268us/step - loss: 1.9194 - accuracy: 0.2973 - val_loss: 1.9286 - val_accuracy: 0.3014\n",
      "Epoch 5/50\n",
      "45000/45000 [==============================] - 11s 254us/step - loss: 1.8609 - accuracy: 0.3237 - val_loss: 1.8460 - val_accuracy: 0.3216\n",
      "Epoch 6/50\n",
      "45000/45000 [==============================] - 11s 234us/step - loss: 1.8077 - accuracy: 0.3462 - val_loss: 1.8141 - val_accuracy: 0.3376\n",
      "Epoch 7/50\n",
      "45000/45000 [==============================] - 11s 236us/step - loss: 1.7684 - accuracy: 0.3576 - val_loss: 1.7854 - val_accuracy: 0.3534\n",
      "Epoch 8/50\n",
      "45000/45000 [==============================] - 11s 236us/step - loss: 1.7337 - accuracy: 0.3728 - val_loss: 1.7540 - val_accuracy: 0.3612\n",
      "Epoch 9/50\n",
      "45000/45000 [==============================] - 11s 250us/step - loss: 1.6985 - accuracy: 0.3869 - val_loss: 1.7147 - val_accuracy: 0.3762\n",
      "Epoch 10/50\n",
      "45000/45000 [==============================] - 10s 227us/step - loss: 1.6727 - accuracy: 0.3968 - val_loss: 1.7160 - val_accuracy: 0.3820\n",
      "Epoch 11/50\n",
      "45000/45000 [==============================] - 10s 219us/step - loss: 1.6511 - accuracy: 0.4048 - val_loss: 1.6995 - val_accuracy: 0.3906\n",
      "Epoch 12/50\n",
      "45000/45000 [==============================] - 12s 260us/step - loss: 1.6290 - accuracy: 0.4129 - val_loss: 1.6590 - val_accuracy: 0.3980\n",
      "Epoch 13/50\n",
      "45000/45000 [==============================] - 10s 233us/step - loss: 1.6106 - accuracy: 0.4206 - val_loss: 1.6512 - val_accuracy: 0.4048\n",
      "Epoch 14/50\n",
      "45000/45000 [==============================] - 11s 242us/step - loss: 1.5920 - accuracy: 0.4264 - val_loss: 1.6526 - val_accuracy: 0.4060\n",
      "Epoch 15/50\n",
      "45000/45000 [==============================] - 11s 238us/step - loss: 1.5788 - accuracy: 0.4317 - val_loss: 1.6116 - val_accuracy: 0.4236\n",
      "Epoch 16/50\n",
      "45000/45000 [==============================] - 11s 236us/step - loss: 1.5616 - accuracy: 0.4383 - val_loss: 1.6315 - val_accuracy: 0.4110\n",
      "Epoch 17/50\n",
      "45000/45000 [==============================] - 11s 238us/step - loss: 1.5493 - accuracy: 0.4429 - val_loss: 1.5976 - val_accuracy: 0.4238\n",
      "Epoch 18/50\n",
      "45000/45000 [==============================] - 11s 248us/step - loss: 1.5333 - accuracy: 0.4498 - val_loss: 1.5994 - val_accuracy: 0.4174\n",
      "Epoch 19/50\n",
      "45000/45000 [==============================] - 14s 313us/step - loss: 1.5217 - accuracy: 0.4518 - val_loss: 1.5923 - val_accuracy: 0.4256\n",
      "Epoch 20/50\n",
      "45000/45000 [==============================] - 12s 261us/step - loss: 1.5133 - accuracy: 0.4564 - val_loss: 1.5937 - val_accuracy: 0.4222\n",
      "Epoch 21/50\n",
      "45000/45000 [==============================] - 11s 250us/step - loss: 1.4963 - accuracy: 0.4606 - val_loss: 1.5756 - val_accuracy: 0.4414\n",
      "Epoch 22/50\n",
      "45000/45000 [==============================] - 11s 239us/step - loss: 1.4886 - accuracy: 0.4646 - val_loss: 1.5666 - val_accuracy: 0.4318\n",
      "Epoch 23/50\n",
      "45000/45000 [==============================] - 11s 236us/step - loss: 1.4798 - accuracy: 0.4703 - val_loss: 1.5944 - val_accuracy: 0.4340\n",
      "Epoch 24/50\n",
      "45000/45000 [==============================] - 12s 260us/step - loss: 1.4655 - accuracy: 0.4742 - val_loss: 1.5785 - val_accuracy: 0.4408\n",
      "Epoch 25/50\n",
      "45000/45000 [==============================] - 12s 261us/step - loss: 1.4585 - accuracy: 0.4724 - val_loss: 1.5700 - val_accuracy: 0.4402\n",
      "Epoch 26/50\n",
      "45000/45000 [==============================] - 12s 256us/step - loss: 1.4487 - accuracy: 0.4817 - val_loss: 1.5554 - val_accuracy: 0.4470\n",
      "Epoch 27/50\n",
      "45000/45000 [==============================] - 12s 275us/step - loss: 1.4350 - accuracy: 0.4845 - val_loss: 1.5473 - val_accuracy: 0.4480\n",
      "Epoch 28/50\n",
      "45000/45000 [==============================] - 10s 230us/step - loss: 1.4289 - accuracy: 0.4868 - val_loss: 1.5584 - val_accuracy: 0.4424\n",
      "Epoch 29/50\n",
      "45000/45000 [==============================] - 11s 249us/step - loss: 1.4210 - accuracy: 0.4904 - val_loss: 1.5508 - val_accuracy: 0.4418\n",
      "Epoch 30/50\n",
      "45000/45000 [==============================] - 12s 271us/step - loss: 1.4082 - accuracy: 0.4944 - val_loss: 1.5427 - val_accuracy: 0.4446\n",
      "Epoch 31/50\n",
      "45000/45000 [==============================] - 10s 226us/step - loss: 1.4024 - accuracy: 0.4965 - val_loss: 1.5397 - val_accuracy: 0.4448\n",
      "Epoch 32/50\n",
      "45000/45000 [==============================] - 11s 253us/step - loss: 1.3904 - accuracy: 0.5029 - val_loss: 1.5339 - val_accuracy: 0.4508\n",
      "Epoch 33/50\n",
      "45000/45000 [==============================] - 14s 315us/step - loss: 1.3826 - accuracy: 0.5030 - val_loss: 1.5505 - val_accuracy: 0.4384\n",
      "Epoch 34/50\n",
      "45000/45000 [==============================] - 11s 236us/step - loss: 1.3719 - accuracy: 0.5057 - val_loss: 1.5441 - val_accuracy: 0.4532\n",
      "Epoch 35/50\n",
      "45000/45000 [==============================] - 12s 265us/step - loss: 1.3657 - accuracy: 0.5103 - val_loss: 1.5783 - val_accuracy: 0.4408\n",
      "Epoch 36/50\n",
      "45000/45000 [==============================] - 10s 223us/step - loss: 1.3584 - accuracy: 0.5126 - val_loss: 1.5473 - val_accuracy: 0.4546\n",
      "Epoch 37/50\n",
      "45000/45000 [==============================] - 13s 292us/step - loss: 1.3518 - accuracy: 0.5147 - val_loss: 1.5478 - val_accuracy: 0.4580\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f21983ff7f0>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size,\n",
    "          validation_data=(X_val, y_val),\n",
    "          callbacks=[TensorBoard(get_run_logdir()), EarlyStopping(patience=5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 57us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.5567552700042724, 0.45100000500679016]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This initial attempt without fine-tuning yields 45% accuracy on the test set. Also note that the training accuracy is 51% and validation accuracy 45%, meaning the model overfit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c) Add Batch Normalization and compare the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, BatchNormalization, Activation\n",
    "\n",
    "def build_hidden_layers_bn(inputs, units, n_layers,\n",
    "                           kernel_initializer='he_normal', activation='elu'):\n",
    "    inputs = BatchNormalization()(inputs)\n",
    "    h = Dense(units, kernel_initializer=kernel_initializer)(inputs)\n",
    "    h = BatchNormalization()(h)\n",
    "    h = Activation(activation)(h)\n",
    "    for idx in range(1, n_layers):\n",
    "        h = Dense(units, kernel_initializer=kernel_initializer)(h)\n",
    "        h = BatchNormalization()(h)\n",
    "        h = Activation(activation)(h)\n",
    "    return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_18 (InputLayer)        (None, 32, 32, 3)         0         \n",
      "_________________________________________________________________\n",
      "flatten_18 (Flatten)         (None, 3072)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_109 (Bat (None, 3072)              12288     \n",
      "_________________________________________________________________\n",
      "dense_182 (Dense)            (None, 100)               307300    \n",
      "_________________________________________________________________\n",
      "batch_normalization_110 (Bat (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_183 (Dense)            (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_111 (Bat (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_184 (Dense)            (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_112 (Bat (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_185 (Dense)            (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_113 (Bat (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_186 (Dense)            (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_114 (Bat (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_187 (Dense)            (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_115 (Bat (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_188 (Dense)            (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_116 (Bat (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_189 (Dense)            (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_117 (Bat (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_190 (Dense)            (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_118 (Bat (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_191 (Dense)            (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_119 (Bat (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_192 (Dense)            (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_120 (Bat (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_193 (Dense)            (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_121 (Bat (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_194 (Dense)            (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_122 (Bat (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_195 (Dense)            (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_123 (Bat (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_196 (Dense)            (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_124 (Bat (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_197 (Dense)            (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_125 (Bat (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_198 (Dense)            (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_126 (Bat (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_199 (Dense)            (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_127 (Bat (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_200 (Dense)            (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_128 (Bat (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_201 (Dense)            (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_129 (Bat (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 520,498\n",
      "Trainable params: 510,354\n",
      "Non-trainable params: 10,144\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputs = Input(shape=X_train.shape[1:])\n",
    "flatten = Flatten()(inputs)\n",
    "hidden = build_hidden_layers_bn(flatten, units=units, n_layers=n_layers, activation='elu')\n",
    "outputs = Dense(10, activation='softmax', name='output')(hidden)\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "optimizer = Adam(learning_rate=learning_rate)\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/50\n",
      "  352/45000 [..............................] - ETA: 9:52 - loss: 2.8518 - accuracy: 0.1023 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/carlos/homl/lib/python3.8/site-packages/keras/callbacks/callbacks.py:92: RuntimeWarning: Method (on_train_batch_end) is slow compared to the batch update (0.357614). Check your callbacks.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45000/45000 [==============================] - 28s 623us/step - loss: 2.1649 - accuracy: 0.2362 - val_loss: 1.8638 - val_accuracy: 0.3266\n",
      "Epoch 2/50\n",
      "45000/45000 [==============================] - 21s 466us/step - loss: 1.8673 - accuracy: 0.3317 - val_loss: 1.7446 - val_accuracy: 0.3776\n",
      "Epoch 3/50\n",
      "45000/45000 [==============================] - 22s 497us/step - loss: 1.7773 - accuracy: 0.3636 - val_loss: 1.6830 - val_accuracy: 0.3988\n",
      "Epoch 4/50\n",
      "45000/45000 [==============================] - 21s 473us/step - loss: 1.7125 - accuracy: 0.3867 - val_loss: 1.6280 - val_accuracy: 0.4234\n",
      "Epoch 5/50\n",
      "45000/45000 [==============================] - 21s 472us/step - loss: 1.6674 - accuracy: 0.4059 - val_loss: 1.5939 - val_accuracy: 0.4340\n",
      "Epoch 6/50\n",
      "45000/45000 [==============================] - 22s 484us/step - loss: 1.6254 - accuracy: 0.4209 - val_loss: 1.5617 - val_accuracy: 0.4516\n",
      "Epoch 7/50\n",
      "45000/45000 [==============================] - 22s 490us/step - loss: 1.5981 - accuracy: 0.4311 - val_loss: 1.5399 - val_accuracy: 0.4580\n",
      "Epoch 8/50\n",
      "45000/45000 [==============================] - 22s 490us/step - loss: 1.5722 - accuracy: 0.4391 - val_loss: 1.5140 - val_accuracy: 0.4696\n",
      "Epoch 9/50\n",
      "45000/45000 [==============================] - 21s 477us/step - loss: 1.5436 - accuracy: 0.4523 - val_loss: 1.4976 - val_accuracy: 0.4748\n",
      "Epoch 10/50\n",
      "45000/45000 [==============================] - 21s 473us/step - loss: 1.5245 - accuracy: 0.4592 - val_loss: 1.4821 - val_accuracy: 0.4822\n",
      "Epoch 11/50\n",
      "45000/45000 [==============================] - 23s 501us/step - loss: 1.5056 - accuracy: 0.4639 - val_loss: 1.4759 - val_accuracy: 0.4788\n",
      "Epoch 12/50\n",
      "45000/45000 [==============================] - 21s 476us/step - loss: 1.4867 - accuracy: 0.4722 - val_loss: 1.4606 - val_accuracy: 0.4828\n",
      "Epoch 13/50\n",
      "45000/45000 [==============================] - 22s 484us/step - loss: 1.4710 - accuracy: 0.4796 - val_loss: 1.4407 - val_accuracy: 0.4956\n",
      "Epoch 14/50\n",
      "45000/45000 [==============================] - 22s 498us/step - loss: 1.4560 - accuracy: 0.4842 - val_loss: 1.4425 - val_accuracy: 0.4960\n",
      "Epoch 15/50\n",
      "45000/45000 [==============================] - 22s 490us/step - loss: 1.4425 - accuracy: 0.4884 - val_loss: 1.4284 - val_accuracy: 0.4916\n",
      "Epoch 16/50\n",
      "45000/45000 [==============================] - 21s 478us/step - loss: 1.4267 - accuracy: 0.4969 - val_loss: 1.4206 - val_accuracy: 0.4984\n",
      "Epoch 17/50\n",
      "45000/45000 [==============================] - 21s 470us/step - loss: 1.4137 - accuracy: 0.4984 - val_loss: 1.4159 - val_accuracy: 0.5026\n",
      "Epoch 18/50\n",
      "45000/45000 [==============================] - 21s 463us/step - loss: 1.3985 - accuracy: 0.5060 - val_loss: 1.4078 - val_accuracy: 0.5026\n",
      "Epoch 19/50\n",
      "45000/45000 [==============================] - 22s 496us/step - loss: 1.3843 - accuracy: 0.5080 - val_loss: 1.4007 - val_accuracy: 0.5068\n",
      "Epoch 20/50\n",
      "45000/45000 [==============================] - 23s 514us/step - loss: 1.3738 - accuracy: 0.5121 - val_loss: 1.3991 - val_accuracy: 0.4994\n",
      "Epoch 21/50\n",
      "45000/45000 [==============================] - 25s 551us/step - loss: 1.3750 - accuracy: 0.5113 - val_loss: 1.3898 - val_accuracy: 0.5030\n",
      "Epoch 22/50\n",
      "45000/45000 [==============================] - 20s 452us/step - loss: 1.3563 - accuracy: 0.5181 - val_loss: 1.3875 - val_accuracy: 0.5036\n",
      "Epoch 23/50\n",
      "45000/45000 [==============================] - 21s 462us/step - loss: 1.3440 - accuracy: 0.5213 - val_loss: 1.3931 - val_accuracy: 0.5052\n",
      "Epoch 24/50\n",
      "45000/45000 [==============================] - 22s 496us/step - loss: 1.3366 - accuracy: 0.5271 - val_loss: 1.3836 - val_accuracy: 0.5020\n",
      "Epoch 25/50\n",
      "45000/45000 [==============================] - 21s 466us/step - loss: 1.3286 - accuracy: 0.5274 - val_loss: 1.3837 - val_accuracy: 0.5116\n",
      "Epoch 26/50\n",
      "45000/45000 [==============================] - 21s 459us/step - loss: 1.3201 - accuracy: 0.5300 - val_loss: 1.3869 - val_accuracy: 0.5084\n",
      "Epoch 27/50\n",
      "45000/45000 [==============================] - 19s 424us/step - loss: 1.3113 - accuracy: 0.5346 - val_loss: 1.3751 - val_accuracy: 0.5092\n",
      "Epoch 28/50\n",
      "45000/45000 [==============================] - 19s 419us/step - loss: 1.3066 - accuracy: 0.5364 - val_loss: 1.3762 - val_accuracy: 0.5134\n",
      "Epoch 29/50\n",
      "45000/45000 [==============================] - 19s 418us/step - loss: 1.2965 - accuracy: 0.5414 - val_loss: 1.3762 - val_accuracy: 0.5126\n",
      "Epoch 30/50\n",
      "45000/45000 [==============================] - 19s 416us/step - loss: 1.2918 - accuracy: 0.5399 - val_loss: 1.3675 - val_accuracy: 0.5166\n",
      "Epoch 31/50\n",
      "45000/45000 [==============================] - 19s 421us/step - loss: 1.2805 - accuracy: 0.5452 - val_loss: 1.3787 - val_accuracy: 0.5066\n",
      "Epoch 32/50\n",
      "45000/45000 [==============================] - 19s 421us/step - loss: 1.2649 - accuracy: 0.5496 - val_loss: 1.3625 - val_accuracy: 0.5152\n",
      "Epoch 33/50\n",
      "45000/45000 [==============================] - 19s 418us/step - loss: 1.2653 - accuracy: 0.5525 - val_loss: 1.3581 - val_accuracy: 0.5142\n",
      "Epoch 34/50\n",
      "45000/45000 [==============================] - 19s 421us/step - loss: 1.2573 - accuracy: 0.5549 - val_loss: 1.3640 - val_accuracy: 0.5162\n",
      "Epoch 35/50\n",
      "45000/45000 [==============================] - 19s 416us/step - loss: 1.2573 - accuracy: 0.5546 - val_loss: 1.3605 - val_accuracy: 0.5126\n",
      "Epoch 36/50\n",
      "45000/45000 [==============================] - 19s 419us/step - loss: 1.2429 - accuracy: 0.5585 - val_loss: 1.3602 - val_accuracy: 0.5172\n",
      "Epoch 37/50\n",
      "45000/45000 [==============================] - 19s 421us/step - loss: 1.2348 - accuracy: 0.5604 - val_loss: 1.3645 - val_accuracy: 0.5132\n",
      "Epoch 38/50\n",
      "45000/45000 [==============================] - 19s 417us/step - loss: 1.2298 - accuracy: 0.5629 - val_loss: 1.3525 - val_accuracy: 0.5156\n",
      "Epoch 39/50\n",
      "45000/45000 [==============================] - 21s 461us/step - loss: 1.2262 - accuracy: 0.5634 - val_loss: 1.3546 - val_accuracy: 0.5170\n",
      "Epoch 40/50\n",
      "45000/45000 [==============================] - 23s 521us/step - loss: 1.2228 - accuracy: 0.5649 - val_loss: 1.3560 - val_accuracy: 0.5182\n",
      "Epoch 41/50\n",
      "45000/45000 [==============================] - 23s 502us/step - loss: 1.2123 - accuracy: 0.5704 - val_loss: 1.3546 - val_accuracy: 0.5170\n",
      "Epoch 42/50\n",
      "45000/45000 [==============================] - 20s 447us/step - loss: 1.2070 - accuracy: 0.5718 - val_loss: 1.3621 - val_accuracy: 0.5142\n",
      "Epoch 43/50\n",
      "45000/45000 [==============================] - 20s 454us/step - loss: 1.1999 - accuracy: 0.5749 - val_loss: 1.3530 - val_accuracy: 0.5198\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f217a28d3d0>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size,\n",
    "          validation_data=(X_val, y_val),\n",
    "          callbacks=[TensorBoard(get_run_logdir()), EarlyStopping(patience=5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 88us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.3627665489196776, 0.515999972820282]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first model took 37 epochs to converge and each training step took around ~12s. With batch normalization the model took 43 epochs to converge, though each step took around an extra 9s (avg 21s per epoch). Finally the performance of the BN model is better with 51% accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## d) \n",
    "Replace Batch Normalization with SELU making the necessary adjustments to ensure the networks self normalizes (i.e. standardize input features, use LeCunn normal initialization, sequential architecture with dense layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras.layers.experimental'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-77-b1eecefa4719>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNormalization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mflatten\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFlatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mstandardized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNormalization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'keras.layers.experimental'"
     ]
    }
   ],
   "source": [
    "# from keras.layers.experimental.preprocessing import Normalization\n",
    "\n",
    "\n",
    "inputs = Input(shape=X_train.shape[1:])\n",
    "flatten = Flatten()(inputs)\n",
    "std_layer = Normalization()\n",
    "std_layer.adapt()()\n",
    "hidden = build_hidden_layers(standardized, units=units, n_layers=n_layers, \n",
    "                             kernel_initializer='lecun_normal', activation='selu')\n",
    "outputs = Dense(10, activation='softmax', name='output')(hidden)\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "optimizer = Adam(learning_rate=learning_rate)\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
