{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *Perceptron* is on of the simplest Artificial neural network architectures, proposed in 1957 by Frank Rosenblatt. It is based on a *threshold logic unit (TLU)* and it computes a weighted sum of its inputs\n",
    "\n",
    "$$ z = w_1x_1 + \\cdots + w_nx_n = \\textbf{x}^{\\intercal}\\textbf{w} $$\n",
    "\n",
    "then applies a step function to that sum and outputs the result: $h_w(\\textbf{x})=\\text{step}(\\textbf{x})$. One of the most common step function used is the *Heaviside step function*\n",
    "\n",
    "$$ \\text{heaviside}(z) = \\begin{cases} 0 & \\text{if } z<0 \\\\ 1 & \\text{if } z\\gt0 \\end{cases}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A single TLU can be used for binary classification; it computes a linear combination of its inputs and if the output reaches a threshold, it outputs a positive class, otherwise outputs the negative class.\n",
    "\n",
    "A perceptron is composed of a single layer of TLUs, with each TLU connected to all the inputs. When all the neurons in a layer are connected to every neuron in the previous layer the layer is called a *fully connected* or *dense* layer. *Input Neurons* are simple inputs that output whatever they are fed and all input neurons form the *input layer*. A bias neuron is generally added, tipycally represented by a *bias neuron*, which outputs 1 all the time. (e.g. architecture pg 286 fig 10-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then write the outputs of a fully connected layer as \n",
    "$$ h_{\\textbf{W, b}}(\\textbf{X}) = \\phi(\\textbf{XW + b})$$\n",
    "Where\n",
    "- $\\textbf{X}$ is the matrix of input features (one row per instance, one col per feature)\n",
    "- $\\textbf{W}$ contains the connection weights, except the ones from the bias neuron (one row per input neuron, one column per artificial neuron in the layer)\n",
    "- $\\phi$ is called the *activation function* (when the neurons are TLU, this is a step function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The perceptron learning rule reinforces connections between neurons tha help reduce the error: the perceptron is fed one training instance at a time, and for each instance it makes its predictions. For every output neuron that produced a wrong predictions, it reinforces the connection weights from the inputs that would have contributed to the correct prediction\n",
    "\n",
    "$$ w_{i,j}^{\\text{next step}} = w_{i,j} +\\eta(y_j - \\hat{y_j})x_i$$\n",
    "\n",
    "where \n",
    "- $w_{i,j}$ is the weight between ith input neuron and jth output neuron\n",
    "- $x_i$ is the ith input value of the current training instance\n",
    "- $\\hat{y_j}$ is the output of the jth output neuron \n",
    "- $y_j$ is the target output of the jth ouptut neuron\n",
    "- $\\eta$ is the learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The multilayer perceptron and backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An MLP consistis of one input layer, one or more layers of TLUs (called *hidden layers*) and one final layer of TLUs called the *output layer*. Every except the output layer includes a bias neuron and is fully connected to the next layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train an MLP, we use [backpropagation](https://homl.info/44). In short, it is Gradient Descent and it is able to compute the gradient of the network's error with regard to every single model parameter, thus it is able to find out how much it should tweak each connection weight and bias in order to reduce the error. This process is called *autodiff*, appendix D has more info on it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's how it works\n",
    "- It handles one mini-batch at a time (e.g. 32 instances) and goes through the training set multiple times, each pass is called an *epoch*\n",
    "- Each mini-batch is passed is passed to the network's input layer, which sends it to the first hidden layer. The algorithm then computes the outputs of this layer and passes it to the next layer, and so on, until we get the output of the output layer. This is called a *forward pass* and the intermediate results are saved\n",
    "- Next we calculate the network's output error (using some loss function)\n",
    "- Then it computs how much each output connection contributed to the error (done using chain rule)\n",
    "- The algorithm then measures how much of these error contributions came from each connection in the layer below until it reaches the input layer\n",
    "- Finally, it performs a gradient descent step to tweak all the connection weights in the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One change that had to be made to the original MLP architecture was replacing the step function with the logistic function $\\sigma(z) = 1 / (1 +\\exp(-z))$, this allows for gradients to be computed as it is a smooth function.\n",
    "\n",
    "Some other choices of function are:\n",
    "- Hyperbolic tan $\\tanh(z) = 2\\sigma(2z) - 1$\n",
    "\n",
    "Another S-shaped function, continues and differentiable. Its outputs are in the range -1 to 1, making each layer's output more or less centered around 0 at the beginning of training, which helps speed up convergence.\n",
    "\n",
    "- Rectified Linear unit $ReLU(z) = \\max(0,z)$\n",
    "\n",
    "Continuous but not differentiable at $z=0$, however it works very well and has become the default."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Activation functions are useful because they can add non-linearity to each layer. Recall that a linear transformation of linear transformations is also linear. Using a non-linear function allows for an MLP to learn more complex patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression MLPs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use MLP for regression we use an output neuron for each value we want to predict. In the univariate case (e.g. predicting house price) only a single output neuron is needed. \n",
    "\n",
    "For multivariate problems, you need one output neuron per output dimension. For example to locate the center of an object in an image, you need to predict 2D coordinates, thus 2 output neurons. If you also want to place a bounding box around the object, you need two more numbers, the width and height of the object. In total, 4 output neurons.\n",
    "\n",
    "In general we do not want to use any activation function for output neurons so they are free to output any range of values. To guarantee the range of values is always positive, use ReLU or *softplus*, which is a smooth variant of ReLU: $\\text{softplus}(z) = \\log(1 + \\exp(z))$. \n",
    "Finally if we want to guarantee the predictions will fall between a range of values we can use the logistic or hyperbolic tangent function, scaling the labels to the appropriate values.\n",
    "\n",
    "The typical loss function used is MSE, however if you have a lot of outliers in your training set you may want to use the mean absolute error instead. Alternatively use [Huber loss](https://en.wikipedia.org/wiki/Huber_loss), which is a combination of both.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typical regression MLP architecture\n",
    "\n",
    "| Hyperparameter | Typical value |\n",
    "|     ---        |      ---      |\n",
    "|# input neurons | One per input feature (e.g. 28x28=784 for MNIST) | \n",
    "|# hidden layers | Variable (typically 1 to 5) |\n",
    "|# neurons per hidden layers | Variable (typically 10 to 100) |\n",
    "|# output layer | 1 per prediction dimension |\n",
    "|Activation function | $\\begin{cases} \n",
    "                        \\text{None} & \\text{ for any range of values } \\\\ \n",
    "                        ReLU/\\text{softplus} & \\text{ positive outputs }\\\\\n",
    "                        \\text{logistic/tanh} & \\text{ bounded outputs}\n",
    "                        \\end{cases}$ |\n",
    "|Loss Function | MSE or MAE/Huber (if outliers)|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification MLPs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For binary classification, we just need a single output neuron using the logistic activation function. The output will be in the range 0 - 1 and we can interpret it as an estimated class probability of the positive class. The estimated probability for the negative class is one minus that number."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLPs can also be used for multilabel binary classification. For example, in an e-mail classification system that tags messages as spam/ham and urgent/non-urgent we would use two output neurons with the logistic function. The first outputs the probability that the e-mail is spam and the second the probability the e-mail is urgent. \n",
    "More generally, we use one output neuron for each positive class.\n",
    "\n",
    "For multiclass calssification (e.g. identifying digit classes 0 through 9), then we need one output neuron per class and should use the softmax activation to ensure estimated probabilities are between 0-1 and they add up to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the loss function, since we're preducting probability distributions, the cross-entropy loss is generally a good choice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typical classification MLP architecture\n",
    "\n",
    "| Hyperparameter | Binary Classification | Multilabel Binary Classification | Multiclass Classification | \n",
    "| --- | --- | --- | --- |\n",
    "|# input neurons | One per input feature | One per input feature | One per input feature |\n",
    "|# hidden layers | Variable (typically 1 to 5) | Variable (typically 1 to 5) | Variable (typically 1 to 5) |\n",
    "|# neurons per hidden layers | Variable (typically 10 to 100) | Variable (typically 10 to 100)| Variable (typically 10 to 100)|\n",
    "|# output neurons | 1 | 1 per label | 1 per class |\n",
    "|Activation Function| Logistic | Logistic | Softmax |\n",
    "|Loss Function | Cross Entropy | Cross Entropy | Cross Entropy |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1\n",
    "Play around in the [Tensorflow Playground](https://playground.tensorflow.org) to get a better feeling for ANNs and explore the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing MLPs with Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building an Image Classifier using the Sequential API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this chapter, we will tackle Fashion MNIST, which is a drop-in replacement of MNIST. The images represent fashion items instead of digits, so each class is more diverse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_full.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data is loaded as ints in the range from 0 to 255. Let's create a validation set and scale the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('uint8')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_full.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid, X_train = X_train_full[:5000]/255.0, X_train_full[5000:]/255.0\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 0, 7, ..., 3, 0, 5], dtype=uint8)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to map the target values to their actual class as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the model using the Sequential API\n",
    "We'll start by creating a classification MLP with two hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten, Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=[28,28])) # Converts inputs to 1D array\n",
    "model.add(Dense(300, activation='relu'))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Sequential API is the simplest kind of model for NNs that are just composed of a single stack of layers connected sequentially. Another way to write the same model could be as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Flatten(input_shape=[28,28]),\n",
    "    Dense(300, activation='relu'),\n",
    "    Dense(100, activation='relu'),\n",
    "    Dense(10, activation='softmax'),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can vew a definition of the model by using summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_2 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 300)               235500    \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 100)               30100     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 266,610\n",
      "Trainable params: 266,610\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the model has a lot of parameters, it has a lot of flexibility to train the data. However this also means that it runs the risk of overfitting, especially when we don't have much training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can view a model's Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.layers.core.Flatten at 0x7f6c61f7d730>,\n",
       " <keras.layers.core.Dense at 0x7f6c61f7db80>,\n",
       " <keras.layers.core.Dense at 0x7f6c61f7dd90>,\n",
       " <keras.layers.core.Dense at 0x7f6c61f7d670>]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dense_4'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden1 = model.layers[1]\n",
    "hidden1.name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As well as it's weights and biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.03716776, -0.03312553, -0.01866506, ..., -0.04552968,\n",
       "        -0.06381333,  0.02029909],\n",
       "       [-0.0718895 ,  0.06939384,  0.03087463, ...,  0.04213425,\n",
       "         0.03456113, -0.00604291],\n",
       "       [ 0.06998566, -0.04743899, -0.05342283, ..., -0.0683958 ,\n",
       "         0.03110686, -0.04788268],\n",
       "       ...,\n",
       "       [ 0.06579678,  0.05081166,  0.01587593, ...,  0.06356768,\n",
       "        -0.02430162, -0.01516398],\n",
       "       [ 0.01724309, -0.00942904, -0.01034736, ...,  0.067991  ,\n",
       "         0.01635592,  0.0557151 ],\n",
       "       [-0.01923738, -0.05680314,  0.07252409, ...,  0.02584322,\n",
       "        -0.023685  ,  0.01471683]], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights, biases = hidden1.get_weights()\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the weights are initialized randomly (to break simmetry) and the biases set to zero. To use other initialization methods we can set the ```kernel_initializer``` or ```bias_initializer``` when creating the layer.\n",
    "\n",
    "Next we compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss = 'sparse_categorical_crossentropy',\n",
    "              optimizer = 'sgd',\n",
    "              metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use ```sparse_categorical_crossentropy``` because we have sparse labels (i.e. for each instance there is only a single target class) and the classes are exclusive. If instead we had one target probability per class for each instance  (such as one-hot vectors for a single class) we'd use ```categorical_crossentropy``` instead. \n",
    "\n",
    "The optimizer set to ```sgd``` means we'll train the model using simple stochastic gradient descent. \n",
    "\n",
    "Finally, we can fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "RELOAD = False\n",
    "CHAPTER_DIR = 'saved_models/10_intro_to_anns/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.2244 - accuracy: 0.9198 - val_loss: 0.3089 - val_accuracy: 0.8874\n",
      "Epoch 2/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.2227 - accuracy: 0.9204 - val_loss: 0.3036 - val_accuracy: 0.8908\n",
      "Epoch 3/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2180 - accuracy: 0.9212 - val_loss: 0.3032 - val_accuracy: 0.8916\n",
      "Epoch 4/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2151 - accuracy: 0.9228 - val_loss: 0.3057 - val_accuracy: 0.8920\n",
      "Epoch 5/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2113 - accuracy: 0.9240 - val_loss: 0.3037 - val_accuracy: 0.8934\n",
      "Epoch 6/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2085 - accuracy: 0.9251 - val_loss: 0.3047 - val_accuracy: 0.8930\n",
      "Epoch 7/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2054 - accuracy: 0.9265 - val_loss: 0.3094 - val_accuracy: 0.8866\n",
      "Epoch 8/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2012 - accuracy: 0.9280 - val_loss: 0.2930 - val_accuracy: 0.8940\n",
      "Epoch 9/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.1984 - accuracy: 0.9289 - val_loss: 0.3036 - val_accuracy: 0.8904\n",
      "Epoch 10/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.1954 - accuracy: 0.9303 - val_loss: 0.2986 - val_accuracy: 0.8942\n",
      "Epoch 11/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.1925 - accuracy: 0.9315 - val_loss: 0.2996 - val_accuracy: 0.8948\n",
      "Epoch 12/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.1898 - accuracy: 0.9332 - val_loss: 0.2972 - val_accuracy: 0.8936\n",
      "Epoch 13/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.1861 - accuracy: 0.9335 - val_loss: 0.2882 - val_accuracy: 0.8964\n",
      "Epoch 14/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.1826 - accuracy: 0.9354 - val_loss: 0.2955 - val_accuracy: 0.8972\n",
      "Epoch 15/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.1808 - accuracy: 0.9355 - val_loss: 0.2849 - val_accuracy: 0.8998\n",
      "Epoch 16/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.1773 - accuracy: 0.9375 - val_loss: 0.3095 - val_accuracy: 0.8930\n",
      "Epoch 17/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.1740 - accuracy: 0.9382 - val_loss: 0.3068 - val_accuracy: 0.8922\n",
      "Epoch 18/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.1715 - accuracy: 0.9403 - val_loss: 0.3558 - val_accuracy: 0.8846\n",
      "Epoch 19/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.1694 - accuracy: 0.9393 - val_loss: 0.3071 - val_accuracy: 0.8952\n",
      "Epoch 20/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.1673 - accuracy: 0.9412 - val_loss: 0.3002 - val_accuracy: 0.8932\n",
      "Epoch 21/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.1641 - accuracy: 0.9421 - val_loss: 0.2944 - val_accuracy: 0.8976\n",
      "Epoch 22/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.1621 - accuracy: 0.9423 - val_loss: 0.3095 - val_accuracy: 0.8958\n",
      "Epoch 23/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.1583 - accuracy: 0.9442 - val_loss: 0.2981 - val_accuracy: 0.8972\n",
      "Epoch 24/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.1575 - accuracy: 0.9447 - val_loss: 0.3060 - val_accuracy: 0.8986\n",
      "Epoch 25/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.1540 - accuracy: 0.9452 - val_loss: 0.3048 - val_accuracy: 0.8938\n",
      "Epoch 26/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.1526 - accuracy: 0.9456 - val_loss: 0.3003 - val_accuracy: 0.8974\n",
      "Epoch 27/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.1485 - accuracy: 0.9467 - val_loss: 0.2990 - val_accuracy: 0.8974\n",
      "Epoch 28/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.1472 - accuracy: 0.9482 - val_loss: 0.3125 - val_accuracy: 0.8966\n",
      "Epoch 29/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.1451 - accuracy: 0.9482 - val_loss: 0.3030 - val_accuracy: 0.8980\n",
      "Epoch 30/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.1421 - accuracy: 0.9492 - val_loss: 0.3075 - val_accuracy: 0.8976\n"
     ]
    }
   ],
   "source": [
    "if RELOAD:\n",
    "    model = keras.models.load_model(CHAPTER_DIR + 'model1.h5')\n",
    "    history = model.history\n",
    "else:\n",
    "    history = model.fit(X_train, y_train, epochs=30, validation_data=(X_valid, y_valid))\n",
    "    model.save(CHAPTER_DIR + 'model1.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: instead of passing a validation set, you can also set the ```validation_split``` argument of the ```fit``` method to the ratio of the training set you want Keras to use for validation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the training set is very skewed, with some classes being underrepresented, it can be useful to set the ```class_weight``` parameter, this would give larger weight to underrepresented classes and a lower weight to overrepresented classes.\n",
    "\n",
    "If you need per instance weights, the ```sample_weight``` can be used. Per-instance weights can be useful if some instances are labelled by experts while others were labeled using a crowdsourcing platform: we might want to give more weights to the former."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can access the ```history.history``` to access the loss and accuracy of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from jupyterthemes import jtplot\n",
    "jtplot.style()\n",
    "\n",
    "pd.DataFrame(history.history).plot(figsize=(10, 5))\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0,1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the model performed better on the validation set than on the training set at the beginning, but this is not true.\n",
    "\n",
    "The validation error is computed at the *end* of each epoch while the training error is computed using a running mean *during* each epoch. So the training curve should actually be shifted half an epoch to the left. \n",
    "\n",
    "We can also tell that the model has not quite converged yet as the validation is still going down. We can continue training from where we left off by calling the fit method again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.1402 - accuracy: 0.9503 - val_loss: 0.3160 - val_accuracy: 0.8932\n",
      "Epoch 2/20\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.1380 - accuracy: 0.9513 - val_loss: 0.3029 - val_accuracy: 0.8976\n",
      "Epoch 3/20\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.1353 - accuracy: 0.9525 - val_loss: 0.3038 - val_accuracy: 0.8992\n",
      "Epoch 4/20\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.1327 - accuracy: 0.9540 - val_loss: 0.3202 - val_accuracy: 0.8990\n",
      "Epoch 5/20\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.1305 - accuracy: 0.9545 - val_loss: 0.3145 - val_accuracy: 0.8994\n",
      "Epoch 6/20\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.1281 - accuracy: 0.9554 - val_loss: 0.3096 - val_accuracy: 0.8942\n",
      "Epoch 7/20\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.1279 - accuracy: 0.9549 - val_loss: 0.3234 - val_accuracy: 0.8936\n",
      "Epoch 8/20\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.1251 - accuracy: 0.9563 - val_loss: 0.3110 - val_accuracy: 0.9010\n",
      "Epoch 9/20\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.1216 - accuracy: 0.9580 - val_loss: 0.3315 - val_accuracy: 0.8898\n",
      "Epoch 10/20\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.1215 - accuracy: 0.9567 - val_loss: 0.3581 - val_accuracy: 0.8892\n",
      "Epoch 11/20\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.1200 - accuracy: 0.9584 - val_loss: 0.3350 - val_accuracy: 0.8948\n",
      "Epoch 12/20\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.1166 - accuracy: 0.9591 - val_loss: 0.3456 - val_accuracy: 0.8944\n",
      "Epoch 13/20\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.1147 - accuracy: 0.9597 - val_loss: 0.3161 - val_accuracy: 0.9006\n",
      "Epoch 14/20\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.1126 - accuracy: 0.9606 - val_loss: 0.3192 - val_accuracy: 0.8980\n",
      "Epoch 15/20\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.1120 - accuracy: 0.9606 - val_loss: 0.3120 - val_accuracy: 0.8996\n",
      "Epoch 16/20\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.1090 - accuracy: 0.9629 - val_loss: 0.3206 - val_accuracy: 0.8960\n",
      "Epoch 17/20\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.1083 - accuracy: 0.9621 - val_loss: 0.3443 - val_accuracy: 0.8896\n",
      "Epoch 18/20\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.1052 - accuracy: 0.9643 - val_loss: 0.3368 - val_accuracy: 0.8950\n",
      "Epoch 19/20\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.1043 - accuracy: 0.9641 - val_loss: 0.3279 - val_accuracy: 0.9002\n",
      "Epoch 20/20\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.1028 - accuracy: 0.9647 - val_loss: 0.3345 - val_accuracy: 0.8962\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if RELOAD:\n",
    "    model = keras.models.load_model(CHAPTER_DIR + 'model1_hist2.h5')\n",
    "    history2 = model.history\n",
    "else:\n",
    "    history2 = model.fit(X_train, y_train, epochs=20, validation_data=(X_valid, y_valid))\n",
    "    model.save(CHAPTER_DIR + 'model1_hist2.h5')\n",
    "\n",
    "pd.DataFrame(history2.history).plot(figsize=(10,5))\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0,1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To improve the model's performance, we perform our usual task: tuning hyperparameters.\n",
    "\n",
    "The first one to check is the learning rate. If that doesn't help we should check the optimizer (always re-tuning the learning rate after changing any other hyperparameter). If performance is still not great, we can tune the architecture, by changing the number of layers, neurons in each hidden layer and the activation function in the hidden layers.\n",
    "\n",
    "To estimate the generalisation error, we use the ```evaluation()``` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 1ms/step - loss: 96.7621 - accuracy: 0.8437\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[96.76213073730469, 0.8436999917030334]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use this model to make predictions on the first 3 instances of the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new = X_test[:3]\n",
    "y_proba = model.predict(X_new)\n",
    "y_proba.round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each instance, the model estimated the probability per class, from class 0 to class 9. If we don't care about probabilities and only want the class that has the highest probability we can use ```predict_classes()``` instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-29-81ace37e545f>:1: Sequential.predict_classes (from tensorflow.python.keras.engine.sequential) is deprecated and will be removed after 2021-01-01.\n",
      "Instructions for updating:\n",
      "Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([9, 2, 1])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model.predict_classes(X_new)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Ankle boot', 'Pullover', 'Trouser'], dtype='<U11')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.array(class_names)[y_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 518.4x172.8 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(7.2, 2.4))\n",
    "for index, image in enumerate(X_new):\n",
    "    plt.subplot(1, 3, index + 1)\n",
    "    plt.imshow(image, cmap=\"binary\", interpolation=\"nearest\")\n",
    "    plt.axis('off')\n",
    "    plt.title(class_names[y_test[index]], fontsize=12)\n",
    "plt.subplots_adjust(wspace=0.2, hspace=0.5)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a regression MLP using the Sequential API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use scikit-learn to import a simplified version of the California housing dataset (only numerical features and no missing values). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(housing.data, housing.target)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_valid = scaler.transform(X_valid)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building the model is pretty similar to the steps taken for the example above. Since we're predicting house prices we only need 1 output and the output layer won't use an activation function and, we'll also use mean squared error as the error function. \n",
    "\n",
    "Since the dataset is noisy, we'll use a single layer with a few neurons to avoid overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11610 samples, validate on 3870 samples\n",
      "Epoch 1/20\n",
      "11610/11610 [==============================] - 0s 40us/step - loss: 0.9333 - val_loss: 0.9546\n",
      "Epoch 2/20\n",
      "11610/11610 [==============================] - 0s 33us/step - loss: 0.7449 - val_loss: 0.4648\n",
      "Epoch 3/20\n",
      "11610/11610 [==============================] - 0s 32us/step - loss: 0.4770 - val_loss: 0.4468\n",
      "Epoch 4/20\n",
      "11610/11610 [==============================] - 0s 32us/step - loss: 0.4573 - val_loss: 0.4297\n",
      "Epoch 5/20\n",
      "11610/11610 [==============================] - 0s 30us/step - loss: 0.4460 - val_loss: 0.4181\n",
      "Epoch 6/20\n",
      "11610/11610 [==============================] - 0s 32us/step - loss: 0.4385 - val_loss: 0.4158\n",
      "Epoch 7/20\n",
      "11610/11610 [==============================] - 0s 30us/step - loss: 0.4337 - val_loss: 0.4084\n",
      "Epoch 8/20\n",
      "11610/11610 [==============================] - 0s 29us/step - loss: 0.4280 - val_loss: 0.4062\n",
      "Epoch 9/20\n",
      "11610/11610 [==============================] - 0s 33us/step - loss: 0.4237 - val_loss: 0.4444\n",
      "Epoch 10/20\n",
      "11610/11610 [==============================] - 0s 31us/step - loss: 0.4213 - val_loss: 0.3967\n",
      "Epoch 11/20\n",
      "11610/11610 [==============================] - 0s 31us/step - loss: 0.4160 - val_loss: 0.3952\n",
      "Epoch 12/20\n",
      "11610/11610 [==============================] - 0s 33us/step - loss: 0.4158 - val_loss: 0.3943\n",
      "Epoch 13/20\n",
      "11610/11610 [==============================] - 0s 32us/step - loss: 0.4170 - val_loss: 0.3924\n",
      "Epoch 14/20\n",
      "11610/11610 [==============================] - 0s 34us/step - loss: 0.4052 - val_loss: 0.3976\n",
      "Epoch 15/20\n",
      "11610/11610 [==============================] - 0s 35us/step - loss: 0.4015 - val_loss: 0.3878\n",
      "Epoch 16/20\n",
      "11610/11610 [==============================] - 0s 33us/step - loss: 0.3990 - val_loss: 0.3827\n",
      "Epoch 17/20\n",
      "11610/11610 [==============================] - 0s 30us/step - loss: 0.3955 - val_loss: 0.3816\n",
      "Epoch 18/20\n",
      "11610/11610 [==============================] - 0s 30us/step - loss: 0.3925 - val_loss: 0.3794\n",
      "Epoch 19/20\n",
      "11610/11610 [==============================] - 0s 33us/step - loss: 0.3905 - val_loss: 0.3756\n",
      "Epoch 20/20\n",
      "11610/11610 [==============================] - 0s 31us/step - loss: 0.3890 - val_loss: 0.3759\n",
      "5160/5160 [==============================] - 0s 14us/step\n"
     ]
    }
   ],
   "source": [
    "if RELOAD:\n",
    "    model = keras.models.load_model(CHAPTER_DIR + 'model2.h5')\n",
    "else:\n",
    "    model = Sequential([\n",
    "        Dense(30, activation='relu', input_shape=X_train.shape[1:]),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    model.compile(loss='mean_squared_error', optimizer='sgd')\n",
    "    history = model.fit(X_train, y_train, epochs=20, validation_data=(X_valid, y_valid))\n",
    "    model.save(CHAPTER_DIR + 'model2.h5')\n",
    "\n",
    "mse_test = model.evaluate(X_test, y_test)\n",
    "X_new = X_test[:3]\n",
    "y_pred = model.predict(X_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 460.8x403.2 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "width = 0.35\n",
    "actual = y_test[:X_new.shape[0]]\n",
    "preds = y_pred.reshape(1,-1)[0]\n",
    "x_pos = np.arange(X_new.shape[0])\n",
    "\n",
    "ax.bar(x= x_pos-width/2.0, height=actual, width=width, label='actual')\n",
    "ax.bar(x= x_pos+width/2.0, height=preds, width=width, label='predicted')\n",
    "ax.legend()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sequential API is quite easy to use, however for more complex models Keras offers the Functional API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building complex models with the Functional API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One example of a non-sequential networs is a *Wide and Deep* NN, which was first introduced in a [2016 paper](https://homl.info/widedeep). This architecture allows for the Neural Net to learn both deep pattens (using the deep path) and simple rules (through the short path) (example image on page 309).\n",
    "\n",
    "In contrast, a regular MLP forces the data to go through the full stack of layers; thus, simple patterns in the data may end up being distorted by this sequence of transformations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One example network for the California housing dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input\n",
    "\n",
    "input_ = keras.layers.Input(shape=X_train.shape[1:])\n",
    "hidden1 = keras.layers.Dense(30, activation=\"relu\")(input_)\n",
    "hidden2 = keras.layers.Dense(30, activation=\"relu\")(hidden1)\n",
    "concat = keras.layers.concatenate([input_, hidden2])\n",
    "output = keras.layers.Dense(1)(concat)\n",
    "model = keras.models.Model(inputs=[input_], outputs=[output])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a breakdown of what's happening in the code above:\n",
    "- We create an Input object, containint a specification of what type of input the model will get (a model may have multiple inputs)\n",
    "- Next create a Dense layer, with 30 neurons using ReLU. We call it like a function passing it the inputs (hence functional api)\n",
    "- Create a second hidden layer as above and use it as a function. We pass it the outputs of the first hidden layer\n",
    "- Next we use the ```concatenate``` function, which creates a Concatenate layer and pass calls it with the given inputs\n",
    "- Create output layer with single neuron, passing it the result of concatenation\n",
    "- Finally create the model, specifying the inputs and outputs\n",
    "\n",
    "Now that's done we do exactly as above, comple, train, evaluate the model and use it to make predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we wanted to send some features through the wide path and a different subset (possibly overlapping) through the deep path (see fig on pg 310)? One solution is to use multiple inputs. For example we could send 5 features through the wide path (feats 0 to 4) and six through the deep path (feats 2 to 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_A = keras.layers.Input(shape=[5], name=\"wide_input\")\n",
    "input_B = keras.layers.Input(shape=[6], name=\"deep_input\")\n",
    "hidden1 = keras.layers.Dense(30, activation=\"relu\")(input_B)\n",
    "hidden2 = keras.layers.Dense(30, activation=\"relu\")(hidden1)\n",
    "concat = keras.layers.concatenate([input_A, hidden2])\n",
    "output = keras.layers.Dense(1, name=\"output\")(concat)\n",
    "model = keras.models.Model(inputs=[input_A, input_B], outputs=[output])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A good tip is to name the most important layers, especially as the model gets more complex. Also note that we specified ```inputs=[input_A, input_B]``` so when we're calling the fit method we need to pass a pair of matrices (X_train_A, X_train_B)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 1.7456 - val_loss: 0.8375\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.7669 - val_loss: 0.6892\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.6739 - val_loss: 0.6246\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.6292 - val_loss: 0.5894\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5971 - val_loss: 0.5637\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5750 - val_loss: 0.5484\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5572 - val_loss: 0.5289\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5435 - val_loss: 0.5167\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5323 - val_loss: 0.5071\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5231 - val_loss: 0.4988\n",
      "Epoch 11/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5154 - val_loss: 0.4959\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5086 - val_loss: 0.4872\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5034 - val_loss: 0.4832\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4989 - val_loss: 0.4782\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4949 - val_loss: 0.4758\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4914 - val_loss: 0.4718\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4880 - val_loss: 0.4684\n",
      "Epoch 18/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4855 - val_loss: 0.4664\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4832 - val_loss: 0.4633\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4808 - val_loss: 0.4627\n",
      "162/162 [==============================] - 0s 764us/step - loss: 0.5077\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='mse', optimizer=keras.optimizers.SGD(lr=1e-3))\n",
    "\n",
    "X_train_A, X_train_B = X_train[:, :5], X_train[:, 2:]\n",
    "X_valid_A, X_valid_B = X_valid[:, :5], X_valid[:, 2:]\n",
    "X_test_A, X_test_B = X_test[:, :5], X_test[:, 2:]\n",
    "X_new_A, X_new_B = X_test_A[:3], X_test_B[:3]\n",
    "\n",
    "if RELOAD:\n",
    "    model = keras.models.load_model(CHAPTER_DIR + 'model3.h5')\n",
    "else:    \n",
    "    history = model.fit((X_train_A, X_train_B), y_train, epochs=20,\n",
    "                         validation_data=((X_valid_A, X_valid_B), y_valid))\n",
    "    model.save(CHAPTER_DIR + 'model3.h5')\n",
    "    \n",
    "mse_test = model.evaluate((X_test_A, X_test_B), y_test)\n",
    "y_pred = model.predict((X_new_A, X_new_B))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are also cases where you may want to have multiple outputs:\n",
    "- The task may demand it; e.g. locate and classify the object in a picture. This is both regression task (finding the coordinates of the object center) and classification task\n",
    "- There may be multiple independent tasks based on the same data. For example you could perform *multitask classification* on pictures of faces to classify the person's facial expression and another to identify whether they're wearing sunglasses or not\n",
    "- Another use case is as a regularization technique. For example you may want to add some auxiliary outputs in a neural network architecture to ensure that the underlying part of the network learns something useful on its own, without relying on the rest of the network (see fig 10-16 pg 312)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To add extra outputs, simply connect them to the appropriate layers and add them to the model's list of outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continuing from the architecture above\n",
    "# ...\n",
    "output = keras.layers.Dense(1, name='main_output')(concat)\n",
    "aux_output = keras.layers.Dense(1, name='aux_output')(hidden2)\n",
    "model = keras.models.Model(inputs=[input_A, input_B], outputs=[output, aux_output])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each output needs its own loss function, when compiling the model we need to passa list of losses (or a single loss, Keras will assume that the same loss must be used for all). By default, Keras will compute all these losses and simply add them to arrive at a final loss for training. If we care more about the main output's loss, we can specify loss weights when compiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=['mse', 'mse'], loss_weights=[0.9, 0.1], optimizer='sgd')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now when training the model we also need to pass the labels to the auxiliary output. In this case they're trying to predict the same output so we just pass y_train again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.7921 - main_output_loss: 0.6716 - aux_output_loss: 1.8774 - val_loss: 0.5427 - val_main_output_loss: 0.4782 - val_aux_output_loss: 1.1231\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5464 - main_output_loss: 0.4898 - aux_output_loss: 1.0560 - val_loss: 0.5206 - val_main_output_loss: 0.4666 - val_aux_output_loss: 1.0060\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5139 - main_output_loss: 0.4663 - aux_output_loss: 0.9417 - val_loss: 0.4870 - val_main_output_loss: 0.4423 - val_aux_output_loss: 0.8894\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4936 - main_output_loss: 0.4548 - aux_output_loss: 0.8429 - val_loss: 0.4734 - val_main_output_loss: 0.4378 - val_aux_output_loss: 0.7941\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.4716 - main_output_loss: 0.4393 - aux_output_loss: 0.7624 - val_loss: 0.4552 - val_main_output_loss: 0.4249 - val_aux_output_loss: 0.7273\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4604 - main_output_loss: 0.4326 - aux_output_loss: 0.7108 - val_loss: 0.4398 - val_main_output_loss: 0.4131 - val_aux_output_loss: 0.6803\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4482 - main_output_loss: 0.4231 - aux_output_loss: 0.6736 - val_loss: 0.4277 - val_main_output_loss: 0.4036 - val_aux_output_loss: 0.6447\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4358 - main_output_loss: 0.4124 - aux_output_loss: 0.6464 - val_loss: 0.4135 - val_main_output_loss: 0.3905 - val_aux_output_loss: 0.6200\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4350 - main_output_loss: 0.4135 - aux_output_loss: 0.6281 - val_loss: 0.4166 - val_main_output_loss: 0.3959 - val_aux_output_loss: 0.6024\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4931 - main_output_loss: 0.4790 - aux_output_loss: 0.6197 - val_loss: 0.4804 - val_main_output_loss: 0.4663 - val_aux_output_loss: 0.6069\n",
      "Epoch 11/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4724 - main_output_loss: 0.4561 - aux_output_loss: 0.6190 - val_loss: 0.4117 - val_main_output_loss: 0.3922 - val_aux_output_loss: 0.5879\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4284 - main_output_loss: 0.4092 - aux_output_loss: 0.6011 - val_loss: 0.4083 - val_main_output_loss: 0.3891 - val_aux_output_loss: 0.5808\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.4220 - main_output_loss: 0.4035 - aux_output_loss: 0.5889 - val_loss: 0.4009 - val_main_output_loss: 0.3819 - val_aux_output_loss: 0.5711\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4192 - main_output_loss: 0.4009 - aux_output_loss: 0.5843 - val_loss: 0.3934 - val_main_output_loss: 0.3761 - val_aux_output_loss: 0.5489\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4223 - main_output_loss: 0.4072 - aux_output_loss: 0.5585 - val_loss: 0.5043 - val_main_output_loss: 0.4953 - val_aux_output_loss: 0.5857\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4145 - main_output_loss: 0.3984 - aux_output_loss: 0.5599 - val_loss: 0.3770 - val_main_output_loss: 0.3608 - val_aux_output_loss: 0.5233\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3916 - main_output_loss: 0.3755 - aux_output_loss: 0.5365 - val_loss: 0.3726 - val_main_output_loss: 0.3567 - val_aux_output_loss: 0.5154\n",
      "Epoch 18/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3896 - main_output_loss: 0.3740 - aux_output_loss: 0.5303 - val_loss: 0.3734 - val_main_output_loss: 0.3583 - val_aux_output_loss: 0.5096\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3814 - main_output_loss: 0.3660 - aux_output_loss: 0.5202 - val_loss: 0.3648 - val_main_output_loss: 0.3499 - val_aux_output_loss: 0.4988\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3795 - main_output_loss: 0.3647 - aux_output_loss: 0.5134 - val_loss: 0.3778 - val_main_output_loss: 0.3640 - val_aux_output_loss: 0.5027\n"
     ]
    }
   ],
   "source": [
    "if RELOAD:\n",
    "    model = keras.models.load_model(CHAPTER_DIR + 'model4.h5')\n",
    "else:\n",
    "    history = model.fit([X_train_A, X_train_B], [y_train, y_train], epochs=20,\n",
    "                        validation_data=([X_valid_A, X_valid_B], [y_valid, y_valid]))\n",
    "    model.save(CHAPTER_DIR + 'model4.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we evalute the model, keras returns the total loss as well as the individual losses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162/162 [==============================] - 0s 847us/step - loss: 0.4106 - main_output_loss: 0.3964 - aux_output_loss: 0.5379\n"
     ]
    }
   ],
   "source": [
    "total_loss, main_loss, aux_loss = model.evaluate([X_test_A, X_test_B], [y_test, y_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly for the ```predict()``` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_main, y_pred_aux = model.predict([X_new_A, X_new_B])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subclassing to build Dynamic Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sequential and functional API are delcarative; you specify which layers you want to use and how are they connected. This means they're easily saved, used, cloned and shared among other advantages. The flip side is that they're static.\n",
    "\n",
    "Some models may have loops, branches, varying shapes and other dynamic behaviors. For these, you can use the subclassing API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simply sublclass the Model class and create the layers you want in the ```call()``` method. The example below builds a class for the ```WideAndDeepModel``` discussed above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WideAndDeepModel(keras.models.Model):\n",
    "    def __init__(self, units=30, activation=\"relu\", **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.hidden1 = keras.layers.Dense(units, activation=activation)\n",
    "        self.hidden2 = keras.layers.Dense(units, activation=activation)\n",
    "        self.main_output = keras.layers.Dense(1)\n",
    "        self.aux_output = keras.layers.Dense(1)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        input_A, input_B = inputs\n",
    "        hidden1 = self.hidden1(input_B)\n",
    "        hidden2 = self.hidden2(hidden1)\n",
    "        concat = keras.layers.concatenate([input_A, hidden2])\n",
    "        main_output = self.main_output(concat)\n",
    "        aux_output = self.aux_output(hidden2)\n",
    "        return main_output, aux_output\n",
    "\n",
    "model = WideAndDeepModel(30, activation=\"relu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usage is similar to the functional API, except we do not need to create the inputs. We just use the input argument to the ```call()``` method and separaate the creation of layers in the constructor from their usage in the ```call()``` method. This enables you to use loops, if statements, low-level tensorflow operations, etc...\n",
    "\n",
    "The cost is that the model architecture is hidden behind the call method. Keras cannot check types, and shapes ahead of time and it is easier to make mistakes. \n",
    "\n",
    "**Note:** Keras models can be used like regular layers so you can easily combine them to build complex architectures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and restoring a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras uses HDF5 to save the model's architecture and the values of the model parameters for every layer (weights and biases). It also saves the optimizer. You would have seen the save function scattered through this notebook and the load function as well to re-store the models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this works when using the Sequential or Functionla APIs but not when using model subclassing. You can use ```save_weights()``` and ```load_weights()``` to at least save and restore the model parameters, but you will need to save and restore everything else yourself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if training lasts several hours? Which is quite common, especially on large datasets. In this case we should not only save the final model but save at regular checkpoints during training to avoid losing everything due to a crash. \n",
    "\n",
    "The ```fit()``` method accepts a ```callbacks``` argument that lets you specify a list of objects that Keras will call at the start and end of training, at the start and end of each epoch, and even before and after processing each batch.\n",
    "\n",
    "The ```ModelCheckpoint``` callback saves checkpoints at regular intervals during training, at the end of each epoch. Moreover, if we use a validation set while training, you can set ```save_best_only=True``` when creating the model checkpoint. Then it only saves the model when its performance on the validation set is the best so far. This way we don't need to save the train the model for too long and overfitting the training set: simply restore the last model saved after training and this will be the best model on the validation set. \n",
    "The following is a simple way of implementing early stopping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 1.8037 - val_loss: 0.8239\n",
      "Epoch 2/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.8041 - val_loss: 0.6976\n",
      "Epoch 3/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.6867 - val_loss: 0.6432\n",
      "Epoch 4/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.6403 - val_loss: 0.6037\n",
      "Epoch 5/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.6088 - val_loss: 0.5770\n",
      "Epoch 6/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5860 - val_loss: 0.5556\n",
      "Epoch 7/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5673 - val_loss: 0.5375\n",
      "Epoch 8/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5518 - val_loss: 0.5226\n",
      "Epoch 9/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5387 - val_loss: 0.5100\n",
      "Epoch 10/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5278 - val_loss: 0.4994\n",
      "162/162 [==============================] - 0s 944us/step - loss: 0.5475\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", input_shape=[8]),\n",
    "    keras.layers.Dense(30, activation=\"relu\"),\n",
    "    keras.layers.Dense(1)\n",
    "])\n",
    "model.compile(loss=\"mse\", optimizer=keras.optimizers.SGD(lr=1e-3))\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(CHAPTER_DIR + \"my_keras_model.h5\", save_best_only=True)\n",
    "history = model.fit(X_train, y_train, epochs=10,\n",
    "                    validation_data=(X_valid, y_valid),\n",
    "                    callbacks=[checkpoint_cb])\n",
    "model = keras.models.load_model(CHAPTER_DIR + \"my_keras_model.h5\") # rollback to best model\n",
    "mse_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way of implementing early stopping is to simply use the ```EarlyStopping``` callback. It interrupts training when it measures no progress on the validation set for a number of epochs (defined by the ```patience``` arg). You can combine both callbacks to save checkpoints of your model and interrupt training early when there is no more progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.5179 - val_loss: 0.4913\n",
      "Epoch 2/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5089 - val_loss: 0.4829\n",
      "Epoch 3/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5011 - val_loss: 0.4763\n",
      "Epoch 4/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4942 - val_loss: 0.4685\n",
      "Epoch 5/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4876 - val_loss: 0.4625\n",
      "Epoch 6/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4824 - val_loss: 0.4576\n",
      "Epoch 7/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4768 - val_loss: 0.4515\n",
      "Epoch 8/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4720 - val_loss: 0.4469\n",
      "Epoch 9/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4675 - val_loss: 0.4433\n",
      "Epoch 10/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4640 - val_loss: 0.4403\n",
      "Epoch 11/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4604 - val_loss: 0.4378\n",
      "Epoch 12/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4569 - val_loss: 0.4345\n",
      "Epoch 13/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4539 - val_loss: 0.4326\n",
      "Epoch 14/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4510 - val_loss: 0.4282\n",
      "Epoch 15/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4484 - val_loss: 0.4262\n",
      "Epoch 16/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4456 - val_loss: 0.4238\n",
      "Epoch 17/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4426 - val_loss: 0.4210\n",
      "Epoch 18/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4408 - val_loss: 0.4197\n",
      "Epoch 19/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4382 - val_loss: 0.4164\n",
      "Epoch 20/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4360 - val_loss: 0.4188\n",
      "Epoch 21/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4343 - val_loss: 0.4126\n",
      "Epoch 22/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4328 - val_loss: 0.4123\n",
      "Epoch 23/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4310 - val_loss: 0.4097\n",
      "Epoch 24/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4291 - val_loss: 0.4085\n",
      "Epoch 25/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4278 - val_loss: 0.4070\n",
      "Epoch 26/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4261 - val_loss: 0.4063\n",
      "Epoch 27/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4244 - val_loss: 0.4034\n",
      "Epoch 28/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4233 - val_loss: 0.4029\n",
      "Epoch 29/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4218 - val_loss: 0.4017\n",
      "Epoch 30/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4203 - val_loss: 0.3998\n",
      "Epoch 31/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4194 - val_loss: 0.4004\n",
      "Epoch 32/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4179 - val_loss: 0.4017\n",
      "Epoch 33/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4168 - val_loss: 0.3995\n",
      "Epoch 34/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4156 - val_loss: 0.3955\n",
      "Epoch 35/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4141 - val_loss: 0.3959\n",
      "Epoch 36/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4131 - val_loss: 0.3936\n",
      "Epoch 37/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4120 - val_loss: 0.3931\n",
      "Epoch 38/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4109 - val_loss: 0.3931\n",
      "Epoch 39/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4098 - val_loss: 0.3918\n",
      "Epoch 40/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4085 - val_loss: 0.3901\n",
      "Epoch 41/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4077 - val_loss: 0.3907\n",
      "Epoch 42/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4066 - val_loss: 0.3887\n",
      "Epoch 43/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4054 - val_loss: 0.3897\n",
      "Epoch 44/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4047 - val_loss: 0.3870\n",
      "Epoch 45/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4037 - val_loss: 0.3864\n",
      "Epoch 46/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4027 - val_loss: 0.3865\n",
      "Epoch 47/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4018 - val_loss: 0.3840\n",
      "Epoch 48/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4009 - val_loss: 0.3832\n",
      "Epoch 49/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3998 - val_loss: 0.3826\n",
      "Epoch 50/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3987 - val_loss: 0.3816\n",
      "Epoch 51/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3980 - val_loss: 0.3811\n",
      "Epoch 52/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3969 - val_loss: 0.3805\n",
      "Epoch 53/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3959 - val_loss: 0.3813\n",
      "Epoch 54/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3953 - val_loss: 0.3779\n",
      "Epoch 55/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3943 - val_loss: 0.3771\n",
      "Epoch 56/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3932 - val_loss: 0.3787\n",
      "Epoch 57/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3924 - val_loss: 0.3753\n",
      "Epoch 58/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3915 - val_loss: 0.3742\n",
      "Epoch 59/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3908 - val_loss: 0.3746\n",
      "Epoch 60/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3896 - val_loss: 0.3729\n",
      "Epoch 61/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3888 - val_loss: 0.3732\n",
      "Epoch 62/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3878 - val_loss: 0.3745\n",
      "Epoch 63/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3868 - val_loss: 0.3704\n",
      "Epoch 64/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3858 - val_loss: 0.3724\n",
      "Epoch 65/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3850 - val_loss: 0.3691\n",
      "Epoch 66/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3844 - val_loss: 0.3684\n",
      "Epoch 67/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3833 - val_loss: 0.3682\n",
      "Epoch 68/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3827 - val_loss: 0.3684\n",
      "Epoch 69/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3818 - val_loss: 0.3673\n",
      "Epoch 70/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3805 - val_loss: 0.3657\n",
      "Epoch 71/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3801 - val_loss: 0.3649\n",
      "Epoch 72/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3793 - val_loss: 0.3653\n",
      "Epoch 73/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3785 - val_loss: 0.3648\n",
      "Epoch 74/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3775 - val_loss: 0.3633\n",
      "Epoch 75/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3769 - val_loss: 0.3635\n",
      "Epoch 76/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3759 - val_loss: 0.3624\n",
      "Epoch 77/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3751 - val_loss: 0.3639\n",
      "Epoch 78/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3745 - val_loss: 0.3608\n",
      "Epoch 79/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3737 - val_loss: 0.3612\n",
      "Epoch 80/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3730 - val_loss: 0.3613\n",
      "Epoch 81/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3721 - val_loss: 0.3595\n",
      "Epoch 82/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3714 - val_loss: 0.3584\n",
      "Epoch 83/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3704 - val_loss: 0.3607\n",
      "Epoch 84/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3700 - val_loss: 0.3580\n",
      "Epoch 85/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3692 - val_loss: 0.3591\n",
      "Epoch 86/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3685 - val_loss: 0.3568\n",
      "Epoch 87/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3677 - val_loss: 0.3581\n",
      "Epoch 88/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3672 - val_loss: 0.3563\n",
      "Epoch 89/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3665 - val_loss: 0.3552\n",
      "Epoch 90/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3655 - val_loss: 0.3553\n",
      "Epoch 91/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3651 - val_loss: 0.3537\n",
      "Epoch 92/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3644 - val_loss: 0.3531\n",
      "Epoch 93/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3634 - val_loss: 0.3528\n",
      "Epoch 94/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3632 - val_loss: 0.3528\n",
      "Epoch 95/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3626 - val_loss: 0.3514\n",
      "Epoch 96/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3615 - val_loss: 0.3517\n",
      "Epoch 97/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3611 - val_loss: 0.3517\n",
      "Epoch 98/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3607 - val_loss: 0.3498\n",
      "Epoch 99/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3600 - val_loss: 0.3498\n",
      "Epoch 100/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3595 - val_loss: 0.3517\n",
      "162/162 [==============================] - 0s 763us/step - loss: 0.3845\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=\"mse\", optimizer=keras.optimizers.SGD(lr=1e-3))\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n",
    "history = model.fit(X_train, y_train, epochs=100,\n",
    "                    validation_data=(X_valid, y_valid),\n",
    "                    callbacks=[checkpoint_cb, early_stopping_cb])\n",
    "mse_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This way we can also set the number of epochs to a large value since training will stop when there's no more progress."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also write your own custom callbacks. Below is an example of a callback that displays the ratio between validation loss and the training loss during training (e.g. to detect overfitting)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "310/363 [========================>.....] - ETA: 0s - loss: 0.3615\n",
      "val/train: 0.97\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3590 - val_loss: 0.3486\n"
     ]
    }
   ],
   "source": [
    "class PrintValTrainRatioCallback(keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        print(\"\\nval/train: {:.2f}\".format(logs[\"val_loss\"] / logs[\"loss\"]))\n",
    "        \n",
    "val_train_ratio_cb = PrintValTrainRatioCallback()\n",
    "history = model.fit(X_train, y_train, epochs=1,\n",
    "                    validation_data=(X_valid, y_valid),\n",
    "                    callbacks=[val_train_ratio_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also implement ```on_train_begin(), on_train_end(), on_epoch_begin(), on_epoch_end(), on_batch_begin(), on_batch_end()```. They can also be used during evaluation and predictions (e.g. for debugging), see notes on pg 316 for more on that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using TensorBoard for Vizualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorBoard is a neat viz tool that allows us to visualize training losses, learninng curves, view images generated by the model and others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To set it up we need to configure the TensorBoard server and its *event files*. In general you want to point the TensorBoard server to a root log directory and configure your program so that it writes to a different subdirectory every time it runs, this way the same server instances allows you to see and compare data from multiple runs, without getting things mixed up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "root_logdir = os.path.join(os.curdir, 'my_logs')\n",
    "\n",
    "def get_run_logdir():\n",
    "    import time\n",
    "    run_id = time.strftime(\"run_%Y_%m_%d-%H_%M_%S\")\n",
    "    return os.path.join(root_logdir, run_id)\n",
    "\n",
    "run_logdir = get_run_logdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And Keras provides a ```TensorBoard()``` callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3583 - val_loss: 0.3482\n",
      "Epoch 2/30\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3573 - val_loss: 0.3473\n",
      "Epoch 3/30\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3567 - val_loss: 0.3488\n",
      "Epoch 4/30\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3560 - val_loss: 0.3489\n",
      "Epoch 5/30\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3554 - val_loss: 0.3472\n",
      "Epoch 6/30\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3550 - val_loss: 0.3464\n",
      "Epoch 7/30\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3544 - val_loss: 0.3449\n",
      "Epoch 8/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3537 - val_loss: 0.3440\n",
      "Epoch 9/30\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3533 - val_loss: 0.3451\n",
      "Epoch 10/30\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3527 - val_loss: 0.3454\n",
      "Epoch 11/30\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3521 - val_loss: 0.3430\n",
      "Epoch 12/30\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3515 - val_loss: 0.3440\n",
      "Epoch 13/30\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3509 - val_loss: 0.3445\n",
      "Epoch 14/30\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3505 - val_loss: 0.3427\n",
      "Epoch 15/30\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3500 - val_loss: 0.3410\n",
      "Epoch 16/30\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3493 - val_loss: 0.3414\n",
      "Epoch 17/30\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3485 - val_loss: 0.3416\n",
      "Epoch 18/30\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3484 - val_loss: 0.3414\n",
      "Epoch 19/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3476 - val_loss: 0.3397\n",
      "Epoch 20/30\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3469 - val_loss: 0.3402\n",
      "Epoch 21/30\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3467 - val_loss: 0.3385\n",
      "Epoch 22/30\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3461 - val_loss: 0.3403\n",
      "Epoch 23/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3458 - val_loss: 0.3380\n",
      "Epoch 24/30\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3450 - val_loss: 0.3379\n",
      "Epoch 25/30\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3447 - val_loss: 0.3379\n",
      "Epoch 26/30\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3443 - val_loss: 0.3370\n",
      "Epoch 27/30\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3437 - val_loss: 0.3365\n",
      "Epoch 28/30\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3436 - val_loss: 0.3362\n",
      "Epoch 29/30\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3428 - val_loss: 0.3369\n",
      "Epoch 30/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3423 - val_loss: 0.3350\n"
     ]
    }
   ],
   "source": [
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\n",
    "history = model.fit(X_train, y_train, epochs=30,\n",
    "                    validation_data=(X_valid, y_valid),\n",
    "                    callbacks=[tensorboard_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we start the TensorBoard server with the following:\n",
    "\n",
    "``` tensorboard --logdir=./my_logs --port=6006 ```\n",
    "\n",
    "Then visit http://localhost:6006\n",
    "\n",
    "Note: to start tensorboard within Jupyter use the following:\n",
    "\n",
    "``` %load_ext tensorboard\n",
    "%tensorboard --logdir=./my_logs --port=6006```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, TensorFlow offers a lower-level API in the tf.summary package. You can use the following snippet to write logs to be visualized with TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_logdir = get_run_logdir()\n",
    "writer = tf.summary.create_file_writer(test_logdir)\n",
    "with writer.as_default():\n",
    "    for step in range(1, 1000 + 1):\n",
    "        tf.summary.scalar(\"my_scalar\", np.sin(step/10), step=step)\n",
    "        data = (np.random.randn(100)+2)*step / 100 # random data\n",
    "        tf.summary.histogram(\"my_hist\", data, buckets=50, step=step)\n",
    "        images = np.random.rand(2, 32, 32, 3) # Random 32x32 RGB images\n",
    "        tf.summary.image(\"my_images\", images*step/1000, step=step)\n",
    "        texts = [f\"The step is {step}. Its square is {step**2}\"]\n",
    "        tf.summary.text(\"my_text\", texts, step=step)\n",
    "        sine_wave = tf.math.sin(tf.range(12000) / 48000 * 2 * np.pi * step)\n",
    "        audio = tf.reshape(tf.cast(sine_wave, tf.float32), [1, -1, 1])\n",
    "        tf.summary.audio(\"my_audio\", audio, sample_rate=48000, step=step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning Neural Network Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One approach to tuning the many NN hyperparameters is to try many combinations and see which ones work best. We can use ```GridSearchCV``` or ```RandomSearchCV``` for this, we just have to wrap our Keras models in objects that mimic Scikit-Learn regressors. \n",
    "\n",
    "The first step is to create a function that builds and compiles the Keras model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(n_hidden=1, n_neurons=30, learning_rate=3e-3, input_shape=[8]):\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.InputLayer(input_shape=input_shape))\n",
    "    for layer in range(n_hidden):\n",
    "        model.add(keras.layers.Dense(n_neurons, activation=\"relu\"))\n",
    "    model.add(keras.layers.Dense(1))\n",
    "    optimizer = keras.optimizers.SGD(lr=learning_rate)\n",
    "    model.compile(loss=\"mse\", optimizer=optimizer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we create a KerasRegressor based on this build_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_reg = keras.wrappers.scikit_learn.KerasRegressor(build_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a thin wrapper around the Keras model built using build_model, using the pre-defined hyperparameters. Now we can use it like a regular Scikit-Learn regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 1.0803 - val_loss: 0.6906\n",
      "Epoch 2/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.6772 - val_loss: 0.5879\n",
      "Epoch 3/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5711 - val_loss: 0.5284\n",
      "Epoch 4/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5302 - val_loss: 0.4933\n",
      "Epoch 5/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5021 - val_loss: 0.4734\n",
      "Epoch 6/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.4861 - val_loss: 0.4628\n",
      "Epoch 7/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.4714 - val_loss: 0.4467\n",
      "Epoch 8/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4613 - val_loss: 0.4376\n",
      "Epoch 9/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.4545 - val_loss: 0.4318\n",
      "Epoch 10/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.4488 - val_loss: 0.4276\n",
      "Epoch 11/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4442 - val_loss: 0.4238\n",
      "Epoch 12/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.4396 - val_loss: 0.4201\n",
      "Epoch 13/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.4365 - val_loss: 0.4174\n",
      "Epoch 14/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.4333 - val_loss: 0.4120\n",
      "Epoch 15/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.4300 - val_loss: 0.4093\n",
      "Epoch 16/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4272 - val_loss: 0.4068\n",
      "Epoch 17/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.4238 - val_loss: 0.4049\n",
      "Epoch 18/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.4222 - val_loss: 0.4029\n",
      "Epoch 19/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4193 - val_loss: 0.3990\n",
      "Epoch 20/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4169 - val_loss: 0.3996\n",
      "Epoch 21/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4148 - val_loss: 0.3948\n",
      "Epoch 22/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4131 - val_loss: 0.3955\n",
      "Epoch 23/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.4111 - val_loss: 0.3913\n",
      "Epoch 24/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.4091 - val_loss: 0.3898\n",
      "Epoch 25/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4075 - val_loss: 0.3890\n",
      "Epoch 26/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4060 - val_loss: 0.3863\n",
      "Epoch 27/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.4042 - val_loss: 0.3850\n",
      "Epoch 28/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.4032 - val_loss: 0.3841\n",
      "Epoch 29/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.4014 - val_loss: 0.3839\n",
      "Epoch 30/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4001 - val_loss: 0.3807\n",
      "Epoch 31/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3987 - val_loss: 0.3818\n",
      "Epoch 32/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3971 - val_loss: 0.3854\n",
      "Epoch 33/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3961 - val_loss: 0.3818\n",
      "Epoch 34/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3944 - val_loss: 0.3772\n",
      "Epoch 35/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3934 - val_loss: 0.3773\n",
      "Epoch 36/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3919 - val_loss: 0.3762\n",
      "Epoch 37/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3917 - val_loss: 0.3748\n",
      "Epoch 38/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3901 - val_loss: 0.3741\n",
      "Epoch 39/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3890 - val_loss: 0.3729\n",
      "Epoch 40/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3874 - val_loss: 0.3727\n",
      "Epoch 41/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3875 - val_loss: 0.3723\n",
      "Epoch 42/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3861 - val_loss: 0.3707\n",
      "Epoch 43/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3863 - val_loss: 0.3736\n",
      "Epoch 44/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3844 - val_loss: 0.3699\n",
      "Epoch 45/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3833 - val_loss: 0.3685\n",
      "Epoch 46/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3830 - val_loss: 0.3715\n",
      "Epoch 47/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3817 - val_loss: 0.3668\n",
      "Epoch 48/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3815 - val_loss: 0.3666\n",
      "Epoch 49/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3803 - val_loss: 0.3666\n",
      "Epoch 50/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3813 - val_loss: 0.3635\n",
      "Epoch 51/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3790 - val_loss: 0.3692\n",
      "Epoch 52/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3792 - val_loss: 0.3670\n",
      "Epoch 53/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3781 - val_loss: 0.3655\n",
      "Epoch 54/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3773 - val_loss: 0.3640\n",
      "Epoch 55/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3783 - val_loss: 0.3599\n",
      "Epoch 56/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3762 - val_loss: 0.3756\n",
      "Epoch 57/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3766 - val_loss: 0.3610\n",
      "Epoch 58/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3739 - val_loss: 0.3607\n",
      "Epoch 59/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3743 - val_loss: 0.3606\n",
      "Epoch 60/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3727 - val_loss: 0.3612\n",
      "Epoch 61/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3727 - val_loss: 0.3611\n",
      "Epoch 62/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3729 - val_loss: 0.3605\n",
      "Epoch 63/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3741 - val_loss: 0.3609\n",
      "Epoch 64/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3728 - val_loss: 0.3620\n",
      "Epoch 65/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3707 - val_loss: 0.3661\n",
      "162/162 [==============================] - 0s 766us/step - loss: 0.4101\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-0.41007909178733826"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras_reg.fit(X_train, y_train, epochs=100,\n",
    "             validation_data=(X_valid, y_valid),\n",
    "             callbacks=[keras.callbacks.EarlyStopping(patience=10), \n",
    "                        tensorboard_cb])\n",
    "mse_test = keras_reg.score(X_test, y_test)\n",
    "y_pred = keras_reg.predict(X_new)\n",
    "mse_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any ```**kwargs``` passed to the fit method are passed to the underlying Keras model. Note the score will be the opposite of the MSE because Scikit-Learn wants scores, not losses (i.e. higher is better)\n",
    "\n",
    "Now we can use ```RandomizedSearchCV``` to explore the number of hidden layers, neurons and learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 1.8226 - val_loss: 0.8300\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.7233 - val_loss: 0.6320\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6075 - val_loss: 0.5726\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5695 - val_loss: 0.5384\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5439 - val_loss: 0.5146\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5246 - val_loss: 0.4959\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5089 - val_loss: 0.4816\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4967 - val_loss: 0.4698\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4874 - val_loss: 0.4598\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4790 - val_loss: 0.4530\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4725 - val_loss: 0.4466\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4668 - val_loss: 0.4408\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4617 - val_loss: 0.4357\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4577 - val_loss: 0.4318\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4540 - val_loss: 0.4287\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4504 - val_loss: 0.4251\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4470 - val_loss: 0.4272\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4440 - val_loss: 0.4242\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4421 - val_loss: 0.4200\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4393 - val_loss: 0.4174\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4372 - val_loss: 0.4175\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4355 - val_loss: 0.4129\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4329 - val_loss: 0.4105\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4311 - val_loss: 0.4083\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4296 - val_loss: 0.4082\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4279 - val_loss: 0.4079\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4261 - val_loss: 0.4066\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4244 - val_loss: 0.4042\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4229 - val_loss: 0.4056\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4214 - val_loss: 0.4024\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4195 - val_loss: 0.4014\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4178 - val_loss: 0.4003\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4165 - val_loss: 0.3980\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4152 - val_loss: 0.3965\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4137 - val_loss: 0.3974\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4120 - val_loss: 0.3960\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4106 - val_loss: 0.3938\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4094 - val_loss: 0.3918\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4074 - val_loss: 0.3968\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4067 - val_loss: 0.3912\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4050 - val_loss: 0.3891\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4038 - val_loss: 0.3880\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4022 - val_loss: 0.3878\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4009 - val_loss: 0.3857\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3991 - val_loss: 0.3858\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3973 - val_loss: 0.3838\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3958 - val_loss: 0.3854\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3943 - val_loss: 0.3852\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3927 - val_loss: 0.3811\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3912 - val_loss: 0.3780\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3903 - val_loss: 0.3785\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3893 - val_loss: 0.3777\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3877 - val_loss: 0.3750\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3867 - val_loss: 0.3744\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3857 - val_loss: 0.3758\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3844 - val_loss: 0.3729\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3835 - val_loss: 0.3745\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3819 - val_loss: 0.3719\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3812 - val_loss: 0.3716\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3799 - val_loss: 0.3696\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3787 - val_loss: 0.3695\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3780 - val_loss: 0.3680\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3769 - val_loss: 0.3660\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3755 - val_loss: 0.3665\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3746 - val_loss: 0.3656\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3735 - val_loss: 0.3660\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3724 - val_loss: 0.3640\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3715 - val_loss: 0.3634\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3705 - val_loss: 0.3635\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3695 - val_loss: 0.3627\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3687 - val_loss: 0.3634\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3674 - val_loss: 0.3634\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3666 - val_loss: 0.3593\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3659 - val_loss: 0.3585\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3654 - val_loss: 0.3575\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3643 - val_loss: 0.3602\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3634 - val_loss: 0.3560\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3627 - val_loss: 0.3585\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3623 - val_loss: 0.3566\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3614 - val_loss: 0.3565\n",
      "Epoch 81/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3608 - val_loss: 0.3538\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3594 - val_loss: 0.3560\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3597 - val_loss: 0.3554\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3584 - val_loss: 0.3519\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3580 - val_loss: 0.3518\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3576 - val_loss: 0.3526\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3565 - val_loss: 0.3510\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3563 - val_loss: 0.3511\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3551 - val_loss: 0.3504\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3548 - val_loss: 0.3497\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3541 - val_loss: 0.3496\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3537 - val_loss: 0.3512\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3531 - val_loss: 0.3495\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3528 - val_loss: 0.3490\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3519 - val_loss: 0.3500\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3517 - val_loss: 0.3483\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3508 - val_loss: 0.3498\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3505 - val_loss: 0.3509\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3500 - val_loss: 0.3479\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3496 - val_loss: 0.3468\n",
      "121/121 [==============================] - 0s 968us/step - loss: 0.3622\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 2.1488 - val_loss: 0.8217\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.8048 - val_loss: 0.6799\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6733 - val_loss: 0.6306\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6364 - val_loss: 0.5940\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6032 - val_loss: 0.5661\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5793 - val_loss: 0.5437\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5594 - val_loss: 0.5244\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5430 - val_loss: 0.5088\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5287 - val_loss: 0.4948\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5161 - val_loss: 0.4848\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5057 - val_loss: 0.4749\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4973 - val_loss: 0.4675\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4896 - val_loss: 0.4586\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4826 - val_loss: 0.4522\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4773 - val_loss: 0.4475\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4709 - val_loss: 0.4415\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.4658 - val_loss: 0.4375\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4625 - val_loss: 0.4346\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4590 - val_loss: 0.4297\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4560 - val_loss: 0.4292\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4522 - val_loss: 0.4266\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4492 - val_loss: 0.4207\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4477 - val_loss: 0.4216\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4442 - val_loss: 0.4175\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4413 - val_loss: 0.4144\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4398 - val_loss: 0.4173\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4372 - val_loss: 0.4096\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4349 - val_loss: 0.4064\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4332 - val_loss: 0.4086\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4313 - val_loss: 0.4056\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4296 - val_loss: 0.4026\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4275 - val_loss: 0.4024\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4265 - val_loss: 0.4002\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4247 - val_loss: 0.3974\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4241 - val_loss: 0.3969\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4215 - val_loss: 0.3962\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4203 - val_loss: 0.3936\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4186 - val_loss: 0.3936\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4182 - val_loss: 0.3916\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4159 - val_loss: 0.3908\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4145 - val_loss: 0.3888\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4145 - val_loss: 0.3912\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4130 - val_loss: 0.3884\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4110 - val_loss: 0.3869\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4098 - val_loss: 0.3848\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4093 - val_loss: 0.3850\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4074 - val_loss: 0.3847\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4056 - val_loss: 0.3827\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4069 - val_loss: 0.3811\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4047 - val_loss: 0.3797\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4024 - val_loss: 0.3890\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4117 - val_loss: 0.3789\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3996 - val_loss: 0.3774\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3987 - val_loss: 0.3772\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3972 - val_loss: 0.3789\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3961 - val_loss: 0.3744\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3953 - val_loss: 0.3762\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3940 - val_loss: 0.3736\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3929 - val_loss: 0.3731\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3916 - val_loss: 0.3716\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3911 - val_loss: 0.3702\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3905 - val_loss: 0.3686\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3895 - val_loss: 0.3694\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3883 - val_loss: 0.3667\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3870 - val_loss: 0.3672\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3856 - val_loss: 0.3679\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3846 - val_loss: 0.3680\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3859 - val_loss: 0.3651\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3836 - val_loss: 0.3648\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3823 - val_loss: 0.3645\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3836 - val_loss: 0.3642\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3815 - val_loss: 0.3631\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3798 - val_loss: 0.3611\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3816 - val_loss: 0.3607\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3800 - val_loss: 0.3626\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3801 - val_loss: 0.3597\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3772 - val_loss: 0.3604\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3767 - val_loss: 0.3617\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3771 - val_loss: 0.3578\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3744 - val_loss: 0.3581\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3751 - val_loss: 0.3572\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3733 - val_loss: 0.3561\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3728 - val_loss: 0.3566\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3721 - val_loss: 0.3540\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3718 - val_loss: 0.3547\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3715 - val_loss: 0.3608\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3708 - val_loss: 0.3546\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3702 - val_loss: 0.3527\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3686 - val_loss: 0.3541\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3689 - val_loss: 0.3537\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3675 - val_loss: 0.3572\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3680 - val_loss: 0.3580\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3674 - val_loss: 0.3501\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3706 - val_loss: 0.3497\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3669 - val_loss: 0.3506\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3638 - val_loss: 0.3480\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3633 - val_loss: 0.3488\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3621 - val_loss: 0.3491\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3615 - val_loss: 0.3483\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3620 - val_loss: 0.3454\n",
      "121/121 [==============================] - 0s 749us/step - loss: 0.3497\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 2.4556 - val_loss: 1.1131\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.9388 - val_loss: 0.7577\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.7378 - val_loss: 0.6691\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6688 - val_loss: 0.6256\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6267 - val_loss: 0.5933\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5964 - val_loss: 0.5684\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5723 - val_loss: 0.5470\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5518 - val_loss: 0.5268\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5329 - val_loss: 0.5099\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5167 - val_loss: 0.4967\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5034 - val_loss: 0.4843\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4917 - val_loss: 0.4736\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4821 - val_loss: 0.4643\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4731 - val_loss: 0.4559\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4659 - val_loss: 0.4496\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4596 - val_loss: 0.4442\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4538 - val_loss: 0.4390\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4491 - val_loss: 0.4350\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4451 - val_loss: 0.4320\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4411 - val_loss: 0.4288\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4367 - val_loss: 0.4283\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4345 - val_loss: 0.4242\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4310 - val_loss: 0.4191\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4287 - val_loss: 0.4205\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4259 - val_loss: 0.4162\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4238 - val_loss: 0.4144\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4217 - val_loss: 0.4117\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4196 - val_loss: 0.4102\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4177 - val_loss: 0.4092\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4164 - val_loss: 0.4088\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4146 - val_loss: 0.4061\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4127 - val_loss: 0.4046\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4117 - val_loss: 0.4032\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4097 - val_loss: 0.4026\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4088 - val_loss: 0.4011\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4074 - val_loss: 0.4001\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4061 - val_loss: 0.3987\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4042 - val_loss: 0.3969\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4032 - val_loss: 0.3958\n",
      "Epoch 40/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4020 - val_loss: 0.3954\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4011 - val_loss: 0.3939\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3998 - val_loss: 0.3934\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3984 - val_loss: 0.3922\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3978 - val_loss: 0.3914\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3965 - val_loss: 0.3901\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3955 - val_loss: 0.3894\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3950 - val_loss: 0.3884\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3937 - val_loss: 0.3882\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3931 - val_loss: 0.3861\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3923 - val_loss: 0.3862\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3911 - val_loss: 0.3854\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3901 - val_loss: 0.3837\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3891 - val_loss: 0.3827\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3881 - val_loss: 0.3833\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3870 - val_loss: 0.3819\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3858 - val_loss: 0.3818\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3850 - val_loss: 0.3805\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3839 - val_loss: 0.3796\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3831 - val_loss: 0.3786\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3818 - val_loss: 0.3783\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3813 - val_loss: 0.3772\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3799 - val_loss: 0.3762\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3794 - val_loss: 0.3758\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3785 - val_loss: 0.3752\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3776 - val_loss: 0.3749\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3768 - val_loss: 0.3735\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3757 - val_loss: 0.3743\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3748 - val_loss: 0.3735\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3743 - val_loss: 0.3730\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3736 - val_loss: 0.3716\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3727 - val_loss: 0.3700\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3717 - val_loss: 0.3690\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3711 - val_loss: 0.3686\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3702 - val_loss: 0.3679\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3697 - val_loss: 0.3694\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3691 - val_loss: 0.3678\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3679 - val_loss: 0.3660\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3676 - val_loss: 0.3680\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3671 - val_loss: 0.3654\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3660 - val_loss: 0.3647\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3656 - val_loss: 0.3648\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3650 - val_loss: 0.3637\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3641 - val_loss: 0.3638\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3638 - val_loss: 0.3639\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3628 - val_loss: 0.3623\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3625 - val_loss: 0.3618\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3614 - val_loss: 0.3608\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3608 - val_loss: 0.3606\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3605 - val_loss: 0.3634\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3595 - val_loss: 0.3598\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3589 - val_loss: 0.3621\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3586 - val_loss: 0.3594\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3576 - val_loss: 0.3594\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3574 - val_loss: 0.3581\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3563 - val_loss: 0.3595\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3558 - val_loss: 0.3565\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3552 - val_loss: 0.3563\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3547 - val_loss: 0.3570\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3537 - val_loss: 0.3555\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3531 - val_loss: 0.3546\n",
      "121/121 [==============================] - 0s 734us/step - loss: 0.4001\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 1.4619 - val_loss: 0.7338\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.8021 - val_loss: 0.6147\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6090 - val_loss: 0.5567\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5651 - val_loss: 0.5219\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5352 - val_loss: 0.4980\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5122 - val_loss: 0.4783\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4932 - val_loss: 0.4634\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4787 - val_loss: 0.4518\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4672 - val_loss: 0.4414\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4583 - val_loss: 0.4365\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4521 - val_loss: 0.4302\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4464 - val_loss: 0.4251\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4409 - val_loss: 0.4197\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4371 - val_loss: 0.4168\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4332 - val_loss: 0.4134\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4294 - val_loss: 0.4097\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4259 - val_loss: 0.4180\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4229 - val_loss: 0.4106\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4215 - val_loss: 0.4068\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4186 - val_loss: 0.4034\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4168 - val_loss: 0.4053\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4150 - val_loss: 0.3982\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4125 - val_loss: 0.3963\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4106 - val_loss: 0.3939\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4094 - val_loss: 0.3938\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4075 - val_loss: 0.3953\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4058 - val_loss: 0.3944\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4036 - val_loss: 0.3901\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4025 - val_loss: 0.3972\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4010 - val_loss: 0.3891\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3993 - val_loss: 0.3888\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3973 - val_loss: 0.3891\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3973 - val_loss: 0.3849\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3962 - val_loss: 0.3836\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3952 - val_loss: 0.3853\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3933 - val_loss: 0.3868\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3921 - val_loss: 0.3822\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3916 - val_loss: 0.3797\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3893 - val_loss: 0.3946\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3893 - val_loss: 0.3794\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3878 - val_loss: 0.3785\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3875 - val_loss: 0.3774\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3860 - val_loss: 0.3776\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3854 - val_loss: 0.3750\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3844 - val_loss: 0.3762\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3829 - val_loss: 0.3750\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3820 - val_loss: 0.3799\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3810 - val_loss: 0.3794\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3797 - val_loss: 0.3739\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3783 - val_loss: 0.3705\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3785 - val_loss: 0.3720\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3780 - val_loss: 0.3707\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3760 - val_loss: 0.3688\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3761 - val_loss: 0.3692\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3752 - val_loss: 0.3732\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3744 - val_loss: 0.3676\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3736 - val_loss: 0.3721\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3726 - val_loss: 0.3685\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3721 - val_loss: 0.3675\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3712 - val_loss: 0.3660\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3701 - val_loss: 0.3665\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3699 - val_loss: 0.3684\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3693 - val_loss: 0.3634\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3680 - val_loss: 0.3648\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3678 - val_loss: 0.3631\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3670 - val_loss: 0.3655\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3660 - val_loss: 0.3607\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3658 - val_loss: 0.3674\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3652 - val_loss: 0.3631\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3648 - val_loss: 0.3630\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3637 - val_loss: 0.3695\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3631 - val_loss: 0.3635\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3625 - val_loss: 0.3581\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3620 - val_loss: 0.3587\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3619 - val_loss: 0.3572\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3608 - val_loss: 0.3634\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3605 - val_loss: 0.3563\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3594 - val_loss: 0.3704\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3601 - val_loss: 0.3575\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3589 - val_loss: 0.3596\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3587 - val_loss: 0.3538\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3570 - val_loss: 0.3594\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3580 - val_loss: 0.3558\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3568 - val_loss: 0.3545\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3565 - val_loss: 0.3523\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3566 - val_loss: 0.3616\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3551 - val_loss: 0.3509\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3552 - val_loss: 0.3519\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3528 - val_loss: 0.3496\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3538 - val_loss: 0.3513\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3527 - val_loss: 0.3496\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3540 - val_loss: 0.3572\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3543 - val_loss: 0.3504\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3551 - val_loss: 0.3522\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3533 - val_loss: 0.3515\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3538 - val_loss: 0.3555\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3508 - val_loss: 0.3507\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3516 - val_loss: 0.3582\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3499 - val_loss: 0.3526\n",
      "Epoch 100/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3510 - val_loss: 0.3506\n",
      "121/121 [==============================] - 0s 800us/step - loss: 0.3606\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 1.3487 - val_loss: 0.6415\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6417 - val_loss: 0.5644\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5795 - val_loss: 0.5284\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5496 - val_loss: 0.5002\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5251 - val_loss: 0.4808\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5062 - val_loss: 0.4642\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4912 - val_loss: 0.4499\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4806 - val_loss: 0.4413\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4696 - val_loss: 0.4314\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4614 - val_loss: 0.4242\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4544 - val_loss: 0.4177\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4492 - val_loss: 0.4168\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4444 - val_loss: 0.4075\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4414 - val_loss: 0.4044\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4372 - val_loss: 0.4049\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4326 - val_loss: 0.3978\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4297 - val_loss: 0.3985\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4269 - val_loss: 0.3942\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4252 - val_loss: 0.3914\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4227 - val_loss: 0.3946\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4198 - val_loss: 0.3894\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4176 - val_loss: 0.3861\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4149 - val_loss: 0.3845\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4127 - val_loss: 0.3853\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4113 - val_loss: 0.3826\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4096 - val_loss: 0.3852\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4073 - val_loss: 0.3790\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4052 - val_loss: 0.3764\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4038 - val_loss: 0.3815\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4019 - val_loss: 0.3770\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4001 - val_loss: 0.3734\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3992 - val_loss: 0.3742\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3979 - val_loss: 0.3707\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3969 - val_loss: 0.3696\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3948 - val_loss: 0.3681\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3927 - val_loss: 0.3695\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3919 - val_loss: 0.3675\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3899 - val_loss: 0.3654\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3881 - val_loss: 0.3663\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3874 - val_loss: 0.3637\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3860 - val_loss: 0.3627\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3837 - val_loss: 0.3634\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3831 - val_loss: 0.3614\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3813 - val_loss: 0.3608\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3800 - val_loss: 0.3585\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3776 - val_loss: 0.3588\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3768 - val_loss: 0.3591\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3761 - val_loss: 0.3594\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3749 - val_loss: 0.3545\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3731 - val_loss: 0.3530\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3710 - val_loss: 0.3530\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3757 - val_loss: 0.3522\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3693 - val_loss: 0.3503\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3703 - val_loss: 0.3527\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3696 - val_loss: 0.3535\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3696 - val_loss: 0.3471\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3666 - val_loss: 0.3494\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3635 - val_loss: 0.3473\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3640 - val_loss: 0.3471\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3613 - val_loss: 0.3471\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3605 - val_loss: 0.3433\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3604 - val_loss: 0.3442\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3581 - val_loss: 0.3417\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3573 - val_loss: 0.3407\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3564 - val_loss: 0.3418\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3547 - val_loss: 0.3423\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3533 - val_loss: 0.3409\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3533 - val_loss: 0.3404\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3526 - val_loss: 0.3392\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3516 - val_loss: 0.3383\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3514 - val_loss: 0.3397\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3498 - val_loss: 0.3354\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3485 - val_loss: 0.3371\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3524 - val_loss: 0.3348\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3484 - val_loss: 0.3364\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3490 - val_loss: 0.3355\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3456 - val_loss: 0.3378\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3479 - val_loss: 0.3373\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3438 - val_loss: 0.3331\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3427 - val_loss: 0.3340\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3421 - val_loss: 0.3329\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3407 - val_loss: 0.3315\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3430 - val_loss: 0.3313\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3403 - val_loss: 0.3284\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3393 - val_loss: 0.3272\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3386 - val_loss: 0.3303\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3388 - val_loss: 0.3294\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3403 - val_loss: 0.3307\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3363 - val_loss: 0.3264\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3376 - val_loss: 0.3299\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3350 - val_loss: 0.3281\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3355 - val_loss: 0.3287\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3335 - val_loss: 0.3244\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3339 - val_loss: 0.3282\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3324 - val_loss: 0.3267\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3324 - val_loss: 0.3250\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3316 - val_loss: 0.3255\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3309 - val_loss: 0.3269\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3301 - val_loss: 0.3248\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3296 - val_loss: 0.3215\n",
      "121/121 [==============================] - 0s 866us/step - loss: 0.3211\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 1.4239 - val_loss: 0.6905\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6264 - val_loss: 0.5942\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5652 - val_loss: 0.5448\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5286 - val_loss: 0.5144\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5011 - val_loss: 0.4889\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4798 - val_loss: 0.4702\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4635 - val_loss: 0.4541\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4517 - val_loss: 0.4408\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4412 - val_loss: 0.4336\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4323 - val_loss: 0.4264\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4262 - val_loss: 0.4199\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4197 - val_loss: 0.4155\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4167 - val_loss: 0.4115\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4120 - val_loss: 0.4067\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4085 - val_loss: 0.4043\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4054 - val_loss: 0.4023\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4019 - val_loss: 0.3992\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3994 - val_loss: 0.3970\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3974 - val_loss: 0.3979\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3950 - val_loss: 0.3960\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3918 - val_loss: 0.3958\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3906 - val_loss: 0.3919\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3877 - val_loss: 0.3873\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3866 - val_loss: 0.3929\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3837 - val_loss: 0.3856\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3823 - val_loss: 0.3868\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3803 - val_loss: 0.3815\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3785 - val_loss: 0.3798\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3763 - val_loss: 0.3808\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3755 - val_loss: 0.3800\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3733 - val_loss: 0.3757\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3715 - val_loss: 0.3751\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3702 - val_loss: 0.3727\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3683 - val_loss: 0.3736\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3670 - val_loss: 0.3708\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3652 - val_loss: 0.3703\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3640 - val_loss: 0.3685\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3620 - val_loss: 0.3675\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3606 - val_loss: 0.3650\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3591 - val_loss: 0.3645\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3579 - val_loss: 0.3646\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3561 - val_loss: 0.3622\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3539 - val_loss: 0.3625\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3537 - val_loss: 0.3604\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3518 - val_loss: 0.3588\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3504 - val_loss: 0.3581\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3493 - val_loss: 0.3580\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3479 - val_loss: 0.3566\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3469 - val_loss: 0.3538\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3460 - val_loss: 0.3550\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3441 - val_loss: 0.3526\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3431 - val_loss: 0.3532\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3423 - val_loss: 0.3492\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3410 - val_loss: 0.3549\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3398 - val_loss: 0.3509\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3388 - val_loss: 0.3518\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3376 - val_loss: 0.3509\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3365 - val_loss: 0.3482\n",
      "Epoch 59/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3352 - val_loss: 0.3462\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3342 - val_loss: 0.3471\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3338 - val_loss: 0.3438\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3323 - val_loss: 0.3461\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3319 - val_loss: 0.3420\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3303 - val_loss: 0.3452\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3296 - val_loss: 0.3435\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3289 - val_loss: 0.3410\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3278 - val_loss: 0.3444\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3267 - val_loss: 0.3394\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3267 - val_loss: 0.3397\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3256 - val_loss: 0.3397\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3245 - val_loss: 0.3383\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3234 - val_loss: 0.3372\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3234 - val_loss: 0.3363\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3226 - val_loss: 0.3343\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3215 - val_loss: 0.3379\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3211 - val_loss: 0.3354\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3196 - val_loss: 0.3327\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3198 - val_loss: 0.3356\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3193 - val_loss: 0.3310\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3180 - val_loss: 0.3330\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3176 - val_loss: 0.3307\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3169 - val_loss: 0.3300\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3159 - val_loss: 0.3291\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3163 - val_loss: 0.3302\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3146 - val_loss: 0.3301\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3146 - val_loss: 0.3292\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3137 - val_loss: 0.3252\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3129 - val_loss: 0.3270\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3130 - val_loss: 0.3325\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3120 - val_loss: 0.3241\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3112 - val_loss: 0.3330\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3112 - val_loss: 0.3255\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3104 - val_loss: 0.3244\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3102 - val_loss: 0.3226\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3085 - val_loss: 0.3261\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3082 - val_loss: 0.3216\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3072 - val_loss: 0.3211\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3071 - val_loss: 0.3289\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3059 - val_loss: 0.3209\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3056 - val_loss: 0.3192\n",
      "121/121 [==============================] - 0s 884us/step - loss: 0.3576\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 1.1242 - val_loss: 1.2643\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 1.4883 - val_loss: 30.5870\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 664.3356 - val_loss: 1068.8141\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1061.4011 - val_loss: 40183.9102\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 959409.6250 - val_loss: 1432307.1250\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 1121814.0000 - val_loss: 52375592.0000\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 975496960.0000 - val_loss: 1837056640.0000\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 39632093184.0000 - val_loss: 63889862656.0000\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 88624824320.0000 - val_loss: 2397174497280.0000\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2102773809152.0000 - val_loss: 86851360653312.0000\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 68844169199616.0000 - val_loss: 3084677187371008.0000\n",
      "121/121 [==============================] - 0s 815us/step - loss: 4384160881836032.0000\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 7.5960 - val_loss: 23.3759\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 159.7732 - val_loss: 5058.7319\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 257857.7500 - val_loss: 1069224.5000\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 4880663.0000 - val_loss: 235764656.0000\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 12191519744.0000 - val_loss: 51336568832.0000\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 272448569344.0000 - val_loss: 11096084709376.0000\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2296579679584256.0000 - val_loss: 2352137229041664.0000\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 478835664629006336.0000 - val_loss: 493516206444118016.0000\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 15581339805911351296.0000 - val_loss: 111656074303722487808.0000\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 551664460520702869504.0000 - val_loss: 24521530265679488352256.0000\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 690197899884769700741120.0000 - val_loss: 5371145389471037785112576.0000\n",
      "121/121 [==============================] - 0s 754us/step - loss: 155016421189883799797760.0000\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 1.1509 - val_loss: 1.6923\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 1.4165 - val_loss: 6.7096\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.8504 - val_loss: 31.6468\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 22.0832 - val_loss: 147.0375\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 95.4079 - val_loss: 713.8648\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 519.9655 - val_loss: 3394.5164\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2549.4949 - val_loss: 16225.3330\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 11322.8623 - val_loss: 77316.5078\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 52026.4453 - val_loss: 368861.5312\n",
      "Epoch 10/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 1ms/step - loss: 252547.9062 - val_loss: 1761546.8750\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 1089302.7500 - val_loss: 8555601.0000\n",
      "121/121 [==============================] - 0s 853us/step - loss: 53920192.0000\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 1.6020 - val_loss: 0.5059\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4816 - val_loss: 1.1250\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 17.8796 - val_loss: 0.8268\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5254 - val_loss: 0.4998\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4336 - val_loss: 0.4563\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4088 - val_loss: 0.4235\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3965 - val_loss: 0.4169\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3888 - val_loss: 0.4121\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3841 - val_loss: 0.3896\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3741 - val_loss: 0.4017\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3699 - val_loss: 0.3987\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3707 - val_loss: 0.3791\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3641 - val_loss: 0.3729\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3581 - val_loss: 0.4256\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3552 - val_loss: 0.3695\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3838 - val_loss: 0.3758\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3490 - val_loss: 0.3904\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3448 - val_loss: 0.3643\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3495 - val_loss: 0.3656\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3400 - val_loss: 0.3670\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3400 - val_loss: 0.3579\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3388 - val_loss: 0.3634\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3382 - val_loss: 0.3577\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3598 - val_loss: 0.3566\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3475 - val_loss: 0.3694\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3388 - val_loss: 0.3626\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3451 - val_loss: 0.3700\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3333 - val_loss: 0.3571\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3328 - val_loss: 0.4091\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3277 - val_loss: 0.3823\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3262 - val_loss: 0.3521\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3248 - val_loss: 0.3543\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3250 - val_loss: 0.3505\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3279 - val_loss: 0.3513\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3271 - val_loss: 0.3525\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3231 - val_loss: 0.3568\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3273 - val_loss: 0.3481\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3237 - val_loss: 0.3380\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3221 - val_loss: 0.4629\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3195 - val_loss: 0.3531\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3202 - val_loss: 0.3864\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3200 - val_loss: 0.3504\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3204 - val_loss: 0.3498\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3185 - val_loss: 0.3453\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3213 - val_loss: 0.3409\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3173 - val_loss: 0.3506\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3173 - val_loss: 0.3453\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3148 - val_loss: 0.3613\n",
      "121/121 [==============================] - 0s 721us/step - loss: 0.3615\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 1.4745 - val_loss: 0.5307\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5718 - val_loss: 0.4185\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4333 - val_loss: 0.3916\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4579 - val_loss: 0.4137\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4082 - val_loss: 0.4007\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3981 - val_loss: 0.3768\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4370 - val_loss: 0.3808\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3975 - val_loss: 0.3628\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4016 - val_loss: 0.3705\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3852 - val_loss: 0.3639\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3808 - val_loss: 0.3675\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4044 - val_loss: 0.3949\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3822 - val_loss: 0.3557\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3817 - val_loss: 0.3588\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3816 - val_loss: 0.3646\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3643 - val_loss: 0.3585\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3637 - val_loss: 0.3591\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3606 - val_loss: 0.3451\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3648 - val_loss: 0.3385\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3843 - val_loss: 0.3507\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.3648 - val_loss: 0.3485\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3547 - val_loss: 0.3371\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3528 - val_loss: 0.3431\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3663 - val_loss: 0.3436\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3497 - val_loss: 0.3344\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3445 - val_loss: 0.3544\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3439 - val_loss: 0.3311\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3418 - val_loss: 0.3302\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3428 - val_loss: 0.3452\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3389 - val_loss: 0.3479\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3374 - val_loss: 0.3231\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3425 - val_loss: 0.3286\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3377 - val_loss: 0.3298\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3378 - val_loss: 0.3327\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3352 - val_loss: 0.3236\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3350 - val_loss: 0.3457\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3390 - val_loss: 0.3274\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3318 - val_loss: 0.3183\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3307 - val_loss: 0.3562\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3281 - val_loss: 0.3360\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3441 - val_loss: 0.3284\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3290 - val_loss: 0.3329\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3282 - val_loss: 0.3343\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3289 - val_loss: 0.3252\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3296 - val_loss: 0.3284\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3262 - val_loss: 0.3196\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3254 - val_loss: 0.3188\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3225 - val_loss: 0.3111\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3234 - val_loss: 0.3155\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3237 - val_loss: 0.3127\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3205 - val_loss: 0.3131\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3268 - val_loss: 0.3570\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3206 - val_loss: 0.3154\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3209 - val_loss: 0.3141\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3184 - val_loss: 0.3234\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3195 - val_loss: 0.3328\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3208 - val_loss: 0.3392\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3209 - val_loss: 0.3249\n",
      "121/121 [==============================] - 0s 954us/step - loss: 0.3255\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.7596 - val_loss: 0.6018\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.6396 - val_loss: 0.5718\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5330 - val_loss: 0.5945\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5642 - val_loss: 0.4330\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4303 - val_loss: 0.4088\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4307 - val_loss: 0.3986\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4073 - val_loss: 0.3909\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3940 - val_loss: 0.3862\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4207 - val_loss: 0.3853\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4288 - val_loss: 0.4088\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3926 - val_loss: 0.3896\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3856 - val_loss: 0.3875\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3740 - val_loss: 0.3744\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3697 - val_loss: 0.3663\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3669 - val_loss: 0.5897\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3680 - val_loss: 0.3727\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3665 - val_loss: 0.3561\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3617 - val_loss: 0.3592\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3545 - val_loss: 0.3489\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3966 - val_loss: 0.3622\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3971 - val_loss: 0.3787\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3736 - val_loss: 0.3647\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3609 - val_loss: 0.3520\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3921 - val_loss: 0.4235\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3658 - val_loss: 0.3562\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3524 - val_loss: 0.3714\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3475 - val_loss: 0.3563\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3450 - val_loss: 0.3413\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3384 - val_loss: 0.3509\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3368 - val_loss: 0.3640\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3436 - val_loss: 0.3365\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3363 - val_loss: 0.3343\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3295 - val_loss: 0.3327\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3286 - val_loss: 0.3238\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3266 - val_loss: 0.3269\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3248 - val_loss: 0.4387\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3299 - val_loss: 0.3328\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3223 - val_loss: 0.3246\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3235 - val_loss: 0.3193\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3201 - val_loss: 0.3289\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3217 - val_loss: 0.3208\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3172 - val_loss: 0.3237\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3116 - val_loss: 0.3375\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3128 - val_loss: 0.3293\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3109 - val_loss: 0.3284\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3106 - val_loss: 0.3435\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3125 - val_loss: 0.3126\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3083 - val_loss: 0.3082\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3085 - val_loss: 0.3096\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3133 - val_loss: 0.3144\n",
      "Epoch 51/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3082 - val_loss: 0.3167\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3214 - val_loss: 0.3792\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3249 - val_loss: 0.3392\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3142 - val_loss: 0.3424\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3289 - val_loss: 0.3230\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3191 - val_loss: 0.3475\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3086 - val_loss: 0.3228\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3130 - val_loss: 0.3094\n",
      "121/121 [==============================] - 0s 895us/step - loss: 0.3473\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 3.8373 - val_loss: 2.7866\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2.2128 - val_loss: 1.9151\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 1.6541 - val_loss: 1.5037\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 1.3466 - val_loss: 1.2364\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 1.1290 - val_loss: 1.0402\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.9708 - val_loss: 0.9015\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.8593 - val_loss: 0.8038\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.7829 - val_loss: 0.7378\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.7316 - val_loss: 0.6938\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6970 - val_loss: 0.6640\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6731 - val_loss: 0.6431\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6557 - val_loss: 0.6273\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6421 - val_loss: 0.6146\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6310 - val_loss: 0.6041\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6215 - val_loss: 0.5952\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6131 - val_loss: 0.5871\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6055 - val_loss: 0.5807\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5985 - val_loss: 0.5743\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5922 - val_loss: 0.5676\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5862 - val_loss: 0.5616\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5806 - val_loss: 0.5566\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5754 - val_loss: 0.5513\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5705 - val_loss: 0.5462\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5660 - val_loss: 0.5420\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5617 - val_loss: 0.5379\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5577 - val_loss: 0.5340\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5539 - val_loss: 0.5306\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5503 - val_loss: 0.5268\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5470 - val_loss: 0.5239\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5438 - val_loss: 0.5206\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5407 - val_loss: 0.5180\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5377 - val_loss: 0.5150\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5350 - val_loss: 0.5125\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5324 - val_loss: 0.5098\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5298 - val_loss: 0.5077\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5273 - val_loss: 0.5054\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5249 - val_loss: 0.5030\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5226 - val_loss: 0.5004\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5204 - val_loss: 0.4984\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5184 - val_loss: 0.4967\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5163 - val_loss: 0.4944\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5144 - val_loss: 0.4925\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5125 - val_loss: 0.4908\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5108 - val_loss: 0.4888\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5090 - val_loss: 0.4871\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5072 - val_loss: 0.4857\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5055 - val_loss: 0.4838\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5038 - val_loss: 0.4824\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5021 - val_loss: 0.4801\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5007 - val_loss: 0.4786\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4992 - val_loss: 0.4772\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4976 - val_loss: 0.4761\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4961 - val_loss: 0.4739\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4948 - val_loss: 0.4725\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4934 - val_loss: 0.4719\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4920 - val_loss: 0.4701\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4907 - val_loss: 0.4687\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4894 - val_loss: 0.4675\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4880 - val_loss: 0.4665\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4867 - val_loss: 0.4648\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4855 - val_loss: 0.4633\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4842 - val_loss: 0.4622\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4831 - val_loss: 0.4610\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4818 - val_loss: 0.4599\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4807 - val_loss: 0.4591\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4795 - val_loss: 0.4585\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4784 - val_loss: 0.4572\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4772 - val_loss: 0.4553\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4762 - val_loss: 0.4547\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4751 - val_loss: 0.4535\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4741 - val_loss: 0.4530\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4730 - val_loss: 0.4523\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4721 - val_loss: 0.4507\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4710 - val_loss: 0.4491\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4702 - val_loss: 0.4484\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4692 - val_loss: 0.4482\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4681 - val_loss: 0.4465\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4674 - val_loss: 0.4463\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4665 - val_loss: 0.4453\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4656 - val_loss: 0.4446\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4647 - val_loss: 0.4435\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4637 - val_loss: 0.4434\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4631 - val_loss: 0.4422\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4622 - val_loss: 0.4412\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4614 - val_loss: 0.4406\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4606 - val_loss: 0.4400\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4598 - val_loss: 0.4395\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4591 - val_loss: 0.4390\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4583 - val_loss: 0.4383\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4575 - val_loss: 0.4371\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4569 - val_loss: 0.4368\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4562 - val_loss: 0.4362\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4555 - val_loss: 0.4356\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4549 - val_loss: 0.4353\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4542 - val_loss: 0.4345\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4535 - val_loss: 0.4339\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4528 - val_loss: 0.4338\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4521 - val_loss: 0.4339\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4517 - val_loss: 0.4327\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4511 - val_loss: 0.4319\n",
      "121/121 [==============================] - 0s 790us/step - loss: 0.4572\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 2.9758 - val_loss: 1.6500\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 1.4268 - val_loss: 1.0966\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 1.0952 - val_loss: 0.9484\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.9648 - val_loss: 0.8774\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.9031 - val_loss: 0.8326\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.8619 - val_loss: 0.8013\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.8316 - val_loss: 0.7776\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.8073 - val_loss: 0.7578\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.7873 - val_loss: 0.7415\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.7700 - val_loss: 0.7273\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.7547 - val_loss: 0.7149\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.7414 - val_loss: 0.7034\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.7290 - val_loss: 0.6928\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.7178 - val_loss: 0.6830\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.7075 - val_loss: 0.6742\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6977 - val_loss: 0.6657\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6888 - val_loss: 0.6579\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6801 - val_loss: 0.6502\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6719 - val_loss: 0.6429\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6641 - val_loss: 0.6360\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6565 - val_loss: 0.6293\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6494 - val_loss: 0.6228\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6425 - val_loss: 0.6163\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6360 - val_loss: 0.6104\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6297 - val_loss: 0.6047\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6236 - val_loss: 0.5993\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6178 - val_loss: 0.5940\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6123 - val_loss: 0.5887\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6071 - val_loss: 0.5838\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6020 - val_loss: 0.5791\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5972 - val_loss: 0.5745\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5924 - val_loss: 0.5699\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5881 - val_loss: 0.5658\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5837 - val_loss: 0.5617\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5795 - val_loss: 0.5578\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5754 - val_loss: 0.5542\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5716 - val_loss: 0.5503\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5679 - val_loss: 0.5467\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5643 - val_loss: 0.5433\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5611 - val_loss: 0.5404\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5578 - val_loss: 0.5371\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5547 - val_loss: 0.5344\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5518 - val_loss: 0.5315\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5489 - val_loss: 0.5286\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5462 - val_loss: 0.5259\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5434 - val_loss: 0.5234\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5408 - val_loss: 0.5209\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5383 - val_loss: 0.5186\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5359 - val_loss: 0.5160\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5335 - val_loss: 0.5137\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5312 - val_loss: 0.5114\n",
      "Epoch 52/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5289 - val_loss: 0.5091\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5267 - val_loss: 0.5067\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5245 - val_loss: 0.5043\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5224 - val_loss: 0.5025\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5204 - val_loss: 0.5003\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5185 - val_loss: 0.4980\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5164 - val_loss: 0.4962\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5145 - val_loss: 0.4943\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5126 - val_loss: 0.4919\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5108 - val_loss: 0.4900\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5089 - val_loss: 0.4880\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5071 - val_loss: 0.4862\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5054 - val_loss: 0.4843\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5036 - val_loss: 0.4827\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5019 - val_loss: 0.4812\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5002 - val_loss: 0.4796\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4986 - val_loss: 0.4778\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4970 - val_loss: 0.4763\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4954 - val_loss: 0.4745\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4939 - val_loss: 0.4734\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4925 - val_loss: 0.4719\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4910 - val_loss: 0.4702\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4897 - val_loss: 0.4689\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4883 - val_loss: 0.4676\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4870 - val_loss: 0.4665\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4857 - val_loss: 0.4651\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4845 - val_loss: 0.4642\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4833 - val_loss: 0.4631\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4820 - val_loss: 0.4620\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4809 - val_loss: 0.4606\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4798 - val_loss: 0.4596\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4787 - val_loss: 0.4585\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4776 - val_loss: 0.4576\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4765 - val_loss: 0.4567\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4754 - val_loss: 0.4556\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4744 - val_loss: 0.4547\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4734 - val_loss: 0.4537\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4723 - val_loss: 0.4532\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4714 - val_loss: 0.4518\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4705 - val_loss: 0.4512\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4695 - val_loss: 0.4505\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4686 - val_loss: 0.4494\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4677 - val_loss: 0.4486\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4669 - val_loss: 0.4479\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4660 - val_loss: 0.4471\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4650 - val_loss: 0.4468\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4644 - val_loss: 0.4460\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4635 - val_loss: 0.4449\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4628 - val_loss: 0.4442\n",
      "121/121 [==============================] - 0s 813us/step - loss: 0.4393\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 3.8355 - val_loss: 2.8960\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 2.3236 - val_loss: 1.9103\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 1.6989 - val_loss: 1.5116\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.4184 - val_loss: 1.3135\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 1.2554 - val_loss: 1.1844\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.1384 - val_loss: 1.0845\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 1.0458 - val_loss: 1.0029\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.9687 - val_loss: 0.9341\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.9039 - val_loss: 0.8761\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.8499 - val_loss: 0.8271\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8045 - val_loss: 0.7862\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.7670 - val_loss: 0.7515\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.7357 - val_loss: 0.7234\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7099 - val_loss: 0.6998\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6883 - val_loss: 0.6804\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6706 - val_loss: 0.6634\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6555 - val_loss: 0.6494\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6424 - val_loss: 0.6373\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6313 - val_loss: 0.6266\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6215 - val_loss: 0.6172\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6126 - val_loss: 0.6090\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6050 - val_loss: 0.6010\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5977 - val_loss: 0.5935\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5910 - val_loss: 0.5866\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5846 - val_loss: 0.5799\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5789 - val_loss: 0.5738\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5731 - val_loss: 0.5679\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5677 - val_loss: 0.5621\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5626 - val_loss: 0.5567\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5576 - val_loss: 0.5516\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5529 - val_loss: 0.5466\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5485 - val_loss: 0.5418\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5440 - val_loss: 0.5372\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5398 - val_loss: 0.5328\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5358 - val_loss: 0.5287\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5319 - val_loss: 0.5247\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5283 - val_loss: 0.5207\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5247 - val_loss: 0.5168\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5213 - val_loss: 0.5133\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5180 - val_loss: 0.5103\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5148 - val_loss: 0.5067\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5119 - val_loss: 0.5036\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5090 - val_loss: 0.5005\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5062 - val_loss: 0.4977\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5034 - val_loss: 0.4949\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5008 - val_loss: 0.4922\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4984 - val_loss: 0.4896\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4959 - val_loss: 0.4874\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4936 - val_loss: 0.4849\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4914 - val_loss: 0.4827\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4892 - val_loss: 0.4808\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4870 - val_loss: 0.4785\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4851 - val_loss: 0.4767\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4832 - val_loss: 0.4748\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4812 - val_loss: 0.4731\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4794 - val_loss: 0.4716\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4777 - val_loss: 0.4697\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4760 - val_loss: 0.4682\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4743 - val_loss: 0.4668\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4727 - val_loss: 0.4653\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4712 - val_loss: 0.4641\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4697 - val_loss: 0.4626\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4682 - val_loss: 0.4614\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4670 - val_loss: 0.4601\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4655 - val_loss: 0.4589\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4644 - val_loss: 0.4578\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4630 - val_loss: 0.4568\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4618 - val_loss: 0.4560\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4607 - val_loss: 0.4547\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4596 - val_loss: 0.4538\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4583 - val_loss: 0.4530\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4574 - val_loss: 0.4519\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4563 - val_loss: 0.4509\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4551 - val_loss: 0.4500\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4543 - val_loss: 0.4493\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4533 - val_loss: 0.4485\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4523 - val_loss: 0.4477\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4513 - val_loss: 0.4471\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4505 - val_loss: 0.4464\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4496 - val_loss: 0.4456\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4487 - val_loss: 0.4448\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4479 - val_loss: 0.4442\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4470 - val_loss: 0.4434\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4462 - val_loss: 0.4429\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4453 - val_loss: 0.4423\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4445 - val_loss: 0.4416\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4437 - val_loss: 0.4408\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4429 - val_loss: 0.4403\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4421 - val_loss: 0.4399\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4414 - val_loss: 0.4393\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4407 - val_loss: 0.4387\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4399 - val_loss: 0.4382\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4392 - val_loss: 0.4375\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4385 - val_loss: 0.4369\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4377 - val_loss: 0.4365\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4371 - val_loss: 0.4356\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4363 - val_loss: 0.4355\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4358 - val_loss: 0.4348\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4351 - val_loss: 0.4342\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4344 - val_loss: 0.4337\n",
      "121/121 [==============================] - 0s 767us/step - loss: 0.4726\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 5.1946 - val_loss: 3.4664\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 2.5767 - val_loss: 1.8950\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.4877 - val_loss: 1.1967\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 1.0101 - val_loss: 0.8736\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.7929 - val_loss: 0.7207\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6913 - val_loss: 0.6455\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6418 - val_loss: 0.6073\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6163 - val_loss: 0.5860\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6019 - val_loss: 0.5733\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5930 - val_loss: 0.5649\n",
      "Epoch 11/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5868 - val_loss: 0.5588\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5818 - val_loss: 0.5539\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5777 - val_loss: 0.5497\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5740 - val_loss: 0.5462\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5706 - val_loss: 0.5428\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5675 - val_loss: 0.5398\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5645 - val_loss: 0.5371\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5618 - val_loss: 0.5346\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5592 - val_loss: 0.5322\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5567 - val_loss: 0.5299\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5545 - val_loss: 0.5278\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5523 - val_loss: 0.5258\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5503 - val_loss: 0.5238\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5483 - val_loss: 0.5220\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5465 - val_loss: 0.5204\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5448 - val_loss: 0.5189\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5431 - val_loss: 0.5175\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5416 - val_loss: 0.5161\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5402 - val_loss: 0.5148\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5389 - val_loss: 0.5136\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5376 - val_loss: 0.5125\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5363 - val_loss: 0.5114\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5352 - val_loss: 0.5105\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5342 - val_loss: 0.5095\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5331 - val_loss: 0.5087\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5322 - val_loss: 0.5079\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5313 - val_loss: 0.5072\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5304 - val_loss: 0.5064\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5296 - val_loss: 0.5057\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5289 - val_loss: 0.5050\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5281 - val_loss: 0.5044\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5275 - val_loss: 0.5038\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5268 - val_loss: 0.5033\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5262 - val_loss: 0.5028\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5257 - val_loss: 0.5023\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5251 - val_loss: 0.5019\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5246 - val_loss: 0.5015\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5241 - val_loss: 0.5013\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5237 - val_loss: 0.5008\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5233 - val_loss: 0.5005\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5229 - val_loss: 0.5002\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5225 - val_loss: 0.4999\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5222 - val_loss: 0.4995\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5218 - val_loss: 0.4992\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5215 - val_loss: 0.4991\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5212 - val_loss: 0.4989\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5209 - val_loss: 0.4987\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5207 - val_loss: 0.4984\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5204 - val_loss: 0.4983\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5201 - val_loss: 0.4981\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5200 - val_loss: 0.4980\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5197 - val_loss: 0.4979\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5195 - val_loss: 0.4977\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5194 - val_loss: 0.4976\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5192 - val_loss: 0.4976\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5190 - val_loss: 0.4976\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5188 - val_loss: 0.4974\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5187 - val_loss: 0.4973\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5186 - val_loss: 0.4972\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5184 - val_loss: 0.4969\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5183 - val_loss: 0.4970\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5182 - val_loss: 0.4970\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5181 - val_loss: 0.4968\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5180 - val_loss: 0.4967\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5179 - val_loss: 0.4966\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5178 - val_loss: 0.4966\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5177 - val_loss: 0.4965\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5176 - val_loss: 0.4965\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5176 - val_loss: 0.4965\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5175 - val_loss: 0.4965\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5174 - val_loss: 0.4964\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5173 - val_loss: 0.4964\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5173 - val_loss: 0.4964\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5172 - val_loss: 0.4963\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5171 - val_loss: 0.4963\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5171 - val_loss: 0.4963\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5171 - val_loss: 0.4962\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5170 - val_loss: 0.4963\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5170 - val_loss: 0.4963\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5169 - val_loss: 0.4963\n",
      "Epoch 91/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5169 - val_loss: 0.4964\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5169 - val_loss: 0.4963\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5168 - val_loss: 0.4963\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5168 - val_loss: 0.4962\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5167 - val_loss: 0.4963\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5167 - val_loss: 0.4963\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5167 - val_loss: 0.4964\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5167 - val_loss: 0.4964\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5166 - val_loss: 0.4964\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5166 - val_loss: 0.4963\n",
      "121/121 [==============================] - 0s 825us/step - loss: 0.5424\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 5.0184 - val_loss: 3.1536\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 2.3814 - val_loss: 1.7030\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 1.3516 - val_loss: 1.0690\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.9126 - val_loss: 0.7802\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.7188 - val_loss: 0.6473\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6304 - val_loss: 0.5837\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5893 - val_loss: 0.5533\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5697 - val_loss: 0.5380\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5601 - val_loss: 0.5299\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5549 - val_loss: 0.5251\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5520 - val_loss: 0.5221\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5501 - val_loss: 0.5205\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5486 - val_loss: 0.5187\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5475 - val_loss: 0.5174\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5465 - val_loss: 0.5167\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5455 - val_loss: 0.5161\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5446 - val_loss: 0.5146\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5439 - val_loss: 0.5141\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5431 - val_loss: 0.5131\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5425 - val_loss: 0.5127\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5418 - val_loss: 0.5125\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5411 - val_loss: 0.5113\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5406 - val_loss: 0.5110\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5400 - val_loss: 0.5104\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5394 - val_loss: 0.5096\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5391 - val_loss: 0.5097\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5385 - val_loss: 0.5098\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5380 - val_loss: 0.5085\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5377 - val_loss: 0.5080\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5373 - val_loss: 0.5083\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5369 - val_loss: 0.5074\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5364 - val_loss: 0.5064\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5363 - val_loss: 0.5069\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5358 - val_loss: 0.5061\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5355 - val_loss: 0.5055\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5353 - val_loss: 0.5062\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5350 - val_loss: 0.5054\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5348 - val_loss: 0.5056\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5344 - val_loss: 0.5045\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5343 - val_loss: 0.5050\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5340 - val_loss: 0.5040\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5340 - val_loss: 0.5044\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5337 - val_loss: 0.5041\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5335 - val_loss: 0.5043\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5331 - val_loss: 0.5032\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5331 - val_loss: 0.5029\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5329 - val_loss: 0.5025\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5329 - val_loss: 0.5035\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5327 - val_loss: 0.5029\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5326 - val_loss: 0.5026\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5324 - val_loss: 0.5033\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5321 - val_loss: 0.5023\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5322 - val_loss: 0.5019\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5322 - val_loss: 0.5022\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5318 - val_loss: 0.5017\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5320 - val_loss: 0.5021\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5318 - val_loss: 0.5025\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5317 - val_loss: 0.5021\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5316 - val_loss: 0.5022\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5315 - val_loss: 0.5021\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5314 - val_loss: 0.5017\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5313 - val_loss: 0.5017\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5312 - val_loss: 0.5011\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5312 - val_loss: 0.5009\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5311 - val_loss: 0.5006\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5311 - val_loss: 0.5009\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5310 - val_loss: 0.5015\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5307 - val_loss: 0.5006\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5307 - val_loss: 0.5005\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5310 - val_loss: 0.5010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5306 - val_loss: 0.5003\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5308 - val_loss: 0.5011\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5307 - val_loss: 0.5014\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5304 - val_loss: 0.5002\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5307 - val_loss: 0.5006\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5305 - val_loss: 0.5007\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5305 - val_loss: 0.5013\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5303 - val_loss: 0.5002\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5305 - val_loss: 0.5004\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5303 - val_loss: 0.5001\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5305 - val_loss: 0.5002\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5304 - val_loss: 0.5005\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5301 - val_loss: 0.4997\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5303 - val_loss: 0.4996\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5303 - val_loss: 0.5004\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5302 - val_loss: 0.5009\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5302 - val_loss: 0.5004\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5300 - val_loss: 0.4995\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5302 - val_loss: 0.5005\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5302 - val_loss: 0.5000\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5301 - val_loss: 0.5006\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5301 - val_loss: 0.5009\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5301 - val_loss: 0.5007\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5297 - val_loss: 0.4994\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5301 - val_loss: 0.4994\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5301 - val_loss: 0.4995\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5300 - val_loss: 0.4996\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5300 - val_loss: 0.4995\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5299 - val_loss: 0.4993\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5299 - val_loss: 0.4992\n",
      "121/121 [==============================] - 0s 765us/step - loss: 0.5095\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 5.9130 - val_loss: 3.4853\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 2.6885 - val_loss: 1.9276\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 1.5887 - val_loss: 1.2786\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 1.1309 - val_loss: 0.9759\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.9149 - val_loss: 0.8257\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8052 - val_loss: 0.7457\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.7445 - val_loss: 0.6993\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.7075 - val_loss: 0.6698\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6827 - val_loss: 0.6496\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6647 - val_loss: 0.6344\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6505 - val_loss: 0.6225\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6388 - val_loss: 0.6126\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6289 - val_loss: 0.6042\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6202 - val_loss: 0.5970\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6126 - val_loss: 0.5908\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6059 - val_loss: 0.5852\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5998 - val_loss: 0.5800\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5943 - val_loss: 0.5754\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5893 - val_loss: 0.5713\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5850 - val_loss: 0.5676\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5807 - val_loss: 0.5641\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5768 - val_loss: 0.5607\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5734 - val_loss: 0.5576\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5702 - val_loss: 0.5550\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5671 - val_loss: 0.5522\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5643 - val_loss: 0.5500\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5617 - val_loss: 0.5478\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5591 - val_loss: 0.5449\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5569 - val_loss: 0.5429\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5548 - val_loss: 0.5412\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5528 - val_loss: 0.5392\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5509 - val_loss: 0.5372\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5491 - val_loss: 0.5355\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5473 - val_loss: 0.5336\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5458 - val_loss: 0.5320\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5443 - val_loss: 0.5311\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5427 - val_loss: 0.5292\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5416 - val_loss: 0.5280\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5400 - val_loss: 0.5263\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5393 - val_loss: 0.5257\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5379 - val_loss: 0.5240\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5370 - val_loss: 0.5231\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5359 - val_loss: 0.5220\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5350 - val_loss: 0.5214\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5339 - val_loss: 0.5200\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5332 - val_loss: 0.5191\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5323 - val_loss: 0.5180\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5315 - val_loss: 0.5180\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5309 - val_loss: 0.5172\n",
      "Epoch 50/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5302 - val_loss: 0.5161\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5295 - val_loss: 0.5159\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5286 - val_loss: 0.5144\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5283 - val_loss: 0.5135\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5278 - val_loss: 0.5133\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5270 - val_loss: 0.5123\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5268 - val_loss: 0.5123\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5262 - val_loss: 0.5119\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5258 - val_loss: 0.5112\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5253 - val_loss: 0.5112\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5249 - val_loss: 0.5104\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5245 - val_loss: 0.5097\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5241 - val_loss: 0.5090\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5237 - val_loss: 0.5085\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5233 - val_loss: 0.5078\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5230 - val_loss: 0.5074\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5228 - val_loss: 0.5076\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5223 - val_loss: 0.5069\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5219 - val_loss: 0.5063\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5220 - val_loss: 0.5063\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5217 - val_loss: 0.5063\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5210 - val_loss: 0.5053\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5213 - val_loss: 0.5057\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5209 - val_loss: 0.5055\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5201 - val_loss: 0.5045\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5206 - val_loss: 0.5048\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5202 - val_loss: 0.5049\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5200 - val_loss: 0.5051\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5194 - val_loss: 0.5037\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5197 - val_loss: 0.5036\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5194 - val_loss: 0.5030\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5194 - val_loss: 0.5030\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5194 - val_loss: 0.5032\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5190 - val_loss: 0.5028\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5188 - val_loss: 0.5023\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5189 - val_loss: 0.5029\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5187 - val_loss: 0.5032\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5185 - val_loss: 0.5024\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5180 - val_loss: 0.5015\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5182 - val_loss: 0.5025\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5182 - val_loss: 0.5022\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5181 - val_loss: 0.5027\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5180 - val_loss: 0.5026\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5180 - val_loss: 0.5024\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5173 - val_loss: 0.5009\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5176 - val_loss: 0.5006\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5178 - val_loss: 0.5006\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5177 - val_loss: 0.5010\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5175 - val_loss: 0.5011\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5172 - val_loss: 0.5005\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5173 - val_loss: 0.5003\n",
      "121/121 [==============================] - 0s 790us/step - loss: 0.5404\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 5.7039 - val_loss: 3.2838\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 2.2369 - val_loss: 1.5501\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 1.1939 - val_loss: 0.9592\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8363 - val_loss: 0.7386\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.7019 - val_loss: 0.6497\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6468 - val_loss: 0.6095\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6209 - val_loss: 0.5890\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6064 - val_loss: 0.5762\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5969 - val_loss: 0.5671\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5895 - val_loss: 0.5600\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5834 - val_loss: 0.5543\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5780 - val_loss: 0.5492\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5731 - val_loss: 0.5446\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5688 - val_loss: 0.5406\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5647 - val_loss: 0.5368\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5610 - val_loss: 0.5334\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5575 - val_loss: 0.5304\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5544 - val_loss: 0.5277\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5515 - val_loss: 0.5250\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5488 - val_loss: 0.5225\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5463 - val_loss: 0.5204\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5440 - val_loss: 0.5184\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5419 - val_loss: 0.5163\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5399 - val_loss: 0.5145\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5382 - val_loss: 0.5130\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5365 - val_loss: 0.5116\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5349 - val_loss: 0.5103\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5335 - val_loss: 0.5091\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5322 - val_loss: 0.5081\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5310 - val_loss: 0.5070\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5299 - val_loss: 0.5060\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5288 - val_loss: 0.5052\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5279 - val_loss: 0.5045\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5271 - val_loss: 0.5037\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5262 - val_loss: 0.5032\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5255 - val_loss: 0.5025\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5248 - val_loss: 0.5021\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5241 - val_loss: 0.5014\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5236 - val_loss: 0.5009\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5230 - val_loss: 0.5004\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5225 - val_loss: 0.5001\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5221 - val_loss: 0.4996\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5217 - val_loss: 0.4993\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5213 - val_loss: 0.4990\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5208 - val_loss: 0.4988\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5205 - val_loss: 0.4986\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5203 - val_loss: 0.4984\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5199 - val_loss: 0.4985\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5197 - val_loss: 0.4980\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5194 - val_loss: 0.4979\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5192 - val_loss: 0.4977\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5190 - val_loss: 0.4975\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5188 - val_loss: 0.4972\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5186 - val_loss: 0.4971\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5185 - val_loss: 0.4972\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5183 - val_loss: 0.4970\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5181 - val_loss: 0.4971\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5181 - val_loss: 0.4968\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5179 - val_loss: 0.4969\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5177 - val_loss: 0.4967\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5177 - val_loss: 0.4967\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5176 - val_loss: 0.4967\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5174 - val_loss: 0.4967\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5174 - val_loss: 0.4967\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5173 - val_loss: 0.4967\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5172 - val_loss: 0.4969\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5172 - val_loss: 0.4967\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5171 - val_loss: 0.4966\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5171 - val_loss: 0.4966\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5170 - val_loss: 0.4963\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5169 - val_loss: 0.4964\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5169 - val_loss: 0.4966\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5169 - val_loss: 0.4965\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5168 - val_loss: 0.4963\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5168 - val_loss: 0.4963\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5167 - val_loss: 0.4963\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5168 - val_loss: 0.4963\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5167 - val_loss: 0.4963\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5167 - val_loss: 0.4963\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5167 - val_loss: 0.4964\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5167 - val_loss: 0.4963\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5166 - val_loss: 0.4963\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5166 - val_loss: 0.4965\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5166 - val_loss: 0.4962\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5165 - val_loss: 0.4962\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5166 - val_loss: 0.4962\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5166 - val_loss: 0.4962\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5165 - val_loss: 0.4964\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5165 - val_loss: 0.4965\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5165 - val_loss: 0.4965\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5165 - val_loss: 0.4965\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5165 - val_loss: 0.4965\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5165 - val_loss: 0.4964\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5164 - val_loss: 0.4963\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5164 - val_loss: 0.4965\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5164 - val_loss: 0.4966\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5164 - val_loss: 0.4967\n",
      "121/121 [==============================] - 0s 778us/step - loss: 0.5457\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 4.3348 - val_loss: 2.5217\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.7798 - val_loss: 1.2627\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 1.0400 - val_loss: 0.8520\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7927 - val_loss: 0.7043\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.7018 - val_loss: 0.6461\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6639 - val_loss: 0.6188\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6446 - val_loss: 0.6035\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6321 - val_loss: 0.5925\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6227 - val_loss: 0.5839\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6146 - val_loss: 0.5764\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6074 - val_loss: 0.5700\n",
      "Epoch 12/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6009 - val_loss: 0.5642\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5949 - val_loss: 0.5588\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5896 - val_loss: 0.5539\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5847 - val_loss: 0.5495\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5801 - val_loss: 0.5456\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5759 - val_loss: 0.5416\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5722 - val_loss: 0.5383\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5687 - val_loss: 0.5351\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5655 - val_loss: 0.5324\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5626 - val_loss: 0.5300\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5599 - val_loss: 0.5272\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5576 - val_loss: 0.5250\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5552 - val_loss: 0.5231\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5532 - val_loss: 0.5210\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5514 - val_loss: 0.5199\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5495 - val_loss: 0.5189\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5480 - val_loss: 0.5165\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5466 - val_loss: 0.5152\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5453 - val_loss: 0.5147\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5440 - val_loss: 0.5129\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5428 - val_loss: 0.5113\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5420 - val_loss: 0.5115\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5409 - val_loss: 0.5098\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5400 - val_loss: 0.5087\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5393 - val_loss: 0.5093\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5386 - val_loss: 0.5079\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5380 - val_loss: 0.5079\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5371 - val_loss: 0.5062\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5368 - val_loss: 0.5067\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5361 - val_loss: 0.5051\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5359 - val_loss: 0.5056\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5354 - val_loss: 0.5050\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5350 - val_loss: 0.5051\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5342 - val_loss: 0.5034\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5342 - val_loss: 0.5031\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5338 - val_loss: 0.5025\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5336 - val_loss: 0.5040\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5333 - val_loss: 0.5028\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5332 - val_loss: 0.5024\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5329 - val_loss: 0.5036\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5323 - val_loss: 0.5018\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5325 - val_loss: 0.5014\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5325 - val_loss: 0.5019\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5319 - val_loss: 0.5011\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5322 - val_loss: 0.5019\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5319 - val_loss: 0.5025\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5317 - val_loss: 0.5016\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5316 - val_loss: 0.5020\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5315 - val_loss: 0.5018\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5314 - val_loss: 0.5011\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5313 - val_loss: 0.5012\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5310 - val_loss: 0.5002\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5312 - val_loss: 0.5000\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5310 - val_loss: 0.4997\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5311 - val_loss: 0.5003\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5310 - val_loss: 0.5013\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5304 - val_loss: 0.4996\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5306 - val_loss: 0.4997\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5310 - val_loss: 0.5007\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5303 - val_loss: 0.4994\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5308 - val_loss: 0.5009\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5306 - val_loss: 0.5014\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5301 - val_loss: 0.4992\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5308 - val_loss: 0.5001\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5305 - val_loss: 0.5004\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5304 - val_loss: 0.5014\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5301 - val_loss: 0.4994\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5305 - val_loss: 0.5000\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5302 - val_loss: 0.4993\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5305 - val_loss: 0.4996\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5305 - val_loss: 0.5003\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5300 - val_loss: 0.4988\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5304 - val_loss: 0.4988\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5303 - val_loss: 0.5004\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5302 - val_loss: 0.5011\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5303 - val_loss: 0.5001\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5299 - val_loss: 0.4986\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5302 - val_loss: 0.5007\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5302 - val_loss: 0.4996\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5302 - val_loss: 0.5009\n",
      "Epoch 92/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5302 - val_loss: 0.5013\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5302 - val_loss: 0.5008\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5295 - val_loss: 0.4985\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5303 - val_loss: 0.4989\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5303 - val_loss: 0.4991\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5301 - val_loss: 0.4993\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5301 - val_loss: 0.4991\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5301 - val_loss: 0.4988\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5301 - val_loss: 0.4986\n",
      "121/121 [==============================] - 0s 890us/step - loss: 0.5089\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 5.3630 - val_loss: 3.3339\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 2.2342 - val_loss: 1.5942\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 1.1946 - val_loss: 0.9938\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8381 - val_loss: 0.7785\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.7111 - val_loss: 0.6967\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6621 - val_loss: 0.6595\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6398 - val_loss: 0.6389\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6273 - val_loss: 0.6253\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6183 - val_loss: 0.6164\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6113 - val_loss: 0.6071\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6049 - val_loss: 0.5993\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5995 - val_loss: 0.5926\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5944 - val_loss: 0.5864\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5899 - val_loss: 0.5812\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5855 - val_loss: 0.5776\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5815 - val_loss: 0.5728\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5772 - val_loss: 0.5671\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5743 - val_loss: 0.5635\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5706 - val_loss: 0.5596\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5683 - val_loss: 0.5571\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5650 - val_loss: 0.5541\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5618 - val_loss: 0.5500\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5597 - val_loss: 0.5478\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5574 - val_loss: 0.5457\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5552 - val_loss: 0.5428\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5528 - val_loss: 0.5419\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5509 - val_loss: 0.5405\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5486 - val_loss: 0.5355\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5475 - val_loss: 0.5337\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5458 - val_loss: 0.5338\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5442 - val_loss: 0.5310\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5427 - val_loss: 0.5287\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5415 - val_loss: 0.5276\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5395 - val_loss: 0.5250\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5389 - val_loss: 0.5236\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5375 - val_loss: 0.5248\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5358 - val_loss: 0.5212\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5357 - val_loss: 0.5210\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5334 - val_loss: 0.5185\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5341 - val_loss: 0.5192\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5321 - val_loss: 0.5166\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5321 - val_loss: 0.5165\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5310 - val_loss: 0.5153\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5305 - val_loss: 0.5158\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5290 - val_loss: 0.5135\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5290 - val_loss: 0.5128\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5282 - val_loss: 0.5115\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5272 - val_loss: 0.5137\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5273 - val_loss: 0.5124\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5266 - val_loss: 0.5108\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5260 - val_loss: 0.5119\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5248 - val_loss: 0.5089\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5253 - val_loss: 0.5081\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5249 - val_loss: 0.5088\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5238 - val_loss: 0.5073\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5243 - val_loss: 0.5081\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5234 - val_loss: 0.5081\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5232 - val_loss: 0.5070\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5228 - val_loss: 0.5082\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5227 - val_loss: 0.5069\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5222 - val_loss: 0.5058\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5219 - val_loss: 0.5049\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5216 - val_loss: 0.5046\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5212 - val_loss: 0.5037\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5211 - val_loss: 0.5034\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5210 - val_loss: 0.5044\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5205 - val_loss: 0.5033\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5199 - val_loss: 0.5027\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5206 - val_loss: 0.5031\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5203 - val_loss: 0.5036\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5190 - val_loss: 0.5018\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5200 - val_loss: 0.5032\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5196 - val_loss: 0.5030\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5183 - val_loss: 0.5012\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5196 - val_loss: 0.5024\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5189 - val_loss: 0.5026\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5186 - val_loss: 0.5037\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5177 - val_loss: 0.5008\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5187 - val_loss: 0.5009\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5182 - val_loss: 0.5002\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5185 - val_loss: 0.5005\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5186 - val_loss: 0.5011\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5180 - val_loss: 0.5003\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5178 - val_loss: 0.4997\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5182 - val_loss: 0.5013\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5178 - val_loss: 0.5021\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5177 - val_loss: 0.5004\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5168 - val_loss: 0.4991\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5175 - val_loss: 0.5014\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5175 - val_loss: 0.5007\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5174 - val_loss: 0.5019\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5176 - val_loss: 0.5017\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5175 - val_loss: 0.5014\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5162 - val_loss: 0.4988\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5172 - val_loss: 0.4986\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5176 - val_loss: 0.4988\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5174 - val_loss: 0.4997\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5172 - val_loss: 0.4998\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5167 - val_loss: 0.4989\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5169 - val_loss: 0.4987\n",
      "121/121 [==============================] - 0s 927us/step - loss: 0.5384\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 2.9997 - val_loss: 5.8507\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 10.1781 - val_loss: 118.5493\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 1004.7584 - val_loss: 2574.1484\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 4457.9375 - val_loss: 57660.9648\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 511208.6875 - val_loss: 1257407.0000\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 1985828.1250 - val_loss: 28109210.0000\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 209679648.0000 - val_loss: 598670848.0000\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 5077194240.0000 - val_loss: 12961251328.0000\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 25901600768.0000 - val_loss: 292338597888.0000\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 480605995008.0000 - val_loss: 6449085808640.0000\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 10161686052864.0000 - val_loss: 141844969160704.0000\n",
      "121/121 [==============================] - 0s 749us/step - loss: 200928284114944.0000\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 4.9351 - val_loss: 3.0222\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 5.2797 - val_loss: 39.8374\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 970.9894 - val_loss: 623.7460\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 934.3049 - val_loss: 9764.8398\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 243069.5781 - val_loss: 157783.8438\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 284180.5625 - val_loss: 2497695.7500\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 92263376.0000 - val_loss: 39056612.0000\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 1476496256.0000 - val_loss: 610341952.0000\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 11297763328.0000 - val_loss: 10015792128.0000\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 15785149440.0000 - val_loss: 162787459072.0000\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2892956696576.0000 - val_loss: 2678976413696.0000\n",
      "121/121 [==============================] - 0s 784us/step - loss: 95242608640.0000\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 1.2250 - val_loss: 0.6229\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6102 - val_loss: 0.5358\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5451 - val_loss: 0.5168\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5698 - val_loss: 0.5028\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5459 - val_loss: 0.5485\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5572 - val_loss: 0.4989\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5566 - val_loss: 0.4987\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5966 - val_loss: 0.4991\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5560 - val_loss: 0.5232\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5458 - val_loss: 0.5008\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5213 - val_loss: 0.4985\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5608 - val_loss: 0.4967\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5417 - val_loss: 0.4967\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5505 - val_loss: 0.4978\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5475 - val_loss: 1.1161\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5718 - val_loss: 0.5140\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5634 - val_loss: 0.5210\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5435 - val_loss: 0.5277\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5728 - val_loss: 0.5290\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6321 - val_loss: 0.5283\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5939 - val_loss: 0.5169\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5433 - val_loss: 0.5133\n",
      "121/121 [==============================] - 0s 825us/step - loss: 0.6161\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 1.0817 - val_loss: 0.5500\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5615 - val_loss: 0.4744\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5179 - val_loss: 0.4345\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4536 - val_loss: 0.4053\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4183 - val_loss: 0.3948\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4041 - val_loss: 0.3757\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3919 - val_loss: 0.3721\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3841 - val_loss: 0.3663\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3761 - val_loss: 0.3541\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3692 - val_loss: 0.3613\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3648 - val_loss: 0.3494\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3602 - val_loss: 0.3418\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3534 - val_loss: 0.3425\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3513 - val_loss: 0.3440\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3465 - val_loss: 0.3362\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3415 - val_loss: 0.3359\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3373 - val_loss: 0.3349\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3330 - val_loss: 0.3332\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3321 - val_loss: 0.3388\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3285 - val_loss: 0.3296\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3267 - val_loss: 0.3291\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3229 - val_loss: 0.3266\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3230 - val_loss: 0.3231\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3182 - val_loss: 0.3134\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3158 - val_loss: 0.3342\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3164 - val_loss: 0.3274\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3142 - val_loss: 0.3262\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3113 - val_loss: 0.3227\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3109 - val_loss: 0.3238\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3085 - val_loss: 0.3157\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3060 - val_loss: 0.3137\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3032 - val_loss: 0.3122\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3027 - val_loss: 0.3130\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3023 - val_loss: 0.3127\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3016 - val_loss: 0.3059\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2990 - val_loss: 0.3195\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2975 - val_loss: 0.3072\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2953 - val_loss: 0.3002\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2929 - val_loss: 0.3493\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2914 - val_loss: 0.3044\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2921 - val_loss: 0.3081\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2917 - val_loss: 0.3101\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2908 - val_loss: 0.3086\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2902 - val_loss: 0.3117\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2903 - val_loss: 0.3070\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2861 - val_loss: 0.2976\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2860 - val_loss: 0.2995\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2848 - val_loss: 0.3157\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2832 - val_loss: 0.3015\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2803 - val_loss: 0.2901\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2841 - val_loss: 0.2983\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2821 - val_loss: 0.2930\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2811 - val_loss: 0.2924\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2794 - val_loss: 0.2960\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2772 - val_loss: 0.3010\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2765 - val_loss: 0.2940\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2767 - val_loss: 0.2944\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2777 - val_loss: 0.3017\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2745 - val_loss: 0.2923\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2744 - val_loss: 0.2987\n",
      "121/121 [==============================] - 0s 678us/step - loss: 0.3090\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 1.0338 - val_loss: 0.5749\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5480 - val_loss: 0.4785\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4821 - val_loss: 0.4333\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4569 - val_loss: 0.4115\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4352 - val_loss: 0.3982\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4167 - val_loss: 0.3834\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4069 - val_loss: 0.3861\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3999 - val_loss: 0.3724\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3888 - val_loss: 0.3652\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3841 - val_loss: 0.3658\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3828 - val_loss: 0.3626\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3743 - val_loss: 0.3733\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3710 - val_loss: 0.3559\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3663 - val_loss: 0.3477\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3705 - val_loss: 0.3469\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3564 - val_loss: 0.3611\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3530 - val_loss: 0.3501\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3488 - val_loss: 0.3420\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3563 - val_loss: 0.3356\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3445 - val_loss: 0.3354\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3412 - val_loss: 0.3344\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3374 - val_loss: 0.3306\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3477 - val_loss: 0.3380\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3327 - val_loss: 0.3284\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3310 - val_loss: 0.3246\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3490 - val_loss: 0.3539\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3307 - val_loss: 0.3272\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3269 - val_loss: 0.3213\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3257 - val_loss: 0.3238\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3228 - val_loss: 0.3468\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3210 - val_loss: 0.3111\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3216 - val_loss: 0.3176\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3852 - val_loss: 0.3282\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3253 - val_loss: 0.3189\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3165 - val_loss: 0.3140\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3149 - val_loss: 0.3138\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3134 - val_loss: 0.3308\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3123 - val_loss: 0.3083\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3098 - val_loss: 0.3209\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3080 - val_loss: 0.3188\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3081 - val_loss: 0.3113\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3045 - val_loss: 0.3144\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3035 - val_loss: 0.3176\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3042 - val_loss: 0.3172\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3030 - val_loss: 0.3139\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2986 - val_loss: 0.3012\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2984 - val_loss: 0.3002\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2978 - val_loss: 0.2966\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2977 - val_loss: 0.2963\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2983 - val_loss: 0.2959\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2940 - val_loss: 0.2962\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2960 - val_loss: 0.3059\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2920 - val_loss: 0.3115\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2936 - val_loss: 0.3037\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2909 - val_loss: 0.2989\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2898 - val_loss: 0.3034\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2893 - val_loss: 0.3214\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2877 - val_loss: 0.3074\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2867 - val_loss: 0.2982\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2881 - val_loss: 0.3210\n",
      "121/121 [==============================] - 0s 664us/step - loss: 0.3268\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.9542 - val_loss: 0.5821\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5354 - val_loss: 0.4949\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4783 - val_loss: 0.4465\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4500 - val_loss: 0.4226\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4300 - val_loss: 0.4173\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4148 - val_loss: 0.3997\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4017 - val_loss: 0.3905\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3938 - val_loss: 0.3893\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3865 - val_loss: 0.3742\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3791 - val_loss: 0.3842\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3734 - val_loss: 0.3689\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3662 - val_loss: 0.3775\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3633 - val_loss: 0.3628\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3578 - val_loss: 0.3543\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3529 - val_loss: 0.3555\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3500 - val_loss: 0.3531\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3457 - val_loss: 0.3462\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3405 - val_loss: 0.3426\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3396 - val_loss: 0.3402\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3360 - val_loss: 0.3361\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3296 - val_loss: 0.3473\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3292 - val_loss: 0.3363\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3249 - val_loss: 0.3309\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3242 - val_loss: 0.3458\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3191 - val_loss: 0.3334\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3191 - val_loss: 0.3459\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3149 - val_loss: 0.3440\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3133 - val_loss: 0.3220\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3086 - val_loss: 0.3340\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3082 - val_loss: 0.3345\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3067 - val_loss: 0.3162\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3124 - val_loss: 0.3242\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3024 - val_loss: 0.3185\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3017 - val_loss: 0.3093\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2984 - val_loss: 0.3076\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2960 - val_loss: 0.3147\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2958 - val_loss: 0.3192\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2938 - val_loss: 0.3089\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2912 - val_loss: 0.3025\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2889 - val_loss: 0.3069\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2893 - val_loss: 0.3028\n",
      "Epoch 42/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2868 - val_loss: 0.3028\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2811 - val_loss: 0.2979\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2824 - val_loss: 0.3099\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2801 - val_loss: 0.3095\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2798 - val_loss: 0.3016\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2793 - val_loss: 0.2923\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2781 - val_loss: 0.3063\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2777 - val_loss: 0.2912\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2803 - val_loss: 0.3167\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2774 - val_loss: 0.2996\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2761 - val_loss: 0.3553\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2763 - val_loss: 0.3219\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2762 - val_loss: 0.3480\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2744 - val_loss: 0.3012\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2736 - val_loss: 0.3147\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2688 - val_loss: 0.2951\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2685 - val_loss: 0.2922\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2668 - val_loss: 0.2910\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2653 - val_loss: 0.2935\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2644 - val_loss: 0.2828\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2635 - val_loss: 0.3101\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2646 - val_loss: 0.2855\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2634 - val_loss: 0.2989\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2620 - val_loss: 0.2856\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2617 - val_loss: 0.3238\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2614 - val_loss: 0.2962\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2592 - val_loss: 0.2855\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2579 - val_loss: 0.2808\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2553 - val_loss: 0.3011\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2561 - val_loss: 0.2882\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2546 - val_loss: 0.2849\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2581 - val_loss: 0.2938\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2555 - val_loss: 0.2812\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2541 - val_loss: 0.2921\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2558 - val_loss: 0.2886\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2533 - val_loss: 0.2866\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2558 - val_loss: 0.2802\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2533 - val_loss: 0.2897\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2509 - val_loss: 0.2827\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2511 - val_loss: 0.2948\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2546 - val_loss: 0.2818\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2523 - val_loss: 0.3343\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2516 - val_loss: 0.2822\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2492 - val_loss: 0.2886\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2525 - val_loss: 0.2750\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2524 - val_loss: 0.2926\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2505 - val_loss: 0.2866\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2501 - val_loss: 0.3038\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2506 - val_loss: 0.2778\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2484 - val_loss: 0.2921\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2470 - val_loss: 0.3004\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2504 - val_loss: 0.2915\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2483 - val_loss: 0.2944\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2432 - val_loss: 0.2890\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2448 - val_loss: 0.2786\n",
      "121/121 [==============================] - 0s 821us/step - loss: 0.3118\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 2.4766 - val_loss: 1.2955\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 1.0378 - val_loss: 0.8473\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7905 - val_loss: 0.7362\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.7216 - val_loss: 0.6904\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6892 - val_loss: 0.6649\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6684 - val_loss: 0.6466\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6522 - val_loss: 0.6323\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6386 - val_loss: 0.6198\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6271 - val_loss: 0.6090\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6165 - val_loss: 0.5991\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6069 - val_loss: 0.5902\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5981 - val_loss: 0.5819\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5900 - val_loss: 0.5740\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5825 - val_loss: 0.5669\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5755 - val_loss: 0.5599\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5689 - val_loss: 0.5537\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5628 - val_loss: 0.5483\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5571 - val_loss: 0.5430\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5519 - val_loss: 0.5374\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5469 - val_loss: 0.5323\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5423 - val_loss: 0.5279\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5380 - val_loss: 0.5235\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5339 - val_loss: 0.5190\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5303 - val_loss: 0.5152\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5267 - val_loss: 0.5118\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5234 - val_loss: 0.5085\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5202 - val_loss: 0.5054\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5173 - val_loss: 0.5023\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5145 - val_loss: 0.4996\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5118 - val_loss: 0.4968\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5093 - val_loss: 0.4942\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5068 - val_loss: 0.4918\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5045 - val_loss: 0.4894\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5023 - val_loss: 0.4873\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5002 - val_loss: 0.4856\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4981 - val_loss: 0.4835\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4961 - val_loss: 0.4817\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4942 - val_loss: 0.4791\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4923 - val_loss: 0.4776\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4905 - val_loss: 0.4759\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4887 - val_loss: 0.4738\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4871 - val_loss: 0.4721\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4854 - val_loss: 0.4707\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4838 - val_loss: 0.4689\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4822 - val_loss: 0.4674\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4806 - val_loss: 0.4659\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4792 - val_loss: 0.4646\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4777 - val_loss: 0.4637\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4761 - val_loss: 0.4616\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4749 - val_loss: 0.4601\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4736 - val_loss: 0.4590\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4723 - val_loss: 0.4579\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4710 - val_loss: 0.4562\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4697 - val_loss: 0.4549\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4686 - val_loss: 0.4544\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4674 - val_loss: 0.4529\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4663 - val_loss: 0.4519\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4651 - val_loss: 0.4507\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4640 - val_loss: 0.4501\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4629 - val_loss: 0.4486\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4620 - val_loss: 0.4476\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4610 - val_loss: 0.4466\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4600 - val_loss: 0.4454\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4590 - val_loss: 0.4445\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4580 - val_loss: 0.4440\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4570 - val_loss: 0.4432\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4561 - val_loss: 0.4420\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4550 - val_loss: 0.4406\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4542 - val_loss: 0.4402\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4532 - val_loss: 0.4388\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4524 - val_loss: 0.4384\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4514 - val_loss: 0.4377\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4506 - val_loss: 0.4362\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4496 - val_loss: 0.4350\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4489 - val_loss: 0.4342\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4479 - val_loss: 0.4340\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4470 - val_loss: 0.4325\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4463 - val_loss: 0.4322\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4454 - val_loss: 0.4311\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4447 - val_loss: 0.4305\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4439 - val_loss: 0.4295\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4430 - val_loss: 0.4292\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4424 - val_loss: 0.4285\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4416 - val_loss: 0.4274\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4409 - val_loss: 0.4268\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4402 - val_loss: 0.4263\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4395 - val_loss: 0.4256\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4389 - val_loss: 0.4250\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4381 - val_loss: 0.4248\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4376 - val_loss: 0.4235\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4370 - val_loss: 0.4232\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4364 - val_loss: 0.4227\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4358 - val_loss: 0.4221\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4352 - val_loss: 0.4217\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4346 - val_loss: 0.4210\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4341 - val_loss: 0.4205\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4335 - val_loss: 0.4202\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4328 - val_loss: 0.4202\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4325 - val_loss: 0.4191\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4319 - val_loss: 0.4183\n",
      "121/121 [==============================] - 0s 846us/step - loss: 0.4362\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 2.9576 - val_loss: 1.4908\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.1903 - val_loss: 0.9521\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.8971 - val_loss: 0.8203\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.8109 - val_loss: 0.7650\n",
      "Epoch 5/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 2ms/step - loss: 0.7678 - val_loss: 0.7320\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.7389 - val_loss: 0.7067\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.7160 - val_loss: 0.6863\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6965 - val_loss: 0.6679\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6792 - val_loss: 0.6517\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6635 - val_loss: 0.6371\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6492 - val_loss: 0.6240\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6362 - val_loss: 0.6114\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6239 - val_loss: 0.5999\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6127 - val_loss: 0.5890\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6022 - val_loss: 0.5791\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5921 - val_loss: 0.5695\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5830 - val_loss: 0.5609\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5744 - val_loss: 0.5524\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5665 - val_loss: 0.5444\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5588 - val_loss: 0.5375\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5517 - val_loss: 0.5306\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5450 - val_loss: 0.5237\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5386 - val_loss: 0.5167\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5326 - val_loss: 0.5112\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5272 - val_loss: 0.5055\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5223 - val_loss: 0.5007\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5177 - val_loss: 0.4962\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5136 - val_loss: 0.4916\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5099 - val_loss: 0.4881\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5067 - val_loss: 0.4851\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5037 - val_loss: 0.4819\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5012 - val_loss: 0.4791\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4990 - val_loss: 0.4772\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4968 - val_loss: 0.4749\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4948 - val_loss: 0.4731\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4929 - val_loss: 0.4719\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4912 - val_loss: 0.4694\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4895 - val_loss: 0.4676\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4878 - val_loss: 0.4657\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4864 - val_loss: 0.4648\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4848 - val_loss: 0.4627\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4834 - val_loss: 0.4619\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4821 - val_loss: 0.4607\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4808 - val_loss: 0.4592\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4796 - val_loss: 0.4576\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4783 - val_loss: 0.4566\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4771 - val_loss: 0.4557\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4760 - val_loss: 0.4548\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4750 - val_loss: 0.4533\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4739 - val_loss: 0.4521\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4729 - val_loss: 0.4515\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4719 - val_loss: 0.4505\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4710 - val_loss: 0.4491\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4701 - val_loss: 0.4482\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4691 - val_loss: 0.4480\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4683 - val_loss: 0.4468\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4675 - val_loss: 0.4458\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4666 - val_loss: 0.4450\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4658 - val_loss: 0.4446\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4650 - val_loss: 0.4430\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4644 - val_loss: 0.4426\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4636 - val_loss: 0.4417\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4629 - val_loss: 0.4408\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4623 - val_loss: 0.4400\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4616 - val_loss: 0.4397\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4608 - val_loss: 0.4394\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4602 - val_loss: 0.4387\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4596 - val_loss: 0.4375\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4590 - val_loss: 0.4371\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4583 - val_loss: 0.4361\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4577 - val_loss: 0.4361\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4571 - val_loss: 0.4354\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4565 - val_loss: 0.4341\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4559 - val_loss: 0.4333\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4554 - val_loss: 0.4329\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4548 - val_loss: 0.4326\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4543 - val_loss: 0.4317\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4538 - val_loss: 0.4318\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4533 - val_loss: 0.4311\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4526 - val_loss: 0.4310\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4522 - val_loss: 0.4295\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4517 - val_loss: 0.4291\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4512 - val_loss: 0.4286\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4507 - val_loss: 0.4283\n",
      "Epoch 85/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4501 - val_loss: 0.4278\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4496 - val_loss: 0.4271\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4491 - val_loss: 0.4266\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4486 - val_loss: 0.4260\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4481 - val_loss: 0.4261\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4477 - val_loss: 0.4249\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4472 - val_loss: 0.4246\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4467 - val_loss: 0.4244\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4461 - val_loss: 0.4238\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4458 - val_loss: 0.4235\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4453 - val_loss: 0.4230\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4448 - val_loss: 0.4223\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4441 - val_loss: 0.4226\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4438 - val_loss: 0.4222\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4434 - val_loss: 0.4211\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4430 - val_loss: 0.4207\n",
      "121/121 [==============================] - 0s 868us/step - loss: 0.4227\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 3.1650 - val_loss: 1.4864\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 1.1394 - val_loss: 0.8626\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.8013 - val_loss: 0.7249\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7082 - val_loss: 0.6739\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6681 - val_loss: 0.6496\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6433 - val_loss: 0.6292\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6248 - val_loss: 0.6136\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6118 - val_loss: 0.6020\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5995 - val_loss: 0.5935\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5897 - val_loss: 0.5835\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5810 - val_loss: 0.5748\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5730 - val_loss: 0.5669\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5665 - val_loss: 0.5601\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5604 - val_loss: 0.5539\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5542 - val_loss: 0.5508\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5497 - val_loss: 0.5440\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5436 - val_loss: 0.5373\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5406 - val_loss: 0.5329\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5352 - val_loss: 0.5284\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5332 - val_loss: 0.5254\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5284 - val_loss: 0.5227\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5246 - val_loss: 0.5167\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5222 - val_loss: 0.5139\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5188 - val_loss: 0.5112\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5156 - val_loss: 0.5067\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5133 - val_loss: 0.5058\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5108 - val_loss: 0.5039\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5082 - val_loss: 0.4986\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5062 - val_loss: 0.4963\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5039 - val_loss: 0.4968\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5021 - val_loss: 0.4921\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5003 - val_loss: 0.4898\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4981 - val_loss: 0.4887\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4959 - val_loss: 0.4859\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4949 - val_loss: 0.4844\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4928 - val_loss: 0.4857\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4910 - val_loss: 0.4813\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4906 - val_loss: 0.4803\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4878 - val_loss: 0.4782\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4880 - val_loss: 0.4789\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4853 - val_loss: 0.4756\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4855 - val_loss: 0.4750\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4838 - val_loss: 0.4736\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4829 - val_loss: 0.4734\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4809 - val_loss: 0.4712\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4805 - val_loss: 0.4700\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4792 - val_loss: 0.4687\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4778 - val_loss: 0.4704\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4774 - val_loss: 0.4676\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4763 - val_loss: 0.4662\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4750 - val_loss: 0.4671\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4731 - val_loss: 0.4639\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4737 - val_loss: 0.4631\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4727 - val_loss: 0.4630\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4708 - val_loss: 0.4614\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4707 - val_loss: 0.4622\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4696 - val_loss: 0.4604\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4687 - val_loss: 0.4593\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4679 - val_loss: 0.4595\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4670 - val_loss: 0.4578\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4661 - val_loss: 0.4570\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4651 - val_loss: 0.4556\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4639 - val_loss: 0.4550\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4633 - val_loss: 0.4540\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4620 - val_loss: 0.4530\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4616 - val_loss: 0.4530\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4605 - val_loss: 0.4520\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4593 - val_loss: 0.4515\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4588 - val_loss: 0.4506\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4577 - val_loss: 0.4499\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4563 - val_loss: 0.4489\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4557 - val_loss: 0.4483\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4547 - val_loss: 0.4473\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4535 - val_loss: 0.4464\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4532 - val_loss: 0.4461\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4523 - val_loss: 0.4456\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4516 - val_loss: 0.4447\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4508 - val_loss: 0.4444\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4503 - val_loss: 0.4438\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4496 - val_loss: 0.4431\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4490 - val_loss: 0.4424\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4485 - val_loss: 0.4419\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4478 - val_loss: 0.4412\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4472 - val_loss: 0.4409\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4467 - val_loss: 0.4404\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4461 - val_loss: 0.4398\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4455 - val_loss: 0.4390\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4449 - val_loss: 0.4387\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4444 - val_loss: 0.4384\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4438 - val_loss: 0.4378\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4433 - val_loss: 0.4371\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4428 - val_loss: 0.4367\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4422 - val_loss: 0.4362\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4416 - val_loss: 0.4355\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4410 - val_loss: 0.4352\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4405 - val_loss: 0.4342\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4400 - val_loss: 0.4343\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4396 - val_loss: 0.4338\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4390 - val_loss: 0.4332\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4385 - val_loss: 0.4327\n",
      "121/121 [==============================] - 0s 832us/step - loss: 0.4753\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Cannot clone object <tensorflow.python.keras.wrappers.scikit_learn.KerasRegressor object at 0x7f6c606a66a0>, as the constructor either does not set or modifies parameter learning_rate",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-63-fc34dd2588bb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m }\n\u001b[1;32m      9\u001b[0m \u001b[0mrnd_search_cv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomizedSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeras_reg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_distribs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m rnd_search_cv.fit(X_train, y_train, epochs=100,\n\u001b[0m\u001b[1;32m     11\u001b[0m                   \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m                   callbacks=[keras.callbacks.EarlyStopping(patience=10), tensorboard_cb])\n",
      "\u001b[0;32m~/homl/lib/python3.8/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    733\u001b[0m             \u001b[0;31m# we clone again after setting params in case some\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m             \u001b[0;31m# of the params are estimators as well.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 735\u001b[0;31m             self.best_estimator_ = clone(clone(base_estimator).set_params(\n\u001b[0m\u001b[1;32m    736\u001b[0m                 **self.best_params_))\n\u001b[1;32m    737\u001b[0m             \u001b[0mrefit_start_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/homl/lib/python3.8/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36mclone\u001b[0;34m(estimator, safe)\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0mparam2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparams_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mparam1\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mparam2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m             raise RuntimeError('Cannot clone object %s, as the constructor '\n\u001b[0m\u001b[1;32m     81\u001b[0m                                \u001b[0;34m'either does not set or modifies parameter %s'\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m                                (estimator, name))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Cannot clone object <tensorflow.python.keras.wrappers.scikit_learn.KerasRegressor object at 0x7f6c606a66a0>, as the constructor either does not set or modifies parameter learning_rate"
     ]
    }
   ],
   "source": [
    "from scipy.stats import reciprocal\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "param_distribs = {\n",
    "    \"n_hidden\" : [0, 1, 2, 3],\n",
    "    \"n_neurons\" : np.arange(1,100),\n",
    "    \"learning_rate\" : reciprocal(3e-4, 3e-2)\n",
    "}\n",
    "rnd_search_cv = RandomizedSearchCV(keras_reg, param_distribs, n_iter=10, cv=3)\n",
    "rnd_search_cv.fit(X_train, y_train, epochs=50,\n",
    "                  validation_data=(X_valid, y_valid), \n",
    "                  callbacks=[keras.callbacks.EarlyStopping(patience=10), tensorboard_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.00571631627106251, 'n_hidden': 3, 'n_neurons': 89}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnd_search_cv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.3158598641554515"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnd_search_cv.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Randomized search works well for many fairly simple problems, however when training is slow, this approach only explores a tiny portion of the hyperparameter space.\n",
    "\n",
    "You can first run a quick Random search using wide ranges of hyperparparameter values, then run another search using smaller ranges of values centered on the best ones found during the first ones, and so one. This, however, is a very time consuming approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some Python libraries you can use to optimize hyperparameters\n",
    "\n",
    "[Hyperopt](https://github.com/hyperopt/hyperopt)\n",
    "    - Popular library for optimization over all sorts of complex seach spaces (include real and discrete values)\n",
    "    \n",
    "[Hyperas](https://github.com/maxpumperla/hyperas), [kopt](https://github.com/Avsecz/kopt), [Talos](https://github.com/autonomio/talos)\n",
    "    - Useful for optimizing Keras hyperparameters\n",
    "    \n",
    "[Keras tuner](https://homl.info/kerastuner)\n",
    "    - Library by Google for Keras models with hosted service for visualization and Analysis\n",
    "    \n",
    "[Scikit-Optimize(skopt)](https://scikit-optimize.github.io)\n",
    "    - General purpose optimization library. ```BayesSearchCV``` performs optimization similarly to GridSearchCV\n",
    "    \n",
    "[Spearmint](https://github.com/JasperSnoek/spearmint)\n",
    "    - Bayesian optimization library\n",
    "    \n",
    "[Hyperband](https://github.com/zygmuntz/hyperband)\n",
    "    - Fast hyperparameter tuning library based on the recent Hyperband paper\n",
    "    \n",
    "[Sklearn-Deap](https://github.com/rsteca/sklearn-deap)\n",
    "    - Based on evolutionary algorithms with GridSearchCV-like interface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of Hidden Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
