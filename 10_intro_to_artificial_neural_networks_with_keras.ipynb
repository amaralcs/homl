{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *Perceptron* is on of the simplest Artificial neural network architectures, proposed in 1957 by Frank Rosenblatt. It is based on a *threshold logic unit (TLU)* and it computes a weighted sum of its inputs\n",
    "\n",
    "$$ z = w_1x_1 + \\cdots + w_nx_n = \\textbf{x}^{\\intercal}\\textbf{w} $$\n",
    "\n",
    "then applies a step function to that sum and outputs the result: $h_w(\\textbf{x})=\\text{step}(\\textbf{x})$. One of the most common step function used is the *Heaviside step function*\n",
    "\n",
    "$$ \\text{heaviside}(z) = \\begin{cases} 0 & \\text{if } z<0 \\\\ 1 & \\text{if } z\\gt0 \\end{cases}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A single TLU can be used for binary classification; it computes a linear combination of its inputs and if the output reaches a threshold, it outputs a positive class, otherwise outputs the negative class.\n",
    "\n",
    "A perceptron is composed of a single layer of TLUs, with each TLU connected to all the inputs. When all the neurons in a layer are connected to every neuron in the previous layer the layer is called a *fully connected* or *dense* layer. *Input Neurons* are simple inputs that output whatever they are fed and all input neurons form the *input layer*. A bias neuron is generally added, tipycally represented by a *bias neuron*, which outputs 1 all the time. (e.g. architecture pg 286 fig 10-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then write the outputs of a fully connected layer as \n",
    "$$ h_{\\textbf{W, b}}(\\textbf{X}) = \\phi(\\textbf{XW + b})$$\n",
    "Where\n",
    "- $\\textbf{X}$ is the matrix of input features (one row per instance, one col per feature)\n",
    "- $\\textbf{W}$ contains the connection weights, except the ones from the bias neuron (one row per input neuron, one column per artificial neuron in the layer)\n",
    "- $\\phi$ is called the *activation function* (when the neurons are TLU, this is a step function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The perceptron learning rule reinforces connections between neurons tha help reduce the error: the perceptron is fed one training instance at a time, and for each instance it makes its predictions. For every output neuron that produced a wrong predictions, it reinforces the connection weights from the inputs that would have contributed to the correct prediction\n",
    "\n",
    "$$ w_{i,j}^{\\text{next step}} = w_{i,j} +\\eta(y_j - \\hat{y_j})x_i$$\n",
    "\n",
    "where \n",
    "- $w_{i,j}$ is the weight between ith input neuron and jth output neuron\n",
    "- $x_i$ is the ith input value of the current training instance\n",
    "- $\\hat{y_j}$ is the output of the jth output neuron \n",
    "- $y_j$ is the target output of the jth ouptut neuron\n",
    "- $\\eta$ is the learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The multilayer perceptron and backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An MLP consistis of one input layer, one or more layers of TLUs (called *hidden layers*) and one final layer of TLUs called the *output layer*. Every except the output layer includes a bias neuron and is fully connected to the next layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train an MLP, we use [backpropagation](https://homl.info/44). In short, it is Gradient Descent and it is able to compute the gradient of the network's error with regard to every single model parameter, thus it is able to find out how much it should tweak each connection weight and bias in order to reduce the error. This process is called *autodiff*, appendix D has more info on it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's how it works\n",
    "- It handles one mini-batch at a time (e.g. 32 instances) and goes through the training set multiple times, each pass is called an *epoch*\n",
    "- Each mini-batch is passed is passed to the network's input layer, which sends it to the first hidden layer. The algorithm then computes the outputs of this layer and passes it to the next layer, and so on, until we get the output of the output layer. This is called a *forward pass* and the intermediate results are saved\n",
    "- Next we calculate the network's output error (using some loss function)\n",
    "- Then it computs how much each output connection contributed to the error (done using chain rule)\n",
    "- The algorithm then measures how much of these error contributions came from each connection in the layer below until it reaches the input layer\n",
    "- Finally, it performs a gradient descent step to tweak all the connection weights in the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One change that had to be made to the original MLP architecture was replacing the step function with the logistic function $\\sigma(z) = 1 / (1 +\\exp(-z))$, this allows for gradients to be computed as it is a smooth function.\n",
    "\n",
    "Some other choices of function are:\n",
    "- Hyperbolic tan $\\tanh(z) = 2\\sigma(2z) - 1$\n",
    "\n",
    "Another S-shaped function, continues and differentiable. Its outputs are in the range -1 to 1, making each layer's output more or less centered around 0 at the beginning of training, which helps speed up convergence.\n",
    "\n",
    "- Rectified Linear unit $ReLU(z) = \\max(0,z)$\n",
    "\n",
    "Continuous but not differentiable at $z=0$, however it works very well and has become the default."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Activation functions are useful because they can add non-linearity to each layer. Recall that a linear transformation of linear transformations is also linear. Using a non-linear function allows for an MLP to learn more complex patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression MLPs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use MLP for regression we use an output neuron for each value we want to predict. In the univariate case (e.g. predicting house price) only a single output neuron is needed. \n",
    "\n",
    "For multivariate problems, you need one output neuron per output dimension. For example to locate the center of an object in an image, you need to predict 2D coordinates, thus 2 output neurons. If you also want to place a bounding box around the object, you need two more numbers, the width and height of the object. In total, 4 output neurons.\n",
    "\n",
    "In general we do not want to use any activation function for output neurons so they are free to output any range of values. To guarantee the range of values is always positive, use ReLU or *softplus*, which is a smooth variant of ReLU: $\\text{softplus}(z) = \\log(1 + \\exp(z))$. \n",
    "Finally if we want to guarantee the predictions will fall between a range of values we can use the logistic or hyperbolic tangent function, scaling the labels to the appropriate values.\n",
    "\n",
    "The typical loss function used is MSE, however if you have a lot of outliers in your training set you may want to use the mean absolute error instead. Alternatively use [Huber loss](https://en.wikipedia.org/wiki/Huber_loss), which is a combination of both.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typical regression MLP architecture\n",
    "\n",
    "| Hyperparameter | Typical value |\n",
    "|     ---        |      ---      |\n",
    "|# input neurons | One per input feature (e.g. 28x28=784 for MNIST) | \n",
    "|# hidden layers | Variable (typically 1 to 5) |\n",
    "|# neurons per hidden layers | Variable (typically 10 to 100) |\n",
    "|# output layer | 1 per prediction dimension |\n",
    "|Activation function | $\\begin{cases} \n",
    "                        \\text{None} & \\text{ for any range of values } \\\\ \n",
    "                        ReLU/\\text{softplus} & \\text{ positive outputs }\\\\\n",
    "                        \\text{logistic/tanh} & \\text{ bounded outputs}\n",
    "                        \\end{cases}$ |\n",
    "|Loss Function | MSE or MAE/Huber (if outliers)|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification MLPs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For binary classification, we just need a single output neuron using the logistic activation function. The output will be in the range 0 - 1 and we can interpret it as an estimated class probability of the positive class. The estimated probability for the negative class is one minus that number."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLPs can also be used for multilabel binary classification. For example, in an e-mail classification system that tags messages as spam/ham and urgent/non-urgent we would use two output neurons with the logistic function. The first outputs the probability that the e-mail is spam and the second the probability the e-mail is urgent. \n",
    "More generally, we use one output neuron for each positive class.\n",
    "\n",
    "For multiclass calssification (e.g. identifying digit classes 0 through 9), then we need one output neuron per class and should use the softmax activation to ensure estimated probabilities are between 0-1 and they add up to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the loss function, since we're preducting probability distributions, the cross-entropy loss is generally a good choice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typical classification MLP architecture\n",
    "\n",
    "| Hyperparameter | Binary Classification | Multilabel Binary Classification | Multiclass Classification | \n",
    "| --- | --- | --- | --- |\n",
    "|# input neurons | One per input feature | One per input feature | One per input feature |\n",
    "|# hidden layers | Variable (typically 1 to 5) | Variable (typically 1 to 5) | Variable (typically 1 to 5) |\n",
    "|# neurons per hidden layers | Variable (typically 10 to 100) | Variable (typically 10 to 100)| Variable (typically 10 to 100)|\n",
    "|# output neurons | 1 | 1 per label | 1 per class |\n",
    "|Activation Function| Logistic | Logistic | Softmax |\n",
    "|Loss Function | Cross Entropy | Cross Entropy | Cross Entropy |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1\n",
    "Play around in the [Tensorflow Playground](https://playground.tensorflow.org) to get a better feeling for ANNs and explore the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing MLPs with Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building an Image Classifier using the Sequential API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this chapter, we will tackle Fashion MNIST, which is a drop-in replacement of MNIST. The images represent fashion items instead of digits, so each class is more diverse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_full.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data is loaded as ints in the range from 0 to 255. Let's create a validation set and scale the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('uint8')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_full.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid, X_train = X_train_full[:5000]/255.0, X_train_full[5000:]/255.0\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 0, 7, ..., 3, 0, 5], dtype=uint8)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to map the target values to their actual class as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the model using the Sequential API\n",
    "We'll start by creating a classification MLP with two hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten, Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=[28,28])) # Converts inputs to 1D array\n",
    "model.add(Dense(300, activation='relu'))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Sequential API is the simplest kind of model for NNs that are just composed of a single stack of layers connected sequentially. Another way to write the same model could be as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Flatten(input_shape=[28,28]),\n",
    "    Dense(300, activation='relu'),\n",
    "    Dense(100, activation='relu'),\n",
    "    Dense(10, activation='softmax'),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can vew a definition of the model by using summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_2 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 300)               235500    \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 100)               30100     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 266,610\n",
      "Trainable params: 266,610\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the model has a lot of parameters, it has a lot of flexibility to train the data. However this also means that it runs the risk of overfitting, especially when we don't have much training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can view a model's Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.layers.core.Flatten at 0x7f89180ff430>,\n",
       " <keras.layers.core.Dense at 0x7f89180ff490>,\n",
       " <keras.layers.core.Dense at 0x7f89180ff610>,\n",
       " <keras.layers.core.Dense at 0x7f89180ff400>]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dense_4'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden1 = model.layers[1]\n",
    "hidden1.name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As well as it's weights and biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.04130178,  0.04122939,  0.05185226, ...,  0.03959126,\n",
       "        -0.04257523,  0.02830424],\n",
       "       [-0.07055026,  0.05017821, -0.03449353, ..., -0.01324287,\n",
       "        -0.02289219, -0.07043223],\n",
       "       [-0.06730034,  0.00134465,  0.00183922, ..., -0.04859115,\n",
       "        -0.05091444, -0.01537013],\n",
       "       ...,\n",
       "       [-0.03324409, -0.01379194, -0.0341923 , ...,  0.0124127 ,\n",
       "        -0.05074498, -0.00316779],\n",
       "       [ 0.00126698, -0.06556682,  0.04718859, ...,  0.04622825,\n",
       "         0.04819635,  0.07382867],\n",
       "       [-0.01058099, -0.06618245, -0.01807966, ..., -0.04497568,\n",
       "         0.05197492, -0.06534347]], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights, biases = hidden1.get_weights()\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the weights are initialized randomly (to break simmetry) and the biases set to zero. To use other initialization methods we can set the ```kernel_initializer``` or ```bias_initializer``` when creating the layer.\n",
    "\n",
    "Next we compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss = 'sparse_categorical_crossentropy',\n",
    "              optimizer = 'sgd',\n",
    "              metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use ```sparse_categorical_crossentropy``` because we have sparse labels (i.e. for each instance there is only a single target class) and the classes are exclusive. If instead we had one target probability per class for each instance  (such as one-hot vectors for a single class) we'd use ```categorical_crossentropy``` instead. \n",
    "\n",
    "The optimizer set to ```sgd``` means we'll train the model using simple stochastic gradient descent. \n",
    "\n",
    "Finally, we can fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "RELOAD = False\n",
    "CHAPTER_DIR = 'saved_models/10_intro_to_anns/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 55000 samples, validate on 5000 samples\n",
      "Epoch 1/30\n",
      "55000/55000 [==============================] - 6s 108us/step - loss: 0.7186 - accuracy: 0.7651 - val_loss: 0.5364 - val_accuracy: 0.8194\n",
      "Epoch 2/30\n",
      "55000/55000 [==============================] - 6s 103us/step - loss: 0.4887 - accuracy: 0.8307 - val_loss: 0.4379 - val_accuracy: 0.8560\n",
      "Epoch 3/30\n",
      "55000/55000 [==============================] - 6s 105us/step - loss: 0.4444 - accuracy: 0.8454 - val_loss: 0.4360 - val_accuracy: 0.8480\n",
      "Epoch 4/30\n",
      "55000/55000 [==============================] - 6s 109us/step - loss: 0.4169 - accuracy: 0.8551 - val_loss: 0.4349 - val_accuracy: 0.8452\n",
      "Epoch 5/30\n",
      "55000/55000 [==============================] - 6s 106us/step - loss: 0.3962 - accuracy: 0.8620 - val_loss: 0.3795 - val_accuracy: 0.8710\n",
      "Epoch 6/30\n",
      "55000/55000 [==============================] - 6s 107us/step - loss: 0.3785 - accuracy: 0.8663 - val_loss: 0.3840 - val_accuracy: 0.8616\n",
      "Epoch 7/30\n",
      "55000/55000 [==============================] - 7s 121us/step - loss: 0.3649 - accuracy: 0.8713 - val_loss: 0.3661 - val_accuracy: 0.8712\n",
      "Epoch 8/30\n",
      "55000/55000 [==============================] - 6s 102us/step - loss: 0.3549 - accuracy: 0.8737 - val_loss: 0.3538 - val_accuracy: 0.8786\n",
      "Epoch 9/30\n",
      "55000/55000 [==============================] - 6s 105us/step - loss: 0.3445 - accuracy: 0.8782 - val_loss: 0.3556 - val_accuracy: 0.8750\n",
      "Epoch 10/30\n",
      "55000/55000 [==============================] - 6s 102us/step - loss: 0.3347 - accuracy: 0.8806 - val_loss: 0.3862 - val_accuracy: 0.8622\n",
      "Epoch 11/30\n",
      "55000/55000 [==============================] - 6s 104us/step - loss: 0.3261 - accuracy: 0.8835 - val_loss: 0.3385 - val_accuracy: 0.8798\n",
      "Epoch 12/30\n",
      "55000/55000 [==============================] - 6s 101us/step - loss: 0.3187 - accuracy: 0.8869 - val_loss: 0.3451 - val_accuracy: 0.8760\n",
      "Epoch 13/30\n",
      "55000/55000 [==============================] - 5s 98us/step - loss: 0.3116 - accuracy: 0.8883 - val_loss: 0.3210 - val_accuracy: 0.8876\n",
      "Epoch 14/30\n",
      "55000/55000 [==============================] - 5s 98us/step - loss: 0.3052 - accuracy: 0.8903 - val_loss: 0.3207 - val_accuracy: 0.8822\n",
      "Epoch 15/30\n",
      "55000/55000 [==============================] - 6s 103us/step - loss: 0.2982 - accuracy: 0.8922 - val_loss: 0.3591 - val_accuracy: 0.8726\n",
      "Epoch 16/30\n",
      "55000/55000 [==============================] - 6s 102us/step - loss: 0.2923 - accuracy: 0.8953 - val_loss: 0.3178 - val_accuracy: 0.8862\n",
      "Epoch 17/30\n",
      "55000/55000 [==============================] - 6s 102us/step - loss: 0.2860 - accuracy: 0.8978 - val_loss: 0.3140 - val_accuracy: 0.8882\n",
      "Epoch 18/30\n",
      "55000/55000 [==============================] - 6s 111us/step - loss: 0.2818 - accuracy: 0.8991 - val_loss: 0.3086 - val_accuracy: 0.8930\n",
      "Epoch 19/30\n",
      "55000/55000 [==============================] - 6s 107us/step - loss: 0.2757 - accuracy: 0.9015 - val_loss: 0.3203 - val_accuracy: 0.8824\n",
      "Epoch 20/30\n",
      "55000/55000 [==============================] - 6s 105us/step - loss: 0.2707 - accuracy: 0.9028 - val_loss: 0.3181 - val_accuracy: 0.8878\n",
      "Epoch 21/30\n",
      "55000/55000 [==============================] - 6s 103us/step - loss: 0.2653 - accuracy: 0.9047 - val_loss: 0.3124 - val_accuracy: 0.8886\n",
      "Epoch 22/30\n",
      "55000/55000 [==============================] - 6s 110us/step - loss: 0.2609 - accuracy: 0.9056 - val_loss: 0.2981 - val_accuracy: 0.8924\n",
      "Epoch 23/30\n",
      "55000/55000 [==============================] - 6s 101us/step - loss: 0.2560 - accuracy: 0.9091 - val_loss: 0.2987 - val_accuracy: 0.8922\n",
      "Epoch 24/30\n",
      "55000/55000 [==============================] - 6s 101us/step - loss: 0.2515 - accuracy: 0.9095 - val_loss: 0.2977 - val_accuracy: 0.8956\n",
      "Epoch 25/30\n",
      "55000/55000 [==============================] - 6s 113us/step - loss: 0.2477 - accuracy: 0.9106 - val_loss: 0.2968 - val_accuracy: 0.8932\n",
      "Epoch 26/30\n",
      "55000/55000 [==============================] - 6s 110us/step - loss: 0.2437 - accuracy: 0.9128 - val_loss: 0.2979 - val_accuracy: 0.8928\n",
      "Epoch 27/30\n",
      "55000/55000 [==============================] - 6s 103us/step - loss: 0.2394 - accuracy: 0.9147 - val_loss: 0.2927 - val_accuracy: 0.8926\n",
      "Epoch 28/30\n",
      "55000/55000 [==============================] - 6s 102us/step - loss: 0.2363 - accuracy: 0.9158 - val_loss: 0.2892 - val_accuracy: 0.8946\n",
      "Epoch 29/30\n",
      "55000/55000 [==============================] - 5s 100us/step - loss: 0.2318 - accuracy: 0.9175 - val_loss: 0.3118 - val_accuracy: 0.8870\n",
      "Epoch 30/30\n",
      "55000/55000 [==============================] - 6s 102us/step - loss: 0.2275 - accuracy: 0.9197 - val_loss: 0.3022 - val_accuracy: 0.8936\n"
     ]
    }
   ],
   "source": [
    "if RELOAD:\n",
    "    model = keras.models.load_model(CHAPTER_DIR + 'model1.h5')\n",
    "    history = model.history\n",
    "else:\n",
    "    history = model.fit(X_train, y_train, epochs=30, validation_data=(X_valid, y_valid))\n",
    "    model.save(CHAPTER_DIR + 'model1.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: instead of passing a validation set, you can also set the ```validation_split``` argument of the ```fit``` method to the ratio of the training set you want Keras to use for validation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the training set is very skewed, with some classes being underrepresented, it can be useful to set the ```class_weight``` parameter, this would give larger weight to underrepresented classes and a lower weight to overrepresented classes.\n",
    "\n",
    "If you need per instance weights, the ```sample_weight``` can be used. Per-instance weights can be useful if some instances are labelled by experts while others were labeled using a crowdsourcing platform: we might want to give more weights to the former."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can access the ```history.history``` to access the loss and accuracy of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from jupyterthemes import jtplot\n",
    "jtplot.style()\n",
    "\n",
    "pd.DataFrame(history.history).plot(figsize=(10, 5))\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0,1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the model performed better on the validation set than on the training set at the beginning, but this is not true.\n",
    "\n",
    "The validation error is computed at the *end* of each epoch while the training error is computed using a running mean *during* each epoch. So the training curve should actually be shifted half an epoch to the left. \n",
    "\n",
    "We can also tell that the model has not quite converged yet as the validation is still going down. We can continue training from where we left off by calling the fit method again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 55000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "55000/55000 [==============================] - 5s 85us/step - loss: 0.2238 - accuracy: 0.9199 - val_loss: 0.3600 - val_accuracy: 0.8776\n",
      "Epoch 2/20\n",
      "55000/55000 [==============================] - 5s 84us/step - loss: 0.2200 - accuracy: 0.9211 - val_loss: 0.2978 - val_accuracy: 0.8940\n",
      "Epoch 3/20\n",
      "55000/55000 [==============================] - 5s 82us/step - loss: 0.2165 - accuracy: 0.9230 - val_loss: 0.2908 - val_accuracy: 0.8952\n",
      "Epoch 4/20\n",
      "55000/55000 [==============================] - 4s 80us/step - loss: 0.2132 - accuracy: 0.9240 - val_loss: 0.2958 - val_accuracy: 0.8948\n",
      "Epoch 5/20\n",
      "55000/55000 [==============================] - 4s 82us/step - loss: 0.2103 - accuracy: 0.9258 - val_loss: 0.3096 - val_accuracy: 0.8908\n",
      "Epoch 6/20\n",
      "55000/55000 [==============================] - 5s 85us/step - loss: 0.2067 - accuracy: 0.9267 - val_loss: 0.2955 - val_accuracy: 0.8958\n",
      "Epoch 7/20\n",
      "55000/55000 [==============================] - 5s 86us/step - loss: 0.2030 - accuracy: 0.9272 - val_loss: 0.2880 - val_accuracy: 0.8964\n",
      "Epoch 8/20\n",
      "55000/55000 [==============================] - 5s 84us/step - loss: 0.2000 - accuracy: 0.9292 - val_loss: 0.2860 - val_accuracy: 0.8968\n",
      "Epoch 9/20\n",
      "55000/55000 [==============================] - 4s 81us/step - loss: 0.1975 - accuracy: 0.9294 - val_loss: 0.2986 - val_accuracy: 0.8894\n",
      "Epoch 10/20\n",
      "55000/55000 [==============================] - 4s 79us/step - loss: 0.1949 - accuracy: 0.9312 - val_loss: 0.2832 - val_accuracy: 0.8966\n",
      "Epoch 11/20\n",
      "55000/55000 [==============================] - 4s 79us/step - loss: 0.1907 - accuracy: 0.9323 - val_loss: 0.3166 - val_accuracy: 0.8894\n",
      "Epoch 12/20\n",
      "55000/55000 [==============================] - 5s 85us/step - loss: 0.1884 - accuracy: 0.9331 - val_loss: 0.2890 - val_accuracy: 0.8976\n",
      "Epoch 13/20\n",
      "55000/55000 [==============================] - 5s 83us/step - loss: 0.1853 - accuracy: 0.9344 - val_loss: 0.2970 - val_accuracy: 0.8924\n",
      "Epoch 14/20\n",
      "55000/55000 [==============================] - 5s 83us/step - loss: 0.1815 - accuracy: 0.9359 - val_loss: 0.2906 - val_accuracy: 0.8946\n",
      "Epoch 15/20\n",
      "55000/55000 [==============================] - 5s 83us/step - loss: 0.1799 - accuracy: 0.9355 - val_loss: 0.2993 - val_accuracy: 0.8954\n",
      "Epoch 16/20\n",
      "55000/55000 [==============================] - 5s 84us/step - loss: 0.1759 - accuracy: 0.9376 - val_loss: 0.2963 - val_accuracy: 0.8948\n",
      "Epoch 17/20\n",
      "55000/55000 [==============================] - 5s 84us/step - loss: 0.1733 - accuracy: 0.9388 - val_loss: 0.2864 - val_accuracy: 0.8974\n",
      "Epoch 18/20\n",
      "55000/55000 [==============================] - 4s 80us/step - loss: 0.1701 - accuracy: 0.9398 - val_loss: 0.3070 - val_accuracy: 0.8916\n",
      "Epoch 19/20\n",
      "55000/55000 [==============================] - 4s 76us/step - loss: 0.1670 - accuracy: 0.9412 - val_loss: 0.2884 - val_accuracy: 0.8984\n",
      "Epoch 20/20\n",
      "55000/55000 [==============================] - 5s 85us/step - loss: 0.1656 - accuracy: 0.9413 - val_loss: 0.2915 - val_accuracy: 0.8974\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if RELOAD:\n",
    "    model = keras.models.load_model(CHAPTER_DIR + 'model1_hist2.h5')\n",
    "    history2 = model.history\n",
    "else:\n",
    "    history2 = model.fit(X_train, y_train, epochs=20, validation_data=(X_valid, y_valid))\n",
    "    model.save(CHAPTER_DIR + 'model1_hist2.h5')\n",
    "\n",
    "pd.DataFrame(history2.history).plot(figsize=(10,5))\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0,1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To improve the model's performance, we perform our usual task: tuning hyperparameters.\n",
    "\n",
    "The first one to check is the learning rate. If that doesn't help we should check the optimizer (always re-tuning the learning rate after changing any other hyperparameter). If performance is still not great, we can tune the architecture, by changing the number of layers, neurons in each hidden layer and the activation function in the hidden layers.\n",
    "\n",
    "To estimate the generalisation error, we use the ```evaluation()``` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 45us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[77.22435361785888, 0.8424999713897705]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use this model to make predictions on the first 3 instances of the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new = X_test[:3]\n",
    "y_proba = model.predict(X_new)\n",
    "y_proba.round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each instance, the model estimated the probability per class, from class 0 to class 9. If we don't care about probabilities and only want the class that has the highest probability we can use ```predict_classes()``` instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9, 2, 1])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model.predict_classes(X_new)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Ankle boot', 'Pullover', 'Trouser'], dtype='<U11')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.array(class_names)[y_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 518.4x172.8 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(7.2, 2.4))\n",
    "for index, image in enumerate(X_new):\n",
    "    plt.subplot(1, 3, index + 1)\n",
    "    plt.imshow(image, cmap=\"binary\", interpolation=\"nearest\")\n",
    "    plt.axis('off')\n",
    "    plt.title(class_names[y_test[index]], fontsize=12)\n",
    "plt.subplots_adjust(wspace=0.2, hspace=0.5)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a regression MLP using the Sequential API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use scikit-learn to import a simplified version of the California housing dataset (only numerical features and no missing values). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(housing.data, housing.target)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_valid = scaler.transform(X_valid)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building the model is pretty similar to the steps taken for the example above. Since we're predicting house prices we only need 1 output and the output layer won't use an activation function and, we'll also use mean squared error as the error function. \n",
    "\n",
    "Since the dataset is noisy, we'll use a single layer with a few neurons to avoid overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11610 samples, validate on 3870 samples\n",
      "Epoch 1/20\n",
      "11610/11610 [==============================] - 0s 36us/step - loss: 1.6890 - val_loss: 0.5816\n",
      "Epoch 2/20\n",
      "11610/11610 [==============================] - 0s 34us/step - loss: 0.5183 - val_loss: 0.4979\n",
      "Epoch 3/20\n",
      "11610/11610 [==============================] - 0s 32us/step - loss: 0.4755 - val_loss: 0.4715\n",
      "Epoch 4/20\n",
      "11610/11610 [==============================] - 0s 30us/step - loss: 0.4482 - val_loss: 0.4615\n",
      "Epoch 5/20\n",
      "11610/11610 [==============================] - 0s 31us/step - loss: 0.4339 - val_loss: 0.4465\n",
      "Epoch 6/20\n",
      "11610/11610 [==============================] - 0s 31us/step - loss: 0.4191 - val_loss: 0.4310\n",
      "Epoch 7/20\n",
      "11610/11610 [==============================] - 0s 30us/step - loss: 0.4108 - val_loss: 0.4252\n",
      "Epoch 8/20\n",
      "11610/11610 [==============================] - 0s 33us/step - loss: 0.4058 - val_loss: 0.4202\n",
      "Epoch 9/20\n",
      "11610/11610 [==============================] - 0s 30us/step - loss: 0.4014 - val_loss: 0.4677\n",
      "Epoch 10/20\n",
      "11610/11610 [==============================] - 0s 31us/step - loss: 0.3983 - val_loss: 0.4120\n",
      "Epoch 11/20\n",
      "11610/11610 [==============================] - 0s 28us/step - loss: 0.3948 - val_loss: 0.4120\n",
      "Epoch 12/20\n",
      "11610/11610 [==============================] - 0s 33us/step - loss: 0.3888 - val_loss: 0.4071\n",
      "Epoch 13/20\n",
      "11610/11610 [==============================] - 0s 33us/step - loss: 0.3855 - val_loss: 0.4130\n",
      "Epoch 14/20\n",
      "11610/11610 [==============================] - 0s 37us/step - loss: 0.3832 - val_loss: 0.4032\n",
      "Epoch 15/20\n",
      "11610/11610 [==============================] - 0s 31us/step - loss: 0.3812 - val_loss: 0.3953\n",
      "Epoch 16/20\n",
      "11610/11610 [==============================] - 0s 27us/step - loss: 0.3790 - val_loss: 0.3948\n",
      "Epoch 17/20\n",
      "11610/11610 [==============================] - 0s 31us/step - loss: 0.3772 - val_loss: 0.3963\n",
      "Epoch 18/20\n",
      "11610/11610 [==============================] - 0s 29us/step - loss: 0.3747 - val_loss: 0.4067\n",
      "Epoch 19/20\n",
      "11610/11610 [==============================] - 0s 29us/step - loss: 0.3738 - val_loss: 0.3885\n",
      "Epoch 20/20\n",
      "11610/11610 [==============================] - 0s 30us/step - loss: 0.3715 - val_loss: 0.3871\n",
      "5160/5160 [==============================] - 0s 11us/step\n"
     ]
    }
   ],
   "source": [
    "if RELOAD:\n",
    "    model = keras.models.load_model(CHAPTER_DIR + 'model2.h5')\n",
    "else:\n",
    "    model = Sequential([\n",
    "        Dense(30, activation='relu', input_shape=X_train.shape[1:]),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    model.compile(loss='mean_squared_error', optimizer='sgd')\n",
    "    history = model.fit(X_train, y_train, epochs=20, validation_data=(X_valid, y_valid))\n",
    "    model.save(CHAPTER_DIR + 'model2.h5')\n",
    "\n",
    "mse_test = model.evaluate(X_test, y_test)\n",
    "X_new = X_test[:3]\n",
    "y_pred = model.predict(X_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 460.8x403.2 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "width = 0.35\n",
    "actual = y_test[:X_new.shape[0]]\n",
    "preds = y_pred.reshape(1,-1)[0]\n",
    "x_pos = np.arange(X_new.shape[0])\n",
    "\n",
    "ax.bar(x= x_pos-width/2.0, height=actual, width=width, label='actual')\n",
    "ax.bar(x= x_pos+width/2.0, height=preds, width=width, label='predicted')\n",
    "ax.legend()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sequential API is quite easy to use, however for more complex models Keras offers the Functional API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building complex models with the Functional API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One example of a non-sequential networs is a *Wide and Deep* NN, which was first introduced in a [2016 paper](https://homl.info/widedeep). This architecture allows for the Neural Net to learn both deep pattens (using the deep path) and simple rules (through the short path) (example image on page 309).\n",
    "\n",
    "In contrast, a regular MLP forces the data to go through the full stack of layers; thus, simple patterns in the data may end up being distorted by this sequence of transformations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One example network for the California housing dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input\n",
    "\n",
    "input_ = keras.layers.Input(shape=X_train.shape[1:])\n",
    "hidden1 = keras.layers.Dense(30, activation=\"relu\")(input_)\n",
    "hidden2 = keras.layers.Dense(30, activation=\"relu\")(hidden1)\n",
    "concat = keras.layers.concatenate([input_, hidden2])\n",
    "output = keras.layers.Dense(1)(concat)\n",
    "model = keras.models.Model(inputs=[input_], outputs=[output])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a breakdown of what's happening in the code above:\n",
    "- We create an Input object, containint a specification of what type of input the model will get (a model may have multiple inputs)\n",
    "- Next create a Dense layer, with 30 neurons using ReLU. We call it like a function passing it the inputs (hence functional api)\n",
    "- Create a second hidden layer as above and use it as a function. We pass it the outputs of the first hidden layer\n",
    "- Next we use the ```concatenate``` function, which creates a Concatenate layer and pass calls it with the given inputs\n",
    "- Create output layer with single neuron, passing it the result of concatenation\n",
    "- Finally create the model, specifying the inputs and outputs\n",
    "\n",
    "Now that's done we do exactly as above, comple, train, evaluate the model and use it to make predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we wanted to send some features through the wide path and a different subset (possibly overlapping) through the deep path (see fig on pg 310)? One solution is to use multiple inputs. For example we could send 5 features through the wide path (feats 0 to 4) and six through the deep path (feats 2 to 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_A = keras.layers.Input(shape=[5], name=\"wide_input\")\n",
    "input_B = keras.layers.Input(shape=[6], name=\"deep_input\")\n",
    "hidden1 = keras.layers.Dense(30, activation=\"relu\")(input_B)\n",
    "hidden2 = keras.layers.Dense(30, activation=\"relu\")(hidden1)\n",
    "concat = keras.layers.concatenate([input_A, hidden2])\n",
    "output = keras.layers.Dense(1, name=\"output\")(concat)\n",
    "model = keras.models.Model(inputs=[input_A, input_B], outputs=[output])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A good tip is to name the most important layers, especially as the model gets more complex. Also note that we specified ```inputs=[input_A, input_B]``` so when we're calling the fit method we need to pass a pair of matrices (X_train_A, X_train_B)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 1.7658 - val_loss: 0.8695\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.7801 - val_loss: 0.7507\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.6871 - val_loss: 0.6771\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.6407 - val_loss: 0.6491\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.6106 - val_loss: 0.6198\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5870 - val_loss: 0.5976\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5691 - val_loss: 0.5790\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.5562 - val_loss: 0.5757\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5454 - val_loss: 0.5611\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5348 - val_loss: 0.5517\n",
      "Epoch 11/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5274 - val_loss: 0.5684\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5225 - val_loss: 0.5364\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5160 - val_loss: 0.5296\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5113 - val_loss: 0.5230\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5068 - val_loss: 0.5150\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5037 - val_loss: 0.5144\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5002 - val_loss: 0.5200\n",
      "Epoch 18/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4979 - val_loss: 0.5157\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4933 - val_loss: 0.5018\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4931 - val_loss: 0.5061\n",
      "162/162 [==============================] - 0s 846us/step - loss: 0.4585\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='mse', optimizer=keras.optimizers.SGD(lr=1e-3))\n",
    "\n",
    "X_train_A, X_train_B = X_train[:, :5], X_train[:, 2:]\n",
    "X_valid_A, X_valid_B = X_valid[:, :5], X_valid[:, 2:]\n",
    "X_test_A, X_test_B = X_test[:, :5], X_test[:, 2:]\n",
    "X_new_A, X_new_B = X_test_A[:3], X_test_B[:3]\n",
    "\n",
    "if RELOAD:\n",
    "    model = keras.models.load_model(CHAPTER_DIR + 'model3.h5')\n",
    "else:    \n",
    "    history = model.fit((X_train_A, X_train_B), y_train, epochs=20,\n",
    "                         validation_data=((X_valid_A, X_valid_B), y_valid))\n",
    "    model.save(CHAPTER_DIR + 'model3.h5')\n",
    "    \n",
    "mse_test = model.evaluate((X_test_A, X_test_B), y_test)\n",
    "y_pred = model.predict((X_new_A, X_new_B))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are also cases where you may want to have multiple outputs:\n",
    "- The task may demand it; e.g. locate and classify the object in a picture. This is both regression task (finding the coordinates of the object center) and classification task\n",
    "- There may be multiple independent tasks based on the same data. For example you could perform *multitask classification* on pictures of faces to classify the person's facial expression and another to identify whether they're wearing sunglasses or not\n",
    "- Another use case is as a regularization technique. For example you may want to add some auxiliary outputs in a neural network architecture to ensure that the underlying part of the network learns something useful on its own, without relying on the rest of the network (see fig 10-16 pg 312)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To add extra outputs, simply connect them to the appropriate layers and add them to the model's list of outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continuing from the architecture above\n",
    "# ...\n",
    "output = keras.layers.Dense(1, name='main_output')(concat)\n",
    "aux_output = keras.layers.Dense(1, name='aux_output')(hidden2)\n",
    "model = keras.models.Model(inputs=[input_A, input_B], outputs=[output, aux_output])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each output needs its own loss function, when compiling the model we need to passa list of losses (or a single loss, Keras will assume that the same loss must be used for all). By default, Keras will compute all these losses and simply add them to arrive at a final loss for training. If we care more about the main output's loss, we can specify loss weights when compiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=['mse', 'mse'], loss_weights=[0.9, 0.1], optimizer='sgd')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now when training the model we also need to pass the labels to the auxiliary output. In this case they're trying to predict the same output so we just pass y_train again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.8033 - main_output_loss: 0.6813 - aux_output_loss: 1.9013 - val_loss: 0.5733 - val_main_output_loss: 0.5098 - val_aux_output_loss: 1.1440\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.5602 - main_output_loss: 0.5030 - aux_output_loss: 1.0746 - val_loss: 0.8491 - val_main_output_loss: 0.8043 - val_aux_output_loss: 1.2525\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5306 - main_output_loss: 0.4822 - aux_output_loss: 0.9659 - val_loss: 0.5141 - val_main_output_loss: 0.4727 - val_aux_output_loss: 0.8866\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.4989 - main_output_loss: 0.4592 - aux_output_loss: 0.8561 - val_loss: 0.4993 - val_main_output_loss: 0.4650 - val_aux_output_loss: 0.8077\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4901 - main_output_loss: 0.4573 - aux_output_loss: 0.7855 - val_loss: 0.4878 - val_main_output_loss: 0.4594 - val_aux_output_loss: 0.7430\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5024 - main_output_loss: 0.4763 - aux_output_loss: 0.7376 - val_loss: 0.4693 - val_main_output_loss: 0.4443 - val_aux_output_loss: 0.6937\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4629 - main_output_loss: 0.4375 - aux_output_loss: 0.6916 - val_loss: 0.4588 - val_main_output_loss: 0.4366 - val_aux_output_loss: 0.6591\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4521 - main_output_loss: 0.4293 - aux_output_loss: 0.6570 - val_loss: 0.5096 - val_main_output_loss: 0.4958 - val_aux_output_loss: 0.6341\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4630 - main_output_loss: 0.4435 - aux_output_loss: 0.6390 - val_loss: 0.4468 - val_main_output_loss: 0.4282 - val_aux_output_loss: 0.6140\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4423 - main_output_loss: 0.4226 - aux_output_loss: 0.6198 - val_loss: 0.4436 - val_main_output_loss: 0.4252 - val_aux_output_loss: 0.6091\n",
      "Epoch 11/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4264 - main_output_loss: 0.4069 - aux_output_loss: 0.6028 - val_loss: 0.4389 - val_main_output_loss: 0.4230 - val_aux_output_loss: 0.5826\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.4228 - main_output_loss: 0.4045 - aux_output_loss: 0.5872 - val_loss: 0.5110 - val_main_output_loss: 0.5010 - val_aux_output_loss: 0.6016\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.4138 - main_output_loss: 0.3961 - aux_output_loss: 0.5736 - val_loss: 0.4160 - val_main_output_loss: 0.4000 - val_aux_output_loss: 0.5605\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.4104 - main_output_loss: 0.3935 - aux_output_loss: 0.5627 - val_loss: 0.4157 - val_main_output_loss: 0.4004 - val_aux_output_loss: 0.5533\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4012 - main_output_loss: 0.3846 - aux_output_loss: 0.5508 - val_loss: 0.4026 - val_main_output_loss: 0.3876 - val_aux_output_loss: 0.5373\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3954 - main_output_loss: 0.3793 - aux_output_loss: 0.5407 - val_loss: 0.3994 - val_main_output_loss: 0.3851 - val_aux_output_loss: 0.5280\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3939 - main_output_loss: 0.3785 - aux_output_loss: 0.5327 - val_loss: 0.4037 - val_main_output_loss: 0.3897 - val_aux_output_loss: 0.5299\n",
      "Epoch 18/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3879 - main_output_loss: 0.3729 - aux_output_loss: 0.5228 - val_loss: 0.3902 - val_main_output_loss: 0.3769 - val_aux_output_loss: 0.5094\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3852 - main_output_loss: 0.3707 - aux_output_loss: 0.5154 - val_loss: 0.3865 - val_main_output_loss: 0.3735 - val_aux_output_loss: 0.5038\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3858 - main_output_loss: 0.3720 - aux_output_loss: 0.5102 - val_loss: 0.3822 - val_main_output_loss: 0.3694 - val_aux_output_loss: 0.4973\n"
     ]
    }
   ],
   "source": [
    "if RELOAD:\n",
    "    model = keras.models.load_model(CHAPTER_DIR + 'model4.h5')\n",
    "else:\n",
    "    history = model.fit([X_train_A, X_train_B], [y_train, y_train], epochs=20,\n",
    "                        validation_data=([X_valid_A, X_valid_B], [y_valid, y_valid]))\n",
    "    model.save(CHAPTER_DIR + 'model4.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we evalute the model, keras returns the total loss as well as the individual losses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162/162 [==============================] - 0s 865us/step - loss: 0.3532 - main_output_loss: 0.3376 - aux_output_loss: 0.4943\n"
     ]
    }
   ],
   "source": [
    "total_loss, main_loss, aux_loss = model.evaluate([X_test_A, X_test_B], [y_test, y_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly for the ```predict()``` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_main, y_pred_aux = model.predict([X_new_A, X_new_B])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subclassing to build Dynamic Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sequential and functional API are delcarative; you specify which layers you want to use and how are they connected. This means they're easily saved, used, cloned and shared among other advantages. The flip side is that they're static.\n",
    "\n",
    "Some models may have loops, branches, varying shapes and other dynamic behaviors. For these, you can use the subclassing API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simply sublclass the Model class and create the layers you want in the ```call()``` method. The example below builds a class for the ```WideAndDeepModel``` discussed above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WideAndDeepModel(keras.models.Model):\n",
    "    def __init__(self, units=30, activation=\"relu\", **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.hidden1 = keras.layers.Dense(units, activation=activation)\n",
    "        self.hidden2 = keras.layers.Dense(units, activation=activation)\n",
    "        self.main_output = keras.layers.Dense(1)\n",
    "        self.aux_output = keras.layers.Dense(1)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        input_A, input_B = inputs\n",
    "        hidden1 = self.hidden1(input_B)\n",
    "        hidden2 = self.hidden2(hidden1)\n",
    "        concat = keras.layers.concatenate([input_A, hidden2])\n",
    "        main_output = self.main_output(concat)\n",
    "        aux_output = self.aux_output(hidden2)\n",
    "        return main_output, aux_output\n",
    "\n",
    "model = WideAndDeepModel(30, activation=\"relu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usage is similar to the functional API, except we do not need to create the inputs. We just use the input argument to the ```call()``` method and separaate the creation of layers in the constructor from their usage in the ```call()``` method. This enables you to use loops, if statements, low-level tensorflow operations, etc...\n",
    "\n",
    "The cost is that the model architecture is hidden behind the call method. Keras cannot check types, and shapes ahead of time and it is easier to make mistakes. \n",
    "\n",
    "**Note:** Keras models can be used like regular layers so you can easily combine them to build complex architectures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and restoring a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras uses HDF5 to save the model's architecture and the values of the model parameters for every layer (weights and biases). It also saves the optimizer. You would have seen the save function scattered through this notebook and the load function as well to re-store the models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this works when using the Sequential or Functionla APIs but not when using model subclassing. You can use ```save_weights()``` and ```load_weights()``` to at least save and restore the model parameters, but you will need to save and restore everything else yourself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if training lasts several hours? Which is quite common, especially on large datasets. In this case we should not only save the final model but save at regular checkpoints during training to avoid losing everything due to a crash. \n",
    "\n",
    "The ```fit()``` method accepts a ```callbacks``` argument that lets you specify a list of objects that Keras will call at the start and end of training, at the start and end of each epoch, and even before and after processing each batch.\n",
    "\n",
    "The ```ModelCheckpoint``` callback saves checkpoints at regular intervals during training, at the end of each epoch. Moreover, if we use a validation set while training, you can set ```save_best_only=True``` when creating the model checkpoint. Then it only saves the model when its performance on the validation set is the best so far. This way we don't need to save the train the model for too long and overfitting the training set: simply restore the last model saved after training and this will be the best model on the validation set. \n",
    "The following is a simple way of implementing early stopping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 1.7886 - val_loss: 0.9643\n",
      "Epoch 2/10\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.7927 - val_loss: 0.7443\n",
      "Epoch 3/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.6983 - val_loss: 0.6865\n",
      "Epoch 4/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.6517 - val_loss: 0.6484\n",
      "Epoch 5/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.6197 - val_loss: 0.6207\n",
      "Epoch 6/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5956 - val_loss: 0.6000\n",
      "Epoch 7/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5770 - val_loss: 0.5816\n",
      "Epoch 8/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5618 - val_loss: 0.5700\n",
      "Epoch 9/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5486 - val_loss: 0.5581\n",
      "Epoch 10/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5370 - val_loss: 0.5471\n",
      "162/162 [==============================] - 0s 905us/step - loss: 0.4948\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", input_shape=[8]),\n",
    "    keras.layers.Dense(30, activation=\"relu\"),\n",
    "    keras.layers.Dense(1)\n",
    "])\n",
    "model.compile(loss=\"mse\", optimizer=keras.optimizers.SGD(lr=1e-3))\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(CHAPTER_DIR + \"my_keras_model.h5\", save_best_only=True)\n",
    "history = model.fit(X_train, y_train, epochs=10,\n",
    "                    validation_data=(X_valid, y_valid),\n",
    "                    callbacks=[checkpoint_cb])\n",
    "model = keras.models.load_model(CHAPTER_DIR + \"my_keras_model.h5\") # rollback to best model\n",
    "mse_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way of implementing early stopping is to simply use the ```EarlyStopping``` callback. It interrupts training when it measures no progress on the validation set for a number of epochs (defined by the ```patience``` arg). You can combine both callbacks to save checkpoints of your model and interrupt training early when there is no more progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5268 - val_loss: 0.5369\n",
      "Epoch 2/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5181 - val_loss: 0.5316\n",
      "Epoch 3/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5100 - val_loss: 0.5229\n",
      "Epoch 4/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5031 - val_loss: 0.5182\n",
      "Epoch 5/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4967 - val_loss: 0.5117\n",
      "Epoch 6/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4906 - val_loss: 0.5059\n",
      "Epoch 7/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4850 - val_loss: 0.4989\n",
      "Epoch 8/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4801 - val_loss: 0.4966\n",
      "Epoch 9/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4754 - val_loss: 0.4925\n",
      "Epoch 10/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4710 - val_loss: 0.4878\n",
      "Epoch 11/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4671 - val_loss: 0.4865\n",
      "Epoch 12/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4637 - val_loss: 0.4808\n",
      "Epoch 13/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4603 - val_loss: 0.4773\n",
      "Epoch 14/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4576 - val_loss: 0.4744\n",
      "Epoch 15/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4544 - val_loss: 0.4703\n",
      "Epoch 16/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4524 - val_loss: 0.4688\n",
      "Epoch 17/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4498 - val_loss: 0.4674\n",
      "Epoch 18/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4475 - val_loss: 0.4642\n",
      "Epoch 19/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4450 - val_loss: 0.4614\n",
      "Epoch 20/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4432 - val_loss: 0.4607\n",
      "Epoch 21/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4413 - val_loss: 0.4573\n",
      "Epoch 22/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4395 - val_loss: 0.4562\n",
      "Epoch 23/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4378 - val_loss: 0.4542\n",
      "Epoch 24/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4357 - val_loss: 0.4520\n",
      "Epoch 25/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4342 - val_loss: 0.4528\n",
      "Epoch 26/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4325 - val_loss: 0.4500\n",
      "Epoch 27/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4314 - val_loss: 0.4495\n",
      "Epoch 28/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4297 - val_loss: 0.4465\n",
      "Epoch 29/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4283 - val_loss: 0.4453\n",
      "Epoch 30/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4268 - val_loss: 0.4444\n",
      "Epoch 31/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4256 - val_loss: 0.4425\n",
      "Epoch 32/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4243 - val_loss: 0.4408\n",
      "Epoch 33/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4229 - val_loss: 0.4399\n",
      "Epoch 34/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4216 - val_loss: 0.4383\n",
      "Epoch 35/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4204 - val_loss: 0.4391\n",
      "Epoch 36/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4194 - val_loss: 0.4366\n",
      "Epoch 37/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4172 - val_loss: 0.4353\n",
      "Epoch 38/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4169 - val_loss: 0.4336\n",
      "Epoch 39/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4154 - val_loss: 0.4323\n",
      "Epoch 40/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4144 - val_loss: 0.4332\n",
      "Epoch 41/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4132 - val_loss: 0.4311\n",
      "Epoch 42/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4121 - val_loss: 0.4308\n",
      "Epoch 43/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4108 - val_loss: 0.4299\n",
      "Epoch 44/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4099 - val_loss: 0.4274\n",
      "Epoch 45/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4090 - val_loss: 0.4264\n",
      "Epoch 46/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4076 - val_loss: 0.4274\n",
      "Epoch 47/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4069 - val_loss: 0.4254\n",
      "Epoch 48/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4058 - val_loss: 0.4241\n",
      "Epoch 49/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4049 - val_loss: 0.4231\n",
      "Epoch 50/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4038 - val_loss: 0.4219\n",
      "Epoch 51/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4028 - val_loss: 0.4207\n",
      "Epoch 52/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.4019 - val_loss: 0.4204\n",
      "Epoch 53/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4004 - val_loss: 0.4184\n",
      "Epoch 54/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4003 - val_loss: 0.4185\n",
      "Epoch 55/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3989 - val_loss: 0.4175\n",
      "Epoch 56/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3983 - val_loss: 0.4162\n",
      "Epoch 57/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3978 - val_loss: 0.4152\n",
      "Epoch 58/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3966 - val_loss: 0.4148\n",
      "Epoch 59/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3955 - val_loss: 0.4141\n",
      "Epoch 60/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3951 - val_loss: 0.4121\n",
      "Epoch 61/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3940 - val_loss: 0.4121\n",
      "Epoch 62/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3933 - val_loss: 0.4117\n",
      "Epoch 63/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3925 - val_loss: 0.4106\n",
      "Epoch 64/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3914 - val_loss: 0.4096\n",
      "Epoch 65/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3911 - val_loss: 0.4080\n",
      "Epoch 66/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3901 - val_loss: 0.4071\n",
      "Epoch 67/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3891 - val_loss: 0.4067\n",
      "Epoch 68/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3883 - val_loss: 0.4060\n",
      "Epoch 69/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3874 - val_loss: 0.4053\n",
      "Epoch 70/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3870 - val_loss: 0.4041\n",
      "Epoch 71/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3860 - val_loss: 0.4032\n",
      "Epoch 72/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3851 - val_loss: 0.4037\n",
      "Epoch 73/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3845 - val_loss: 0.4018\n",
      "Epoch 74/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3835 - val_loss: 0.4030\n",
      "Epoch 75/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3828 - val_loss: 0.4019\n",
      "Epoch 76/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3823 - val_loss: 0.4001\n",
      "Epoch 77/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3814 - val_loss: 0.4003\n",
      "Epoch 78/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3802 - val_loss: 0.3995\n",
      "Epoch 79/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3800 - val_loss: 0.3979\n",
      "Epoch 80/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3791 - val_loss: 0.3978\n",
      "Epoch 81/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3786 - val_loss: 0.3959\n",
      "Epoch 82/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3779 - val_loss: 0.3954\n",
      "Epoch 83/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3770 - val_loss: 0.3953\n",
      "Epoch 84/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3761 - val_loss: 0.3957\n",
      "Epoch 85/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3757 - val_loss: 0.3938\n",
      "Epoch 86/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3749 - val_loss: 0.3941\n",
      "Epoch 87/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3743 - val_loss: 0.3929\n",
      "Epoch 88/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3735 - val_loss: 0.3919\n",
      "Epoch 89/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3724 - val_loss: 0.3938\n",
      "Epoch 90/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3721 - val_loss: 0.3907\n",
      "Epoch 91/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3715 - val_loss: 0.3896\n",
      "Epoch 92/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3708 - val_loss: 0.3899\n",
      "Epoch 93/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3700 - val_loss: 0.3883\n",
      "Epoch 94/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3695 - val_loss: 0.3876\n",
      "Epoch 95/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3688 - val_loss: 0.3887\n",
      "Epoch 96/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3680 - val_loss: 0.3868\n",
      "Epoch 97/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3675 - val_loss: 0.3870\n",
      "Epoch 98/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3666 - val_loss: 0.3873\n",
      "Epoch 99/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3660 - val_loss: 0.3848\n",
      "Epoch 100/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3653 - val_loss: 0.3845\n",
      "162/162 [==============================] - 0s 730us/step - loss: 0.3520\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=\"mse\", optimizer=keras.optimizers.SGD(lr=1e-3))\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n",
    "history = model.fit(X_train, y_train, epochs=100,\n",
    "                    validation_data=(X_valid, y_valid),\n",
    "                    callbacks=[checkpoint_cb, early_stopping_cb])\n",
    "mse_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This way we can also set the number of epochs to a large value since training will stop when there's no more progress."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also write your own custom callbacks. Below is an example of a callback that displays the ratio between validation loss and the training loss during training (e.g. to detect overfitting)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "336/363 [==========================>...] - ETA: 0s - loss: 0.3636\n",
      "val/train: 1.05\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3650 - val_loss: 0.3834\n"
     ]
    }
   ],
   "source": [
    "class PrintValTrainRatioCallback(keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        print(\"\\nval/train: {:.2f}\".format(logs[\"val_loss\"] / logs[\"loss\"]))\n",
    "        \n",
    "val_train_ratio_cb = PrintValTrainRatioCallback()\n",
    "history = model.fit(X_train, y_train, epochs=1,\n",
    "                    validation_data=(X_valid, y_valid),\n",
    "                    callbacks=[val_train_ratio_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also implement ```on_train_begin(), on_train_end(), on_epoch_begin(), on_epoch_end(), on_batch_begin(), on_batch_end()```. They can also be used during evaluation and predictions (e.g. for debugging), see notes on pg 316 for more on that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using TensorBoard for Vizualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorBoard is a neat viz tool that allows us to visualize training losses, learninng curves, view images generated by the model and others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To set it up we need to configure the TensorBoard server and its *event files*. In general you want to point the TensorBoard server to a root log directory and configure your program so that it writes to a different subdirectory every time it runs, this way the same server instances allows you to see and compare data from multiple runs, without getting things mixed up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "root_logdir = os.path.join(os.curdir, 'my_logs')\n",
    "\n",
    "def get_run_logdir():\n",
    "    import time\n",
    "    run_id = time.strftime(\"run_%Y_%m_%d-%H_%M_%S\")\n",
    "    return os.path.join(root_logdir, run_id)\n",
    "\n",
    "run_logdir = get_run_logdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And Keras provides a ```TensorBoard()``` callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3644 - val_loss: 0.3830\n",
      "Epoch 2/30\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3635 - val_loss: 0.3822\n",
      "Epoch 3/30\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3629 - val_loss: 0.3832\n",
      "Epoch 4/30\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3623 - val_loss: 0.3820\n",
      "Epoch 5/30\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3618 - val_loss: 0.3809\n",
      "Epoch 6/30\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3611 - val_loss: 0.3814\n",
      "Epoch 7/30\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3604 - val_loss: 0.3799\n",
      "Epoch 8/30\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3600 - val_loss: 0.3800\n",
      "Epoch 9/30\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3595 - val_loss: 0.3791\n",
      "Epoch 10/30\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3587 - val_loss: 0.3784\n",
      "Epoch 11/30\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3581 - val_loss: 0.3776\n",
      "Epoch 12/30\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3576 - val_loss: 0.3774\n",
      "Epoch 13/30\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3570 - val_loss: 0.3769\n",
      "Epoch 14/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3566 - val_loss: 0.3772\n",
      "Epoch 15/30\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3558 - val_loss: 0.3762\n",
      "Epoch 16/30\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3554 - val_loss: 0.3759\n",
      "Epoch 17/30\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3549 - val_loss: 0.3760\n",
      "Epoch 18/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3542 - val_loss: 0.3744\n",
      "Epoch 19/30\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3538 - val_loss: 0.3749\n",
      "Epoch 20/30\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3531 - val_loss: 0.3740\n",
      "Epoch 21/30\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3528 - val_loss: 0.3734\n",
      "Epoch 22/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3521 - val_loss: 0.3736\n",
      "Epoch 23/30\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3519 - val_loss: 0.3725\n",
      "Epoch 24/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3509 - val_loss: 0.3720\n",
      "Epoch 25/30\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3507 - val_loss: 0.3716\n",
      "Epoch 26/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3499 - val_loss: 0.3720\n",
      "Epoch 27/30\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3497 - val_loss: 0.3708\n",
      "Epoch 28/30\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3491 - val_loss: 0.3711\n",
      "Epoch 29/30\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3488 - val_loss: 0.3702\n",
      "Epoch 30/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3482 - val_loss: 0.3701\n"
     ]
    }
   ],
   "source": [
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\n",
    "history = model.fit(X_train, y_train, epochs=30,\n",
    "                    validation_data=(X_valid, y_valid),\n",
    "                    callbacks=[tensorboard_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we start the TensorBoard server with the following:\n",
    "\n",
    "``` tensorboard --logdir=./my_logs --port=6006 ```\n",
    "\n",
    "Then visit http://localhost:6006\n",
    "\n",
    "Note: to start tensorboard within Jupyter use the following:\n",
    "\n",
    "``` %load_ext tensorboard\n",
    "%tensorboard --logdir=./my_logs --port=6006```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, TensorFlow offers a lower-level API in the tf.summary package. You can use the following snippet to write logs to be visualized with TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_logdir = get_run_logdir()\n",
    "writer = tf.summary.create_file_writer(test_logdir)\n",
    "with writer.as_default():\n",
    "    for step in range(1, 1000 + 1):\n",
    "        tf.summary.scalar(\"my_scalar\", np.sin(step/10), step=step)\n",
    "        data = (np.random.randn(100)+2)*step / 100 # random data\n",
    "        tf.summary.histogram(\"my_hist\", data, buckets=50, step=step)\n",
    "        images = np.random.rand(2, 32, 32, 3) # Random 32x32 RGB images\n",
    "        tf.summary.image(\"my_images\", images*step/1000, step=step)\n",
    "        texts = [f\"The step is {step}. Its square is {step**2}\"]\n",
    "        tf.summary.text(\"my_text\", texts, step=step)\n",
    "        sine_wave = tf.math.sin(tf.range(12000) / 48000 * 2 * np.pi * step)\n",
    "        audio = tf.reshape(tf.cast(sine_wave, tf.float32), [1, -1, 1])\n",
    "        tf.summary.audio(\"my_audio\", audio, sample_rate=48000, step=step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning Neural Network Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One approach to tuning the many NN hyperparameters is to try many combinations and see which ones work best. We can use ```GridSearchCV``` or ```RandomSearchCV``` for this, we just have to wrap our Keras models in objects that mimic Scikit-Learn regressors. \n",
    "\n",
    "The first step is to create a function that builds and compiles the Keras model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(n_hidden=1, n_neurons=30, learning_rate=3e-3, input_shape=[8]):\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.InputLayer(input_shape=input_shape))\n",
    "    for layer in range(n_hidden):\n",
    "        model.add(keras.layers.Dense(n_neurons, activation=\"relu\"))\n",
    "    model.add(keras.layers.Dense(1))\n",
    "    optimizer = keras.optimizers.SGD(lr=learning_rate)\n",
    "    model.compile(loss=\"mse\", optimizer=optimizer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we create a KerasRegressor based on this build_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_reg = keras.wrappers.scikit_learn.KerasRegressor(build_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a thin wrapper around the Keras model built using build_model, using the pre-defined hyperparameters. Now we can use it like a regular Scikit-Learn regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 1.0945 - val_loss: 0.7070\n",
      "Epoch 2/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.6606 - val_loss: 0.6223\n",
      "Epoch 3/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5812 - val_loss: 0.5627\n",
      "Epoch 4/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.5373 - val_loss: 0.5315\n",
      "Epoch 5/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.5107 - val_loss: 0.5147\n",
      "Epoch 6/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.4934 - val_loss: 0.5013\n",
      "Epoch 7/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.4812 - val_loss: 0.4863\n",
      "Epoch 8/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4722 - val_loss: 0.4784\n",
      "Epoch 9/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4647 - val_loss: 0.4755\n",
      "Epoch 10/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.4588 - val_loss: 0.4662\n",
      "Epoch 11/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4532 - val_loss: 0.4617\n",
      "Epoch 12/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.4490 - val_loss: 0.4579\n",
      "Epoch 13/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.4454 - val_loss: 0.4545\n",
      "Epoch 14/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4421 - val_loss: 0.4521\n",
      "Epoch 15/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4390 - val_loss: 0.4485\n",
      "Epoch 16/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4364 - val_loss: 0.4466\n",
      "Epoch 17/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.4337 - val_loss: 0.4442\n",
      "Epoch 18/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.4302 - val_loss: 0.4411\n",
      "Epoch 19/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4280 - val_loss: 0.4390\n",
      "Epoch 20/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.4279 - val_loss: 0.4375\n",
      "Epoch 21/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4237 - val_loss: 0.4353\n",
      "Epoch 22/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4210 - val_loss: 0.4343\n",
      "Epoch 23/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4191 - val_loss: 0.4320\n",
      "Epoch 24/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4169 - val_loss: 0.4292\n",
      "Epoch 25/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.4151 - val_loss: 0.4275\n",
      "Epoch 26/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.4128 - val_loss: 0.4287\n",
      "Epoch 27/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4115 - val_loss: 0.4256\n",
      "Epoch 28/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4094 - val_loss: 0.4249\n",
      "Epoch 29/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.4081 - val_loss: 0.4214\n",
      "Epoch 30/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.4060 - val_loss: 0.4225\n",
      "Epoch 31/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.4051 - val_loss: 0.4189\n",
      "Epoch 32/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.4037 - val_loss: 0.4173\n",
      "Epoch 33/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4018 - val_loss: 0.4169\n",
      "Epoch 34/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4005 - val_loss: 0.4170\n",
      "Epoch 35/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3994 - val_loss: 0.4176\n",
      "Epoch 36/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3984 - val_loss: 0.4152\n",
      "Epoch 37/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3965 - val_loss: 0.4129\n",
      "Epoch 38/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3960 - val_loss: 0.4106\n",
      "Epoch 39/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3945 - val_loss: 0.4117\n",
      "Epoch 40/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3948 - val_loss: 0.4105\n",
      "Epoch 41/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3925 - val_loss: 0.4074\n",
      "Epoch 42/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3914 - val_loss: 0.4114\n",
      "Epoch 43/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3904 - val_loss: 0.4059\n",
      "Epoch 44/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3895 - val_loss: 0.4046\n",
      "Epoch 45/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3885 - val_loss: 0.4058\n",
      "Epoch 46/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3870 - val_loss: 0.4103\n",
      "Epoch 47/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3868 - val_loss: 0.4047\n",
      "Epoch 48/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3852 - val_loss: 0.4031\n",
      "Epoch 49/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3848 - val_loss: 0.4050\n",
      "Epoch 50/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3839 - val_loss: 0.4033\n",
      "Epoch 51/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3831 - val_loss: 0.4006\n",
      "Epoch 52/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3825 - val_loss: 0.3996\n",
      "Epoch 53/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3824 - val_loss: 0.3981\n",
      "Epoch 54/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3817 - val_loss: 0.4027\n",
      "Epoch 55/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3799 - val_loss: 0.3973\n",
      "Epoch 56/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3795 - val_loss: 0.3970\n",
      "Epoch 57/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3802 - val_loss: 0.3973\n",
      "Epoch 58/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3783 - val_loss: 0.3958\n",
      "Epoch 59/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3801 - val_loss: 0.3942\n",
      "Epoch 60/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3773 - val_loss: 0.3941\n",
      "Epoch 61/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3760 - val_loss: 0.3957\n",
      "Epoch 62/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3781 - val_loss: 0.3944\n",
      "Epoch 63/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3750 - val_loss: 0.3941\n",
      "Epoch 64/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3749 - val_loss: 0.3917\n",
      "Epoch 65/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3744 - val_loss: 0.3911\n",
      "Epoch 66/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3735 - val_loss: 0.3910\n",
      "Epoch 67/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3735 - val_loss: 0.3890\n",
      "Epoch 68/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3722 - val_loss: 0.3929\n",
      "Epoch 69/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3718 - val_loss: 0.3902\n",
      "Epoch 70/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3717 - val_loss: 0.3889\n",
      "Epoch 71/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3741 - val_loss: 0.3879\n",
      "Epoch 72/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3713 - val_loss: 0.3893\n",
      "Epoch 73/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3702 - val_loss: 0.3865\n",
      "Epoch 74/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3702 - val_loss: 0.3879\n",
      "Epoch 75/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3688 - val_loss: 0.3878\n",
      "Epoch 76/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3688 - val_loss: 0.3881\n",
      "Epoch 77/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3682 - val_loss: 0.3888\n",
      "Epoch 78/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3718 - val_loss: 0.3845\n",
      "Epoch 79/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3676 - val_loss: 0.3841\n",
      "Epoch 80/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3670 - val_loss: 0.3851\n",
      "Epoch 81/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3676 - val_loss: 0.3841\n",
      "Epoch 82/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3670 - val_loss: 0.3825\n",
      "Epoch 83/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3661 - val_loss: 0.3845\n",
      "Epoch 84/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3649 - val_loss: 0.3835\n",
      "Epoch 85/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3648 - val_loss: 0.3819\n",
      "Epoch 86/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3647 - val_loss: 0.3837\n",
      "Epoch 87/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3645 - val_loss: 0.3926\n",
      "Epoch 88/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3641 - val_loss: 0.3804\n",
      "Epoch 89/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3635 - val_loss: 0.3854\n",
      "Epoch 90/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3631 - val_loss: 0.3810\n",
      "Epoch 91/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3633 - val_loss: 0.3778\n",
      "Epoch 92/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3624 - val_loss: 0.3815\n",
      "Epoch 93/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3636 - val_loss: 0.3769\n",
      "Epoch 94/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3620 - val_loss: 0.3777\n",
      "Epoch 95/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3617 - val_loss: 0.3798\n",
      "Epoch 96/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3608 - val_loss: 0.3770\n",
      "Epoch 97/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3609 - val_loss: 0.3772\n",
      "Epoch 98/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3603 - val_loss: 0.3773\n",
      "Epoch 99/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3612 - val_loss: 0.3792\n",
      "Epoch 100/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3598 - val_loss: 0.3750\n",
      "162/162 [==============================] - 0s 750us/step - loss: 0.3450\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-0.3450339734554291"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras_reg.fit(X_train, y_train, epochs=100,\n",
    "             validation_data=(X_valid, y_valid),\n",
    "             callbacks=[keras.callbacks.EarlyStopping(patience=10), \n",
    "                        tensorboard_cb])\n",
    "mse_test = keras_reg.score(X_test, y_test)\n",
    "y_pred = keras_reg.predict(X_new)\n",
    "mse_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any ```**kwargs``` passed to the fit method are passed to the underlying Keras model. Note the score will be the opposite of the MSE because Scikit-Learn wants scores, not losses (i.e. higher is better)\n",
    "\n",
    "Now we can use ```RandomizedSearchCV``` to explore the number of hidden layers, neurons and learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 1.8607 - val_loss: 0.6564\n",
      "Epoch 2/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5778 - val_loss: 0.5728\n",
      "Epoch 3/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5520 - val_loss: 0.5640\n",
      "Epoch 4/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5459 - val_loss: 0.5586\n",
      "Epoch 5/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5400 - val_loss: 0.5555\n",
      "Epoch 6/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5367 - val_loss: 0.5634\n",
      "Epoch 7/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5332 - val_loss: 0.6019\n",
      "Epoch 8/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5352 - val_loss: 0.5577\n",
      "Epoch 9/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5314 - val_loss: 0.5485\n",
      "Epoch 10/50\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5279 - val_loss: 0.6002\n",
      "Epoch 11/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5319 - val_loss: 0.5522\n",
      "Epoch 12/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5303 - val_loss: 0.5473\n",
      "Epoch 13/50\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5335 - val_loss: 0.5584\n",
      "Epoch 14/50\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5266 - val_loss: 0.5471\n",
      "Epoch 15/50\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5292 - val_loss: 0.5513\n",
      "Epoch 16/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5298 - val_loss: 0.5564\n",
      "Epoch 17/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5291 - val_loss: 0.5464\n",
      "Epoch 18/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5310 - val_loss: 0.5486\n",
      "Epoch 19/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5293 - val_loss: 0.5745\n",
      "Epoch 20/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5307 - val_loss: 0.5506\n",
      "Epoch 21/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5309 - val_loss: 0.5539\n",
      "Epoch 22/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5271 - val_loss: 0.5977\n",
      "Epoch 23/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5260 - val_loss: 0.5494\n",
      "Epoch 24/50\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5266 - val_loss: 0.6060\n",
      "Epoch 25/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5314 - val_loss: 0.5770\n",
      "Epoch 26/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5305 - val_loss: 0.5692\n",
      "Epoch 27/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5290 - val_loss: 0.5689\n",
      "121/121 [==============================] - 0s 785us/step - loss: 0.5329\n",
      "Epoch 1/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 2.3193 - val_loss: 0.6611\n",
      "Epoch 2/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6620 - val_loss: 0.5882\n",
      "Epoch 3/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6264 - val_loss: 0.5669\n",
      "Epoch 4/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5597 - val_loss: 0.5644\n",
      "Epoch 5/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5471 - val_loss: 0.5551\n",
      "Epoch 6/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5414 - val_loss: 0.5608\n",
      "Epoch 7/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5346 - val_loss: 0.5854\n",
      "Epoch 8/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5339 - val_loss: 0.5567\n",
      "Epoch 9/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5306 - val_loss: 0.5520\n",
      "Epoch 10/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5278 - val_loss: 0.5945\n",
      "Epoch 11/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5285 - val_loss: 0.5512\n",
      "Epoch 12/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5294 - val_loss: 0.5531\n",
      "Epoch 13/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5281 - val_loss: 0.5562\n",
      "Epoch 14/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5257 - val_loss: 0.5503\n",
      "Epoch 15/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5263 - val_loss: 0.5481\n",
      "Epoch 16/50\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5275 - val_loss: 0.5596\n",
      "Epoch 17/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5257 - val_loss: 0.5478\n",
      "Epoch 18/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5267 - val_loss: 0.5507\n",
      "Epoch 19/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5274 - val_loss: 0.5598\n",
      "Epoch 20/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5291 - val_loss: 0.5601\n",
      "Epoch 21/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5284 - val_loss: 0.5637\n",
      "Epoch 22/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5249 - val_loss: 0.5882\n",
      "Epoch 23/50\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5246 - val_loss: 0.5581\n",
      "Epoch 24/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5244 - val_loss: 0.5953\n",
      "Epoch 25/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5288 - val_loss: 0.5763\n",
      "Epoch 26/50\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5281 - val_loss: 0.5765\n",
      "Epoch 27/50\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5292 - val_loss: 0.5634\n",
      "121/121 [==============================] - 0s 888us/step - loss: 0.5389\n",
      "Epoch 1/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 1.9772 - val_loss: 0.6033\n",
      "Epoch 2/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5777 - val_loss: 0.5571\n",
      "Epoch 3/50\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5478 - val_loss: 0.5556\n",
      "Epoch 4/50\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5401 - val_loss: 0.5570\n",
      "Epoch 5/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5418 - val_loss: 0.5555\n",
      "Epoch 6/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5382 - val_loss: 0.5579\n",
      "Epoch 7/50\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5374 - val_loss: 0.5526\n",
      "Epoch 8/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5369 - val_loss: 0.5544\n",
      "Epoch 9/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5372 - val_loss: 0.5555\n",
      "Epoch 10/50\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5368 - val_loss: 0.5526\n",
      "Epoch 11/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5367 - val_loss: 0.5587\n",
      "Epoch 12/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5358 - val_loss: 0.5584\n",
      "Epoch 13/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5358 - val_loss: 0.5584\n",
      "Epoch 14/50\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5360 - val_loss: 0.5599\n",
      "Epoch 15/50\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5354 - val_loss: 0.5515\n",
      "Epoch 16/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5369 - val_loss: 0.5554\n",
      "Epoch 17/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5358 - val_loss: 0.5559\n",
      "Epoch 18/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5362 - val_loss: 0.5551\n",
      "Epoch 19/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5352 - val_loss: 0.5502\n",
      "Epoch 20/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5356 - val_loss: 0.5547\n",
      "Epoch 21/50\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5359 - val_loss: 0.5554\n",
      "Epoch 22/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5357 - val_loss: 0.5558\n",
      "Epoch 23/50\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5357 - val_loss: 0.5620\n",
      "Epoch 24/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5358 - val_loss: 0.5555\n",
      "Epoch 25/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5351 - val_loss: 0.5579\n",
      "Epoch 26/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5354 - val_loss: 0.5572\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5353 - val_loss: 0.5589\n",
      "Epoch 28/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5362 - val_loss: 0.5552\n",
      "Epoch 29/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5352 - val_loss: 0.5591\n",
      "121/121 [==============================] - 0s 786us/step - loss: 0.5140\n",
      "Epoch 1/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 4.7675 - val_loss: 3.6583\n",
      "Epoch 2/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 3.1342 - val_loss: 2.4915\n",
      "Epoch 3/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 2.1928 - val_loss: 1.8176\n",
      "Epoch 4/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 1.6419 - val_loss: 1.4220\n",
      "Epoch 5/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 1.3145 - val_loss: 1.1850\n",
      "Epoch 6/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 1.1162 - val_loss: 1.0403\n",
      "Epoch 7/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.9937 - val_loss: 0.9495\n",
      "Epoch 8/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.9162 - val_loss: 0.8921\n",
      "Epoch 9/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.8653 - val_loss: 0.8539\n",
      "Epoch 10/50\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8304 - val_loss: 0.8256\n",
      "Epoch 11/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.8051 - val_loss: 0.8053\n",
      "Epoch 12/50\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7858 - val_loss: 0.7890\n",
      "Epoch 13/50\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7702 - val_loss: 0.7751\n",
      "Epoch 14/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.7571 - val_loss: 0.7638\n",
      "Epoch 15/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.7456 - val_loss: 0.7532\n",
      "Epoch 16/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.7352 - val_loss: 0.7437\n",
      "Epoch 17/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.7258 - val_loss: 0.7344\n",
      "Epoch 18/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.7169 - val_loss: 0.7257\n",
      "Epoch 19/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.7085 - val_loss: 0.7179\n",
      "Epoch 20/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.7007 - val_loss: 0.7103\n",
      "Epoch 21/50\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6933 - val_loss: 0.7029\n",
      "Epoch 22/50\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6861 - val_loss: 0.6958\n",
      "Epoch 23/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6793 - val_loss: 0.6893\n",
      "Epoch 24/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6728 - val_loss: 0.6828\n",
      "Epoch 25/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6667 - val_loss: 0.6768\n",
      "Epoch 26/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6608 - val_loss: 0.6711\n",
      "Epoch 27/50\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6551 - val_loss: 0.6657\n",
      "Epoch 28/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6496 - val_loss: 0.6605\n",
      "Epoch 29/50\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6445 - val_loss: 0.6555\n",
      "Epoch 30/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6396 - val_loss: 0.6508\n",
      "Epoch 31/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6348 - val_loss: 0.6462\n",
      "Epoch 32/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6302 - val_loss: 0.6419\n",
      "Epoch 33/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6259 - val_loss: 0.6378\n",
      "Epoch 34/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6218 - val_loss: 0.6339\n",
      "Epoch 35/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6177 - val_loss: 0.6300\n",
      "Epoch 36/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6139 - val_loss: 0.6263\n",
      "Epoch 37/50\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6103 - val_loss: 0.6230\n",
      "Epoch 38/50\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6068 - val_loss: 0.6199\n",
      "Epoch 39/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6034 - val_loss: 0.6167\n",
      "Epoch 40/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6002 - val_loss: 0.6136\n",
      "Epoch 41/50\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5971 - val_loss: 0.6107\n",
      "Epoch 42/50\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5942 - val_loss: 0.6082\n",
      "Epoch 43/50\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5913 - val_loss: 0.6057\n",
      "Epoch 44/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5887 - val_loss: 0.6033\n",
      "Epoch 45/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5861 - val_loss: 0.6008\n",
      "Epoch 46/50\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5835 - val_loss: 0.5987\n",
      "Epoch 47/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5812 - val_loss: 0.5964\n",
      "Epoch 48/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5789 - val_loss: 0.5949\n",
      "Epoch 49/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5767 - val_loss: 0.5926\n",
      "Epoch 50/50\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5746 - val_loss: 0.5909\n",
      "121/121 [==============================] - 0s 852us/step - loss: 0.5920\n",
      "Epoch 1/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 6.3388 - val_loss: 4.4525\n",
      "Epoch 2/50\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.8089 - val_loss: 2.8084\n",
      "Epoch 3/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 2.4643 - val_loss: 1.9161\n",
      "Epoch 4/50\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.7219 - val_loss: 1.4174\n",
      "Epoch 5/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 1.3038 - val_loss: 1.1329\n",
      "Epoch 6/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 1.0630 - val_loss: 0.9680\n",
      "Epoch 7/50\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.9220 - val_loss: 0.8705\n",
      "Epoch 8/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.8374 - val_loss: 0.8120\n",
      "Epoch 9/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.7854 - val_loss: 0.7757\n",
      "Epoch 10/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.7522 - val_loss: 0.7516\n",
      "Epoch 11/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.7298 - val_loss: 0.7351\n",
      "Epoch 12/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.7139 - val_loss: 0.7226\n",
      "Epoch 13/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.7017 - val_loss: 0.7125\n",
      "Epoch 14/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6918 - val_loss: 0.7044\n",
      "Epoch 15/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6835 - val_loss: 0.6969\n",
      "Epoch 16/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6761 - val_loss: 0.6901\n",
      "Epoch 17/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6695 - val_loss: 0.6836\n",
      "Epoch 18/50\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6632 - val_loss: 0.6775\n",
      "Epoch 19/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6573 - val_loss: 0.6718\n",
      "Epoch 20/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6517 - val_loss: 0.6664\n",
      "Epoch 21/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6465 - val_loss: 0.6611\n",
      "Epoch 22/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6414 - val_loss: 0.6561\n",
      "Epoch 23/50\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6365 - val_loss: 0.6513\n",
      "Epoch 24/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6319 - val_loss: 0.6467\n",
      "Epoch 25/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6275 - val_loss: 0.6424\n",
      "Epoch 26/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6233 - val_loss: 0.6383\n",
      "Epoch 27/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6192 - val_loss: 0.6343\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/50\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6152 - val_loss: 0.6304\n",
      "Epoch 29/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6114 - val_loss: 0.6268\n",
      "Epoch 30/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6080 - val_loss: 0.6234\n",
      "Epoch 31/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6045 - val_loss: 0.6200\n",
      "Epoch 32/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6012 - val_loss: 0.6170\n",
      "Epoch 33/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5980 - val_loss: 0.6142\n",
      "Epoch 34/50\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5950 - val_loss: 0.6115\n",
      "Epoch 35/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5920 - val_loss: 0.6085\n",
      "Epoch 36/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5892 - val_loss: 0.6057\n",
      "Epoch 37/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5866 - val_loss: 0.6035\n",
      "Epoch 38/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5840 - val_loss: 0.6015\n",
      "Epoch 39/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5815 - val_loss: 0.5991\n",
      "Epoch 40/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5792 - val_loss: 0.5969\n",
      "Epoch 41/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5769 - val_loss: 0.5948\n",
      "Epoch 42/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5747 - val_loss: 0.5932\n",
      "Epoch 43/50\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5726 - val_loss: 0.5917\n",
      "Epoch 44/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5707 - val_loss: 0.5898\n",
      "Epoch 45/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5687 - val_loss: 0.5879\n",
      "Epoch 46/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5669 - val_loss: 0.5866\n",
      "Epoch 47/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5651 - val_loss: 0.5850\n",
      "Epoch 48/50\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5634 - val_loss: 0.5841\n",
      "Epoch 49/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5618 - val_loss: 0.5824\n",
      "Epoch 50/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5602 - val_loss: 0.5812\n",
      "121/121 [==============================] - 0s 769us/step - loss: 0.5702\n",
      "Epoch 1/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 5.7353 - val_loss: 3.8480\n",
      "Epoch 2/50\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.3892 - val_loss: 2.4605\n",
      "Epoch 3/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 2.1500 - val_loss: 1.6735\n",
      "Epoch 4/50\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.4701 - val_loss: 1.2205\n",
      "Epoch 5/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 1.0907 - val_loss: 0.9560\n",
      "Epoch 6/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.8738 - val_loss: 0.8002\n",
      "Epoch 7/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.7478 - val_loss: 0.7076\n",
      "Epoch 8/50\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6735 - val_loss: 0.6521\n",
      "Epoch 9/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6293 - val_loss: 0.6185\n",
      "Epoch 10/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6025 - val_loss: 0.5981\n",
      "Epoch 11/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5860 - val_loss: 0.5855\n",
      "Epoch 12/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5756 - val_loss: 0.5775\n",
      "Epoch 13/50\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5689 - val_loss: 0.5724\n",
      "Epoch 14/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5643 - val_loss: 0.5690\n",
      "Epoch 15/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5611 - val_loss: 0.5666\n",
      "Epoch 16/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5588 - val_loss: 0.5649\n",
      "Epoch 17/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5570 - val_loss: 0.5636\n",
      "Epoch 18/50\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5556 - val_loss: 0.5626\n",
      "Epoch 19/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5544 - val_loss: 0.5618\n",
      "Epoch 20/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5534 - val_loss: 0.5611\n",
      "Epoch 21/50\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5525 - val_loss: 0.5605\n",
      "Epoch 22/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5516 - val_loss: 0.5600\n",
      "Epoch 23/50\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5509 - val_loss: 0.5596\n",
      "Epoch 24/50\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5501 - val_loss: 0.5591\n",
      "Epoch 25/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5495 - val_loss: 0.5587\n",
      "Epoch 26/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5488 - val_loss: 0.5583\n",
      "Epoch 27/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5482 - val_loss: 0.5579\n",
      "Epoch 28/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5477 - val_loss: 0.5575\n",
      "Epoch 29/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5471 - val_loss: 0.5573\n",
      "Epoch 30/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5466 - val_loss: 0.5568\n",
      "Epoch 31/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5461 - val_loss: 0.5565\n",
      "Epoch 32/50\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5457 - val_loss: 0.5563\n",
      "Epoch 33/50\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5452 - val_loss: 0.5560\n",
      "Epoch 34/50\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5447 - val_loss: 0.5557\n",
      "Epoch 35/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5443 - val_loss: 0.5556\n",
      "Epoch 36/50\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5439 - val_loss: 0.5554\n",
      "Epoch 37/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5435 - val_loss: 0.5552\n",
      "Epoch 38/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5432 - val_loss: 0.5550\n",
      "Epoch 39/50\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5428 - val_loss: 0.5548\n",
      "Epoch 40/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5424 - val_loss: 0.5546\n",
      "Epoch 41/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5421 - val_loss: 0.5545\n",
      "Epoch 42/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5418 - val_loss: 0.5543\n",
      "Epoch 43/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5415 - val_loss: 0.5542\n",
      "Epoch 44/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5412 - val_loss: 0.5541\n",
      "Epoch 45/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5409 - val_loss: 0.5540\n",
      "Epoch 46/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5407 - val_loss: 0.5539\n",
      "Epoch 47/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5404 - val_loss: 0.5538\n",
      "Epoch 48/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5402 - val_loss: 0.5536\n",
      "Epoch 49/50\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5399 - val_loss: 0.5535\n",
      "Epoch 50/50\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5397 - val_loss: 0.5534\n",
      "121/121 [==============================] - 0s 780us/step - loss: 0.5208\n",
      "Epoch 1/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 2.6860 - val_loss: 1.0965\n",
      "Epoch 2/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.8417 - val_loss: 0.7474\n",
      "Epoch 3/50\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6951 - val_loss: 0.6870\n",
      "Epoch 4/50\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6547 - val_loss: 0.6580\n",
      "Epoch 5/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6278 - val_loss: 0.6361\n",
      "Epoch 6/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6051 - val_loss: 0.6185\n",
      "Epoch 7/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5861 - val_loss: 0.6043\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5699 - val_loss: 0.5874\n",
      "Epoch 9/50\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5559 - val_loss: 0.5738\n",
      "Epoch 10/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5439 - val_loss: 0.5670\n",
      "Epoch 11/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5341 - val_loss: 0.5550\n",
      "Epoch 12/50\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5250 - val_loss: 0.5461\n",
      "Epoch 13/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5177 - val_loss: 0.5399\n",
      "Epoch 14/50\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5108 - val_loss: 0.5329\n",
      "Epoch 15/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5053 - val_loss: 0.5276\n",
      "Epoch 16/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5003 - val_loss: 0.5241\n",
      "Epoch 17/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4958 - val_loss: 0.5188\n",
      "Epoch 18/50\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4916 - val_loss: 0.5144\n",
      "Epoch 19/50\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4876 - val_loss: 0.5125\n",
      "Epoch 20/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4846 - val_loss: 0.5072\n",
      "Epoch 21/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4817 - val_loss: 0.5048\n",
      "Epoch 22/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4788 - val_loss: 0.5039\n",
      "Epoch 23/50\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4763 - val_loss: 0.4994\n",
      "Epoch 24/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4738 - val_loss: 0.4994\n",
      "Epoch 25/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4717 - val_loss: 0.4961\n",
      "Epoch 26/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4701 - val_loss: 0.4938\n",
      "Epoch 27/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4679 - val_loss: 0.4930\n",
      "Epoch 28/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4662 - val_loss: 0.4900\n",
      "Epoch 29/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4646 - val_loss: 0.4887\n",
      "Epoch 30/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4629 - val_loss: 0.4876\n",
      "Epoch 31/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4613 - val_loss: 0.4848\n",
      "Epoch 32/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4600 - val_loss: 0.4832\n",
      "Epoch 33/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4586 - val_loss: 0.4836\n",
      "Epoch 34/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4573 - val_loss: 0.4824\n",
      "Epoch 35/50\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4559 - val_loss: 0.4799\n",
      "Epoch 36/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4549 - val_loss: 0.4779\n",
      "Epoch 37/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4537 - val_loss: 0.4789\n",
      "Epoch 38/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4525 - val_loss: 0.4777\n",
      "Epoch 39/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4515 - val_loss: 0.4756\n",
      "Epoch 40/50\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4505 - val_loss: 0.4745\n",
      "Epoch 41/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4494 - val_loss: 0.4728\n",
      "Epoch 42/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4485 - val_loss: 0.4722\n",
      "Epoch 43/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4475 - val_loss: 0.4719\n",
      "Epoch 44/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4466 - val_loss: 0.4707\n",
      "Epoch 45/50\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4458 - val_loss: 0.4691\n",
      "Epoch 46/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4448 - val_loss: 0.4689\n",
      "Epoch 47/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4441 - val_loss: 0.4677\n",
      "Epoch 48/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4431 - val_loss: 0.4672\n",
      "Epoch 49/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4423 - val_loss: 0.4655\n",
      "Epoch 50/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4414 - val_loss: 0.4658\n",
      "121/121 [==============================] - 0s 773us/step - loss: 0.4588\n",
      "Epoch 1/50\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 2.4785 - val_loss: 1.3881\n",
      "Epoch 2/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 1.0908 - val_loss: 0.9280\n",
      "Epoch 3/50\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8520 - val_loss: 0.8213\n",
      "Epoch 4/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.7873 - val_loss: 0.7786\n",
      "Epoch 5/50\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7518 - val_loss: 0.7487\n",
      "Epoch 6/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.7225 - val_loss: 0.7245\n",
      "Epoch 7/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6969 - val_loss: 0.7041\n",
      "Epoch 8/50\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6743 - val_loss: 0.6828\n",
      "Epoch 9/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6536 - val_loss: 0.6633\n",
      "Epoch 10/50\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6346 - val_loss: 0.6492\n",
      "Epoch 11/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6176 - val_loss: 0.6314\n",
      "Epoch 12/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6024 - val_loss: 0.6177\n",
      "Epoch 13/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5880 - val_loss: 0.6054\n",
      "Epoch 14/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5750 - val_loss: 0.5930\n",
      "Epoch 15/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5640 - val_loss: 0.5822\n",
      "Epoch 16/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5537 - val_loss: 0.5739\n",
      "Epoch 17/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5445 - val_loss: 0.5649\n",
      "Epoch 18/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5362 - val_loss: 0.5576\n",
      "Epoch 19/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5287 - val_loss: 0.5507\n",
      "Epoch 20/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5223 - val_loss: 0.5452\n",
      "Epoch 21/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5162 - val_loss: 0.5399\n",
      "Epoch 22/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5106 - val_loss: 0.5364\n",
      "Epoch 23/50\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5055 - val_loss: 0.5295\n",
      "Epoch 24/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5010 - val_loss: 0.5269\n",
      "Epoch 25/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4971 - val_loss: 0.5221\n",
      "Epoch 26/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4933 - val_loss: 0.5189\n",
      "Epoch 27/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4898 - val_loss: 0.5152\n",
      "Epoch 28/50\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4864 - val_loss: 0.5110\n",
      "Epoch 29/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4833 - val_loss: 0.5088\n",
      "Epoch 30/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4804 - val_loss: 0.5057\n",
      "Epoch 31/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4779 - val_loss: 0.5025\n",
      "Epoch 32/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4753 - val_loss: 0.5006\n",
      "Epoch 33/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4729 - val_loss: 0.4998\n",
      "Epoch 34/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4708 - val_loss: 0.4973\n",
      "Epoch 35/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4686 - val_loss: 0.4935\n",
      "Epoch 36/50\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4669 - val_loss: 0.4913\n",
      "Epoch 37/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4650 - val_loss: 0.4914\n",
      "Epoch 38/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4632 - val_loss: 0.4902\n",
      "Epoch 39/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4617 - val_loss: 0.4871\n",
      "Epoch 40/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4601 - val_loss: 0.4862\n",
      "Epoch 41/50\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4586 - val_loss: 0.4829\n",
      "Epoch 42/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4571 - val_loss: 0.4819\n",
      "Epoch 43/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4557 - val_loss: 0.4818\n",
      "Epoch 44/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4544 - val_loss: 0.4786\n",
      "Epoch 45/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4531 - val_loss: 0.4772\n",
      "Epoch 46/50\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4517 - val_loss: 0.4771\n",
      "Epoch 47/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4507 - val_loss: 0.4748\n",
      "Epoch 48/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4496 - val_loss: 0.4744\n",
      "Epoch 49/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4485 - val_loss: 0.4725\n",
      "Epoch 50/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4475 - val_loss: 0.4718\n",
      "121/121 [==============================] - 0s 813us/step - loss: 0.4695\n",
      "Epoch 1/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 2.7284 - val_loss: 1.4721\n",
      "Epoch 2/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 1.0521 - val_loss: 0.9562\n",
      "Epoch 3/50\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8331 - val_loss: 0.7990\n",
      "Epoch 4/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.7674 - val_loss: 0.7365\n",
      "Epoch 5/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.7324 - val_loss: 0.7058\n",
      "Epoch 6/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.7062 - val_loss: 0.6845\n",
      "Epoch 7/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6842 - val_loss: 0.6653\n",
      "Epoch 8/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6645 - val_loss: 0.6497\n",
      "Epoch 9/50\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6469 - val_loss: 0.6337\n",
      "Epoch 10/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6310 - val_loss: 0.6198\n",
      "Epoch 11/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6164 - val_loss: 0.6074\n",
      "Epoch 12/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6033 - val_loss: 0.5962\n",
      "Epoch 13/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5913 - val_loss: 0.5855\n",
      "Epoch 14/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5804 - val_loss: 0.5758\n",
      "Epoch 15/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5704 - val_loss: 0.5664\n",
      "Epoch 16/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5617 - val_loss: 0.5592\n",
      "Epoch 17/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5533 - val_loss: 0.5521\n",
      "Epoch 18/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5460 - val_loss: 0.5452\n",
      "Epoch 19/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5397 - val_loss: 0.5393\n",
      "Epoch 20/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5337 - val_loss: 0.5357\n",
      "Epoch 21/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5287 - val_loss: 0.5300\n",
      "Epoch 22/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5238 - val_loss: 0.5262\n",
      "Epoch 23/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5197 - val_loss: 0.5229\n",
      "Epoch 24/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5158 - val_loss: 0.5198\n",
      "Epoch 25/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5124 - val_loss: 0.5175\n",
      "Epoch 26/50\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5092 - val_loss: 0.5150\n",
      "Epoch 27/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5062 - val_loss: 0.5130\n",
      "Epoch 28/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5037 - val_loss: 0.5099\n",
      "Epoch 29/50\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5011 - val_loss: 0.5092\n",
      "Epoch 30/50\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.4988 - val_loss: 0.5056\n",
      "Epoch 31/50\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.4966 - val_loss: 0.5042\n",
      "Epoch 32/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4948 - val_loss: 0.5028\n",
      "Epoch 33/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4928 - val_loss: 0.5016\n",
      "Epoch 34/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4911 - val_loss: 0.4999\n",
      "Epoch 35/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4894 - val_loss: 0.4983\n",
      "Epoch 36/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4878 - val_loss: 0.4972\n",
      "Epoch 37/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4862 - val_loss: 0.4959\n",
      "Epoch 38/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4849 - val_loss: 0.4952\n",
      "Epoch 39/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4834 - val_loss: 0.4931\n",
      "Epoch 40/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4820 - val_loss: 0.4921\n",
      "Epoch 41/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4807 - val_loss: 0.4905\n",
      "Epoch 42/50\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4795 - val_loss: 0.4888\n",
      "Epoch 43/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4783 - val_loss: 0.4883\n",
      "Epoch 44/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4772 - val_loss: 0.4871\n",
      "Epoch 45/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4759 - val_loss: 0.4874\n",
      "Epoch 46/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4749 - val_loss: 0.4854\n",
      "Epoch 47/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4738 - val_loss: 0.4842\n",
      "Epoch 48/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4727 - val_loss: 0.4826\n",
      "Epoch 49/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4717 - val_loss: 0.4817\n",
      "Epoch 50/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4707 - val_loss: 0.4809\n",
      "121/121 [==============================] - 0s 694us/step - loss: 0.4411\n",
      "Epoch 1/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 1.1653 - val_loss: 0.6169\n",
      "Epoch 2/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5983 - val_loss: 0.5866\n",
      "Epoch 3/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5812 - val_loss: 0.5779\n",
      "Epoch 4/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5811 - val_loss: 0.5617\n",
      "Epoch 5/50\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5450 - val_loss: 0.5577\n",
      "Epoch 6/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5426 - val_loss: 0.5660\n",
      "Epoch 7/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5327 - val_loss: 0.8921\n",
      "Epoch 8/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5589 - val_loss: 0.5626\n",
      "Epoch 9/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5410 - val_loss: 0.5467\n",
      "Epoch 10/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5282 - val_loss: 0.7776\n",
      "Epoch 11/50\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5459 - val_loss: 0.5533\n",
      "Epoch 12/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5387 - val_loss: 0.5456\n",
      "Epoch 13/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5542 - val_loss: 0.5460\n",
      "Epoch 14/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5318 - val_loss: 0.5483\n",
      "Epoch 15/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5311 - val_loss: 0.5578\n",
      "Epoch 16/50\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5354 - val_loss: 0.5557\n",
      "Epoch 17/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5343 - val_loss: 0.5619\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5434 - val_loss: 0.5490\n",
      "Epoch 19/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5351 - val_loss: 0.5737\n",
      "Epoch 20/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5614 - val_loss: 0.5502\n",
      "Epoch 21/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5413 - val_loss: 0.5521\n",
      "Epoch 22/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5306 - val_loss: 0.6942\n",
      "121/121 [==============================] - 0s 785us/step - loss: 0.5575\n",
      "Epoch 1/50\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 1.3048 - val_loss: 0.5862\n",
      "Epoch 2/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5655 - val_loss: 0.5636\n",
      "Epoch 3/50\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5575 - val_loss: 0.5539\n",
      "Epoch 4/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5537 - val_loss: 0.5525\n",
      "Epoch 5/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5853 - val_loss: 0.5485\n",
      "Epoch 6/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5971 - val_loss: 0.5618\n",
      "Epoch 7/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5333 - val_loss: 0.8544\n",
      "Epoch 8/50\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5747 - val_loss: 0.5550\n",
      "Epoch 9/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 1.1235 - val_loss: 0.5547\n",
      "Epoch 10/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5752 - val_loss: 0.8116\n",
      "Epoch 11/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5952 - val_loss: 0.5453\n",
      "Epoch 12/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 1.4268 - val_loss: 0.5648\n",
      "Epoch 13/50\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5677 - val_loss: 0.5407\n",
      "Epoch 14/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.7541 - val_loss: 0.5486\n",
      "Epoch 15/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5410 - val_loss: 0.5419\n",
      "Epoch 16/50\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8059 - val_loss: 0.5562\n",
      "Epoch 17/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5452 - val_loss: 0.5609\n",
      "Epoch 18/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5335 - val_loss: 0.5484\n",
      "Epoch 19/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6093 - val_loss: 0.5520\n",
      "Epoch 20/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6209 - val_loss: 0.5659\n",
      "Epoch 21/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5446 - val_loss: 0.5584\n",
      "Epoch 22/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5365 - val_loss: 0.6048\n",
      "Epoch 23/50\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5500 - val_loss: 0.5513\n",
      "121/121 [==============================] - 0s 808us/step - loss: 0.5934\n",
      "Epoch 1/50\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 1.2403 - val_loss: 0.5830\n",
      "Epoch 2/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5663 - val_loss: 0.5598\n",
      "Epoch 3/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5493 - val_loss: 0.5530\n",
      "Epoch 4/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5434 - val_loss: 0.5509\n",
      "Epoch 5/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.7393 - val_loss: 0.5467\n",
      "Epoch 6/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5469 - val_loss: 0.5532\n",
      "Epoch 7/50\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5368 - val_loss: 0.5448\n",
      "Epoch 8/50\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5628 - val_loss: 0.5534\n",
      "Epoch 9/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5369 - val_loss: 0.5546\n",
      "Epoch 10/50\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5411 - val_loss: 0.5459\n",
      "Epoch 11/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5835 - val_loss: 0.5552\n",
      "Epoch 12/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5469 - val_loss: 0.5546\n",
      "Epoch 13/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5366 - val_loss: 0.5608\n",
      "Epoch 14/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5365 - val_loss: 0.5682\n",
      "Epoch 15/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5368 - val_loss: 0.5469\n",
      "Epoch 16/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5516 - val_loss: 0.5500\n",
      "Epoch 17/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5483 - val_loss: 0.5545\n",
      "121/121 [==============================] - 0s 952us/step - loss: 0.5074\n",
      "Epoch 1/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 3.7823 - val_loss: 1.2709\n",
      "Epoch 2/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.9069 - val_loss: 0.6873\n",
      "Epoch 3/50\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6407 - val_loss: 0.6127\n",
      "Epoch 4/50\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5973 - val_loss: 0.5958\n",
      "Epoch 5/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5816 - val_loss: 0.5874\n",
      "Epoch 6/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5712 - val_loss: 0.5825\n",
      "Epoch 7/50\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5632 - val_loss: 0.5840\n",
      "Epoch 8/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5576 - val_loss: 0.5720\n",
      "Epoch 9/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5519 - val_loss: 0.5656\n",
      "Epoch 10/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5470 - val_loss: 0.5757\n",
      "Epoch 11/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5447 - val_loss: 0.5622\n",
      "Epoch 12/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5415 - val_loss: 0.5583\n",
      "Epoch 13/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5400 - val_loss: 0.5615\n",
      "Epoch 14/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5359 - val_loss: 0.5543\n",
      "Epoch 15/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5357 - val_loss: 0.5549\n",
      "Epoch 16/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5343 - val_loss: 0.5569\n",
      "Epoch 17/50\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5329 - val_loss: 0.5527\n",
      "Epoch 18/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5325 - val_loss: 0.5543\n",
      "Epoch 19/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5312 - val_loss: 0.5626\n",
      "Epoch 20/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5304 - val_loss: 0.5532\n",
      "Epoch 21/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5304 - val_loss: 0.5549\n",
      "Epoch 22/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5287 - val_loss: 0.5690\n",
      "Epoch 23/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5275 - val_loss: 0.5516\n",
      "Epoch 24/50\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5278 - val_loss: 0.5700\n",
      "Epoch 25/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5289 - val_loss: 0.5640\n",
      "Epoch 26/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5286 - val_loss: 0.5612\n",
      "Epoch 27/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5280 - val_loss: 0.5628\n",
      "Epoch 28/50\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5271 - val_loss: 0.5534\n",
      "Epoch 29/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5281 - val_loss: 0.5550\n",
      "Epoch 30/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5278 - val_loss: 0.5592\n",
      "Epoch 31/50\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5268 - val_loss: 0.5519\n",
      "Epoch 32/50\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5274 - val_loss: 0.5557\n",
      "Epoch 33/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5271 - val_loss: 0.5673\n",
      "121/121 [==============================] - 0s 772us/step - loss: 0.5340\n",
      "Epoch 1/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 2.9561 - val_loss: 1.3188\n",
      "Epoch 2/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 1.0129 - val_loss: 0.8619\n",
      "Epoch 3/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.7830 - val_loss: 0.7753\n",
      "Epoch 4/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.7243 - val_loss: 0.7303\n",
      "Epoch 5/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6890 - val_loss: 0.6941\n",
      "Epoch 6/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6612 - val_loss: 0.6668\n",
      "Epoch 7/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6377 - val_loss: 0.6451\n",
      "Epoch 8/50\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6189 - val_loss: 0.6268\n",
      "Epoch 9/50\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6035 - val_loss: 0.6120\n",
      "Epoch 10/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5898 - val_loss: 0.6040\n",
      "Epoch 11/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5788 - val_loss: 0.5897\n",
      "Epoch 12/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5703 - val_loss: 0.5823\n",
      "Epoch 13/50\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5625 - val_loss: 0.5789\n",
      "Epoch 14/50\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5551 - val_loss: 0.5703\n",
      "Epoch 15/50\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5508 - val_loss: 0.5656\n",
      "Epoch 16/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5464 - val_loss: 0.5643\n",
      "Epoch 17/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5426 - val_loss: 0.5601\n",
      "Epoch 18/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5398 - val_loss: 0.5602\n",
      "Epoch 19/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5371 - val_loss: 0.5597\n",
      "Epoch 20/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5352 - val_loss: 0.5579\n",
      "Epoch 21/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5335 - val_loss: 0.5597\n",
      "Epoch 22/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5312 - val_loss: 0.5673\n",
      "Epoch 23/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5297 - val_loss: 0.5560\n",
      "Epoch 24/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5287 - val_loss: 0.5683\n",
      "Epoch 25/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5291 - val_loss: 0.5636\n",
      "Epoch 26/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5283 - val_loss: 0.5632\n",
      "Epoch 27/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5277 - val_loss: 0.5603\n",
      "Epoch 28/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5265 - val_loss: 0.5552\n",
      "Epoch 29/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5263 - val_loss: 0.5563\n",
      "Epoch 30/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5262 - val_loss: 0.5640\n",
      "Epoch 31/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5258 - val_loss: 0.5573\n",
      "Epoch 32/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5253 - val_loss: 0.5569\n",
      "Epoch 33/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5246 - val_loss: 0.5691\n",
      "Epoch 34/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5253 - val_loss: 0.5668\n",
      "Epoch 35/50\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5239 - val_loss: 0.5576\n",
      "Epoch 36/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5247 - val_loss: 0.5548\n",
      "Epoch 37/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5248 - val_loss: 0.5694\n",
      "Epoch 38/50\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5245 - val_loss: 0.5735\n",
      "Epoch 39/50\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5249 - val_loss: 0.5624\n",
      "Epoch 40/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5246 - val_loss: 0.5636\n",
      "Epoch 41/50\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5248 - val_loss: 0.5623\n",
      "Epoch 42/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5243 - val_loss: 0.5651\n",
      "Epoch 43/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5235 - val_loss: 0.5761\n",
      "Epoch 44/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5248 - val_loss: 0.5602\n",
      "Epoch 45/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5244 - val_loss: 0.5568\n",
      "Epoch 46/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5244 - val_loss: 0.5673\n",
      "121/121 [==============================] - 0s 876us/step - loss: 0.5378\n",
      "Epoch 1/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 2.4894 - val_loss: 1.2115\n",
      "Epoch 2/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.9238 - val_loss: 0.8064\n",
      "Epoch 3/50\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7396 - val_loss: 0.7362\n",
      "Epoch 4/50\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6954 - val_loss: 0.7001\n",
      "Epoch 5/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6691 - val_loss: 0.6724\n",
      "Epoch 6/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6477 - val_loss: 0.6508\n",
      "Epoch 7/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6300 - val_loss: 0.6328\n",
      "Epoch 8/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6149 - val_loss: 0.6186\n",
      "Epoch 9/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6027 - val_loss: 0.6059\n",
      "Epoch 10/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5923 - val_loss: 0.5963\n",
      "Epoch 11/50\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5834 - val_loss: 0.5889\n",
      "Epoch 12/50\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5760 - val_loss: 0.5827\n",
      "Epoch 13/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5698 - val_loss: 0.5775\n",
      "Epoch 14/50\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5646 - val_loss: 0.5731\n",
      "Epoch 15/50\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5600 - val_loss: 0.5686\n",
      "Epoch 16/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5565 - val_loss: 0.5664\n",
      "Epoch 17/50\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5531 - val_loss: 0.5640\n",
      "Epoch 18/50\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5505 - val_loss: 0.5622\n",
      "Epoch 19/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5481 - val_loss: 0.5597\n",
      "Epoch 20/50\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5462 - val_loss: 0.5596\n",
      "Epoch 21/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5447 - val_loss: 0.5587\n",
      "Epoch 22/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5431 - val_loss: 0.5583\n",
      "Epoch 23/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5420 - val_loss: 0.5596\n",
      "Epoch 24/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5410 - val_loss: 0.5574\n",
      "Epoch 25/50\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5400 - val_loss: 0.5577\n",
      "Epoch 26/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5394 - val_loss: 0.5574\n",
      "Epoch 27/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5386 - val_loss: 0.5580\n",
      "Epoch 28/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5383 - val_loss: 0.5564\n",
      "Epoch 29/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5377 - val_loss: 0.5573\n",
      "Epoch 30/50\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5372 - val_loss: 0.5541\n",
      "Epoch 31/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5369 - val_loss: 0.5541\n",
      "Epoch 32/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5369 - val_loss: 0.5566\n",
      "Epoch 33/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5363 - val_loss: 0.5550\n",
      "Epoch 34/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5363 - val_loss: 0.5555\n",
      "Epoch 35/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5361 - val_loss: 0.5563\n",
      "Epoch 36/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5358 - val_loss: 0.5564\n",
      "Epoch 37/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5358 - val_loss: 0.5567\n",
      "Epoch 38/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5357 - val_loss: 0.5570\n",
      "Epoch 39/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5356 - val_loss: 0.5553\n",
      "Epoch 40/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5354 - val_loss: 0.5566\n",
      "Epoch 41/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5352 - val_loss: 0.5566\n",
      "121/121 [==============================] - 0s 805us/step - loss: 0.5149\n",
      "Epoch 1/50\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 1.0510 - val_loss: 0.6177\n",
      "Epoch 2/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6052 - val_loss: 0.5869\n",
      "Epoch 3/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5874 - val_loss: 0.5786\n",
      "Epoch 4/50\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6069 - val_loss: 0.5587\n",
      "Epoch 5/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5426 - val_loss: 0.5707\n",
      "Epoch 6/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5410 - val_loss: 0.5605\n",
      "Epoch 7/50\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5313 - val_loss: 0.9445\n",
      "Epoch 8/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5664 - val_loss: 0.5627\n",
      "Epoch 9/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5461 - val_loss: 0.5459\n",
      "Epoch 10/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5293 - val_loss: 0.8951\n",
      "Epoch 11/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5529 - val_loss: 0.5551\n",
      "Epoch 12/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5438 - val_loss: 0.5455\n",
      "Epoch 13/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5656 - val_loss: 0.5445\n",
      "Epoch 14/50\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5355 - val_loss: 0.5502\n",
      "Epoch 15/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5321 - val_loss: 0.5626\n",
      "Epoch 16/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5376 - val_loss: 0.5560\n",
      "Epoch 17/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5366 - val_loss: 0.5807\n",
      "Epoch 18/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5561 - val_loss: 0.5553\n",
      "Epoch 19/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5383 - val_loss: 0.5709\n",
      "Epoch 20/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5977 - val_loss: 0.5516\n",
      "Epoch 21/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5466 - val_loss: 0.5524\n",
      "Epoch 22/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5333 - val_loss: 0.7488\n",
      "Epoch 23/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5713 - val_loss: 0.5558\n",
      "121/121 [==============================] - 0s 753us/step - loss: 0.5429\n",
      "Epoch 1/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 1.5485 - val_loss: 0.6689\n",
      "Epoch 2/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 1.4095 - val_loss: 0.7385\n",
      "Epoch 3/50\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 99.0158 - val_loss: 2.5439\n",
      "Epoch 4/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 98.5233 - val_loss: 6.1867\n",
      "Epoch 5/50\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 6210.0332 - val_loss: 20.5511\n",
      "Epoch 6/50\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 21821.5508 - val_loss: 162.9858\n",
      "Epoch 7/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 11557.2549 - val_loss: 888.2389\n",
      "Epoch 8/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 106077.5938 - val_loss: 6210.4619\n",
      "Epoch 9/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 7566998.5000 - val_loss: 118287.6094\n",
      "Epoch 10/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 12088845.0000 - val_loss: 898329.4375\n",
      "Epoch 11/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 89709544.0000 - val_loss: 1947786.2500\n",
      "121/121 [==============================] - 0s 908us/step - loss: 173034864.0000\n",
      "Epoch 1/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 1.3823 - val_loss: 0.6546\n",
      "Epoch 2/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6439 - val_loss: 0.5857\n",
      "Epoch 3/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5664 - val_loss: 0.5684\n",
      "Epoch 4/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5590 - val_loss: 0.5654\n",
      "Epoch 5/50\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2.2525 - val_loss: 0.5566\n",
      "Epoch 6/50\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5955 - val_loss: 0.5554\n",
      "Epoch 7/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5380 - val_loss: 0.5477\n",
      "Epoch 8/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6844 - val_loss: 0.5763\n",
      "Epoch 9/50\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5450 - val_loss: 0.5554\n",
      "Epoch 10/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5469 - val_loss: 0.5467\n",
      "Epoch 11/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.9272 - val_loss: 0.5575\n",
      "Epoch 12/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6016 - val_loss: 0.5532\n",
      "Epoch 13/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5384 - val_loss: 0.5638\n",
      "Epoch 14/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5883 - val_loss: 0.5728\n",
      "Epoch 15/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5417 - val_loss: 0.5483\n",
      "Epoch 16/50\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5893 - val_loss: 0.5614\n",
      "Epoch 17/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5713 - val_loss: 0.5498\n",
      "Epoch 18/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5892 - val_loss: 0.5623\n",
      "Epoch 19/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 1.6958 - val_loss: 0.5406\n",
      "Epoch 20/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6843 - val_loss: 0.5518\n",
      "Epoch 21/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5389 - val_loss: 0.5604\n",
      "Epoch 22/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5368 - val_loss: 0.5552\n",
      "Epoch 23/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5428 - val_loss: 0.5692\n",
      "Epoch 24/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6154 - val_loss: 0.5634\n",
      "Epoch 25/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 1.0466 - val_loss: 0.5736\n",
      "Epoch 26/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5466 - val_loss: 0.5485\n",
      "Epoch 27/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 1.2143 - val_loss: 0.5692\n",
      "Epoch 28/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5517 - val_loss: 0.5628\n",
      "Epoch 29/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5562 - val_loss: 0.5664\n",
      "121/121 [==============================] - 0s 694us/step - loss: 0.5159\n",
      "Epoch 1/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 2.9585 - val_loss: 1.6446\n",
      "Epoch 2/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 1.2017 - val_loss: 0.9481\n",
      "Epoch 3/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.8691 - val_loss: 0.8098\n",
      "Epoch 4/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.7818 - val_loss: 0.7579\n",
      "Epoch 5/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.7380 - val_loss: 0.7276\n",
      "Epoch 6/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.7082 - val_loss: 0.7056\n",
      "Epoch 7/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6854 - val_loss: 0.6882\n",
      "Epoch 8/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6661 - val_loss: 0.6728\n",
      "Epoch 9/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6493 - val_loss: 0.6593\n",
      "Epoch 10/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6344 - val_loss: 0.6465\n",
      "Epoch 11/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6211 - val_loss: 0.6352\n",
      "Epoch 12/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6088 - val_loss: 0.6246\n",
      "Epoch 13/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5977 - val_loss: 0.6150\n",
      "Epoch 14/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5875 - val_loss: 0.6059\n",
      "Epoch 15/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5782 - val_loss: 0.5974\n",
      "Epoch 16/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5695 - val_loss: 0.5898\n",
      "Epoch 17/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5615 - val_loss: 0.5826\n",
      "Epoch 18/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5541 - val_loss: 0.5758\n",
      "Epoch 19/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5471 - val_loss: 0.5698\n",
      "Epoch 20/50\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5409 - val_loss: 0.5637\n",
      "Epoch 21/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5350 - val_loss: 0.5581\n",
      "Epoch 22/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5294 - val_loss: 0.5531\n",
      "Epoch 23/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5242 - val_loss: 0.5480\n",
      "Epoch 24/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5192 - val_loss: 0.5438\n",
      "Epoch 25/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5146 - val_loss: 0.5393\n",
      "Epoch 26/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5106 - val_loss: 0.5352\n",
      "Epoch 27/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5065 - val_loss: 0.5317\n",
      "Epoch 28/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5028 - val_loss: 0.5280\n",
      "Epoch 29/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4993 - val_loss: 0.5248\n",
      "Epoch 30/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4959 - val_loss: 0.5217\n",
      "Epoch 31/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4928 - val_loss: 0.5185\n",
      "Epoch 32/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4898 - val_loss: 0.5154\n",
      "Epoch 33/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4870 - val_loss: 0.5129\n",
      "Epoch 34/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4843 - val_loss: 0.5104\n",
      "Epoch 35/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4818 - val_loss: 0.5080\n",
      "Epoch 36/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4795 - val_loss: 0.5053\n",
      "Epoch 37/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4772 - val_loss: 0.5036\n",
      "Epoch 38/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4750 - val_loss: 0.5013\n",
      "Epoch 39/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4730 - val_loss: 0.4991\n",
      "Epoch 40/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4711 - val_loss: 0.4971\n",
      "Epoch 41/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4691 - val_loss: 0.4951\n",
      "Epoch 42/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4674 - val_loss: 0.4932\n",
      "Epoch 43/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4657 - val_loss: 0.4915\n",
      "Epoch 44/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4639 - val_loss: 0.4901\n",
      "Epoch 45/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4624 - val_loss: 0.4883\n",
      "Epoch 46/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4607 - val_loss: 0.4868\n",
      "Epoch 47/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4593 - val_loss: 0.4853\n",
      "Epoch 48/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4578 - val_loss: 0.4838\n",
      "Epoch 49/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4564 - val_loss: 0.4825\n",
      "Epoch 50/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4549 - val_loss: 0.4816\n",
      "121/121 [==============================] - 0s 705us/step - loss: 0.4751\n",
      "Epoch 1/50\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 3.6023 - val_loss: 1.9766\n",
      "Epoch 2/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 1.5111 - val_loss: 1.1363\n",
      "Epoch 3/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.9971 - val_loss: 0.8828\n",
      "Epoch 4/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.8185 - val_loss: 0.7814\n",
      "Epoch 5/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.7433 - val_loss: 0.7333\n",
      "Epoch 6/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.7055 - val_loss: 0.7067\n",
      "Epoch 7/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6829 - val_loss: 0.6884\n",
      "Epoch 8/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6660 - val_loss: 0.6741\n",
      "Epoch 9/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6518 - val_loss: 0.6615\n",
      "Epoch 10/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6393 - val_loss: 0.6511\n",
      "Epoch 11/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6278 - val_loss: 0.6408\n",
      "Epoch 12/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6174 - val_loss: 0.6320\n",
      "Epoch 13/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6074 - val_loss: 0.6233\n",
      "Epoch 14/50\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5981 - val_loss: 0.6151\n",
      "Epoch 15/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5893 - val_loss: 0.6074\n",
      "Epoch 16/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5810 - val_loss: 0.6007\n",
      "Epoch 17/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5731 - val_loss: 0.5936\n",
      "Epoch 18/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5655 - val_loss: 0.5869\n",
      "Epoch 19/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5584 - val_loss: 0.5808\n",
      "Epoch 20/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5517 - val_loss: 0.5758\n",
      "Epoch 21/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5454 - val_loss: 0.5701\n",
      "Epoch 22/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5392 - val_loss: 0.5655\n",
      "Epoch 23/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5335 - val_loss: 0.5599\n",
      "Epoch 24/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5281 - val_loss: 0.5556\n",
      "Epoch 25/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5231 - val_loss: 0.5509\n",
      "Epoch 26/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5183 - val_loss: 0.5472\n",
      "Epoch 27/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5137 - val_loss: 0.5435\n",
      "Epoch 28/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5094 - val_loss: 0.5390\n",
      "Epoch 29/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5053 - val_loss: 0.5357\n",
      "Epoch 30/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5014 - val_loss: 0.5318\n",
      "Epoch 31/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4979 - val_loss: 0.5288\n",
      "Epoch 32/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4944 - val_loss: 0.5258\n",
      "Epoch 33/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4911 - val_loss: 0.5238\n",
      "Epoch 34/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4880 - val_loss: 0.5206\n",
      "Epoch 35/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4850 - val_loss: 0.5174\n",
      "Epoch 36/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4823 - val_loss: 0.5148\n",
      "Epoch 37/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4796 - val_loss: 0.5130\n",
      "Epoch 38/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4770 - val_loss: 0.5113\n",
      "Epoch 39/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4747 - val_loss: 0.5086\n",
      "Epoch 40/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4724 - val_loss: 0.5067\n",
      "Epoch 41/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4702 - val_loss: 0.5039\n",
      "Epoch 42/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4681 - val_loss: 0.5025\n",
      "Epoch 43/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4661 - val_loss: 0.5015\n",
      "Epoch 44/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4642 - val_loss: 0.4989\n",
      "Epoch 45/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4624 - val_loss: 0.4972\n",
      "Epoch 46/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4604 - val_loss: 0.4965\n",
      "Epoch 47/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4589 - val_loss: 0.4940\n",
      "Epoch 48/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4573 - val_loss: 0.4927\n",
      "Epoch 49/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4557 - val_loss: 0.4905\n",
      "Epoch 50/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4541 - val_loss: 0.4900\n",
      "121/121 [==============================] - 0s 816us/step - loss: 0.4773\n",
      "Epoch 1/50\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 3.6018 - val_loss: 2.2816\n",
      "Epoch 2/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 1.5999 - val_loss: 1.3979\n",
      "Epoch 3/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.9865 - val_loss: 1.0105\n",
      "Epoch 4/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.7774 - val_loss: 0.8083\n",
      "Epoch 5/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6926 - val_loss: 0.7104\n",
      "Epoch 6/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6511 - val_loss: 0.6624\n",
      "Epoch 7/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6258 - val_loss: 0.6312\n",
      "Epoch 8/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6077 - val_loss: 0.6115\n",
      "Epoch 9/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5938 - val_loss: 0.5943\n",
      "Epoch 10/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5822 - val_loss: 0.5810\n",
      "Epoch 11/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5723 - val_loss: 0.5711\n",
      "Epoch 12/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5636 - val_loss: 0.5631\n",
      "Epoch 13/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5558 - val_loss: 0.5554\n",
      "Epoch 14/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5489 - val_loss: 0.5493\n",
      "Epoch 15/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5425 - val_loss: 0.5428\n",
      "Epoch 16/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5369 - val_loss: 0.5383\n",
      "Epoch 17/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5314 - val_loss: 0.5334\n",
      "Epoch 18/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5264 - val_loss: 0.5284\n",
      "Epoch 19/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5221 - val_loss: 0.5244\n",
      "Epoch 20/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5176 - val_loss: 0.5217\n",
      "Epoch 21/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5140 - val_loss: 0.5170\n",
      "Epoch 22/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5102 - val_loss: 0.5136\n",
      "Epoch 23/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5068 - val_loss: 0.5104\n",
      "Epoch 24/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5037 - val_loss: 0.5075\n",
      "Epoch 25/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5007 - val_loss: 0.5051\n",
      "Epoch 26/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4979 - val_loss: 0.5023\n",
      "Epoch 27/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4951 - val_loss: 0.5003\n",
      "Epoch 28/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4929 - val_loss: 0.4972\n",
      "Epoch 29/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4902 - val_loss: 0.4955\n",
      "Epoch 30/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4881 - val_loss: 0.4928\n",
      "Epoch 31/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4857 - val_loss: 0.4907\n",
      "Epoch 32/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4839 - val_loss: 0.4888\n",
      "Epoch 33/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4818 - val_loss: 0.4872\n",
      "Epoch 34/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4799 - val_loss: 0.4853\n",
      "Epoch 35/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4782 - val_loss: 0.4835\n",
      "Epoch 36/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4764 - val_loss: 0.4818\n",
      "Epoch 37/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4747 - val_loss: 0.4803\n",
      "Epoch 38/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4732 - val_loss: 0.4791\n",
      "Epoch 39/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4717 - val_loss: 0.4773\n",
      "Epoch 40/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4702 - val_loss: 0.4759\n",
      "Epoch 41/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4686 - val_loss: 0.4747\n",
      "Epoch 42/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4674 - val_loss: 0.4733\n",
      "Epoch 43/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4662 - val_loss: 0.4724\n",
      "Epoch 44/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4648 - val_loss: 0.4714\n",
      "Epoch 45/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4636 - val_loss: 0.4706\n",
      "Epoch 46/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4624 - val_loss: 0.4692\n",
      "Epoch 47/50\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4611 - val_loss: 0.4683\n",
      "Epoch 48/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4601 - val_loss: 0.4672\n",
      "Epoch 49/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4590 - val_loss: 0.4660\n",
      "Epoch 50/50\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4578 - val_loss: 0.4651\n",
      "121/121 [==============================] - 0s 741us/step - loss: 0.4268\n",
      "Epoch 1/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.9733 - val_loss: 0.6116\n",
      "Epoch 2/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5600 - val_loss: 0.5550\n",
      "Epoch 3/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5052 - val_loss: 0.5075\n",
      "Epoch 4/50\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4889 - val_loss: 0.4913\n",
      "Epoch 5/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4682 - val_loss: 0.4841\n",
      "Epoch 6/50\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4557 - val_loss: 0.4731\n",
      "Epoch 7/50\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4484 - val_loss: 0.5009\n",
      "Epoch 8/50\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4439 - val_loss: 0.4672\n",
      "Epoch 9/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4408 - val_loss: 0.4516\n",
      "Epoch 10/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4306 - val_loss: 0.5650\n",
      "Epoch 11/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4302 - val_loss: 0.4489\n",
      "Epoch 12/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4254 - val_loss: 0.4408\n",
      "Epoch 13/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4228 - val_loss: 0.4371\n",
      "Epoch 14/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4158 - val_loss: 0.4317\n",
      "Epoch 15/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4125 - val_loss: 0.4352\n",
      "Epoch 16/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4103 - val_loss: 0.4332\n",
      "Epoch 17/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4089 - val_loss: 0.4286\n",
      "Epoch 18/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4077 - val_loss: 0.4376\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4027 - val_loss: 0.4236\n",
      "Epoch 20/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4066 - val_loss: 0.4185\n",
      "Epoch 21/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3970 - val_loss: 0.4164\n",
      "Epoch 22/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3954 - val_loss: 0.4255\n",
      "Epoch 23/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3966 - val_loss: 0.4249\n",
      "Epoch 24/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3909 - val_loss: 0.4241\n",
      "Epoch 25/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3923 - val_loss: 0.4112\n",
      "Epoch 26/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3896 - val_loss: 0.4132\n",
      "Epoch 27/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3862 - val_loss: 0.4156\n",
      "Epoch 28/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3851 - val_loss: 0.4140\n",
      "Epoch 29/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3847 - val_loss: 0.4164\n",
      "Epoch 30/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3797 - val_loss: 0.4154\n",
      "Epoch 31/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3798 - val_loss: 0.4033\n",
      "Epoch 32/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3795 - val_loss: 0.4135\n",
      "Epoch 33/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3780 - val_loss: 0.4742\n",
      "Epoch 34/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3841 - val_loss: 0.3998\n",
      "Epoch 35/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3730 - val_loss: 0.4018\n",
      "Epoch 36/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3760 - val_loss: 0.4036\n",
      "Epoch 37/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3733 - val_loss: 0.4043\n",
      "Epoch 38/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3721 - val_loss: 0.4654\n",
      "Epoch 39/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3719 - val_loss: 0.3973\n",
      "Epoch 40/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3669 - val_loss: 0.3972\n",
      "Epoch 41/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3729 - val_loss: 0.3928\n",
      "Epoch 42/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3676 - val_loss: 0.3962\n",
      "Epoch 43/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3651 - val_loss: 0.4406\n",
      "Epoch 44/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3655 - val_loss: 0.4099\n",
      "Epoch 45/50\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3767 - val_loss: 0.4096\n",
      "Epoch 46/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3677 - val_loss: 0.3898\n",
      "Epoch 47/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3707 - val_loss: 0.3882\n",
      "Epoch 48/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3695 - val_loss: 0.4649\n",
      "Epoch 49/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3706 - val_loss: 0.3897\n",
      "Epoch 50/50\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3685 - val_loss: 0.3912\n",
      "121/121 [==============================] - 0s 786us/step - loss: 0.5012\n",
      "Epoch 1/50\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 1.0711 - val_loss: 0.6895\n",
      "Epoch 2/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6767 - val_loss: 0.5538\n",
      "Epoch 3/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5077 - val_loss: 0.5071\n",
      "Epoch 4/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4755 - val_loss: 0.4874\n",
      "Epoch 5/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4599 - val_loss: 0.4748\n",
      "Epoch 6/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4508 - val_loss: 0.4718\n",
      "Epoch 7/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4444 - val_loss: 0.4763\n",
      "Epoch 8/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4389 - val_loss: 0.4616\n",
      "Epoch 9/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4355 - val_loss: 0.4566\n",
      "Epoch 10/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4290 - val_loss: 0.4945\n",
      "Epoch 11/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4261 - val_loss: 0.4483\n",
      "Epoch 12/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4212 - val_loss: 0.4458\n",
      "Epoch 13/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4169 - val_loss: 0.4547\n",
      "Epoch 14/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4164 - val_loss: 0.4357\n",
      "Epoch 15/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4107 - val_loss: 0.4318\n",
      "Epoch 16/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4138 - val_loss: 0.4338\n",
      "Epoch 17/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4052 - val_loss: 0.4335\n",
      "Epoch 18/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4030 - val_loss: 0.4702\n",
      "Epoch 19/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4125 - val_loss: 0.4248\n",
      "Epoch 20/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4173 - val_loss: 0.4218\n",
      "Epoch 21/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3941 - val_loss: 0.4205\n",
      "Epoch 22/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3914 - val_loss: 0.4195\n",
      "Epoch 23/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3889 - val_loss: 0.4136\n",
      "Epoch 24/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3879 - val_loss: 0.4361\n",
      "Epoch 25/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3879 - val_loss: 0.4115\n",
      "Epoch 26/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3845 - val_loss: 0.4115\n",
      "Epoch 27/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3833 - val_loss: 0.4094\n",
      "Epoch 28/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3793 - val_loss: 0.4110\n",
      "Epoch 29/50\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3745 - val_loss: 0.4299\n",
      "Epoch 30/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3749 - val_loss: 0.4140\n",
      "Epoch 31/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3733 - val_loss: 0.4117\n",
      "Epoch 32/50\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3809 - val_loss: 0.3983\n",
      "Epoch 33/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3765 - val_loss: 0.5064\n",
      "Epoch 34/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3721 - val_loss: 0.3961\n",
      "Epoch 35/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3674 - val_loss: 0.3934\n",
      "Epoch 36/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3679 - val_loss: 0.3947\n",
      "Epoch 37/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3676 - val_loss: 0.3963\n",
      "Epoch 38/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3622 - val_loss: 0.3897\n",
      "Epoch 39/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3610 - val_loss: 0.3870\n",
      "Epoch 40/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3586 - val_loss: 0.3984\n",
      "Epoch 41/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3556 - val_loss: 0.3864\n",
      "Epoch 42/50\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3551 - val_loss: 0.3839\n",
      "Epoch 43/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3545 - val_loss: 0.5621\n",
      "Epoch 44/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3710 - val_loss: 0.3945\n",
      "Epoch 45/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3755 - val_loss: 0.3928\n",
      "Epoch 46/50\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3511 - val_loss: 0.3800\n",
      "Epoch 47/50\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3492 - val_loss: 0.3981\n",
      "Epoch 48/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3499 - val_loss: 0.3932\n",
      "Epoch 49/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3498 - val_loss: 0.3916\n",
      "Epoch 50/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3473 - val_loss: 0.3732\n",
      "121/121 [==============================] - 0s 784us/step - loss: 0.3794\n",
      "Epoch 1/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 1.4426 - val_loss: 0.5932\n",
      "Epoch 2/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6567 - val_loss: 0.5340\n",
      "Epoch 3/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5110 - val_loss: 0.5065\n",
      "Epoch 4/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4851 - val_loss: 0.4932\n",
      "Epoch 5/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4721 - val_loss: 0.4690\n",
      "Epoch 6/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4622 - val_loss: 0.4620\n",
      "Epoch 7/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4533 - val_loss: 0.4499\n",
      "Epoch 8/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4455 - val_loss: 0.4437\n",
      "Epoch 9/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4416 - val_loss: 0.4401\n",
      "Epoch 10/50\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4358 - val_loss: 0.4352\n",
      "Epoch 11/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4299 - val_loss: 0.4278\n",
      "Epoch 12/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4270 - val_loss: 0.4298\n",
      "Epoch 13/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4226 - val_loss: 0.4227\n",
      "Epoch 14/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4189 - val_loss: 0.4175\n",
      "Epoch 15/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4152 - val_loss: 0.4119\n",
      "Epoch 16/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4114 - val_loss: 0.4088\n",
      "Epoch 17/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4097 - val_loss: 0.4107\n",
      "Epoch 18/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4065 - val_loss: 0.4030\n",
      "Epoch 19/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4039 - val_loss: 0.4050\n",
      "Epoch 20/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4011 - val_loss: 0.4122\n",
      "Epoch 21/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3998 - val_loss: 0.4039\n",
      "Epoch 22/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3972 - val_loss: 0.4277\n",
      "Epoch 23/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3953 - val_loss: 0.4031\n",
      "Epoch 24/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3931 - val_loss: 0.4059\n",
      "Epoch 25/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3914 - val_loss: 0.4005\n",
      "Epoch 26/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3888 - val_loss: 0.4119\n",
      "Epoch 27/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3886 - val_loss: 0.3925\n",
      "Epoch 28/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3880 - val_loss: 0.3870\n",
      "Epoch 29/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3831 - val_loss: 0.3875\n",
      "Epoch 30/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3840 - val_loss: 0.3885\n",
      "Epoch 31/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3794 - val_loss: 0.3988\n",
      "Epoch 32/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3792 - val_loss: 0.3903\n",
      "Epoch 33/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3780 - val_loss: 0.3880\n",
      "Epoch 34/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3761 - val_loss: 0.3882\n",
      "Epoch 35/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3757 - val_loss: 0.3803\n",
      "Epoch 36/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3748 - val_loss: 0.3778\n",
      "Epoch 37/50\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.3719 - val_loss: 0.3872\n",
      "Epoch 38/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3734 - val_loss: 0.3765\n",
      "Epoch 39/50\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3695 - val_loss: 0.3957\n",
      "Epoch 40/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3692 - val_loss: 0.3721\n",
      "Epoch 41/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3673 - val_loss: 0.3764\n",
      "Epoch 42/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3692 - val_loss: 0.3752\n",
      "Epoch 43/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3667 - val_loss: 0.3881\n",
      "Epoch 44/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3660 - val_loss: 0.3724\n",
      "Epoch 45/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3656 - val_loss: 0.3794\n",
      "Epoch 46/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3642 - val_loss: 0.3834\n",
      "Epoch 47/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3638 - val_loss: 0.3726\n",
      "Epoch 48/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3612 - val_loss: 0.3805\n",
      "Epoch 49/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3611 - val_loss: 0.3749\n",
      "Epoch 50/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3590 - val_loss: 0.3790\n",
      "121/121 [==============================] - 0s 839us/step - loss: 0.3526\n",
      "Epoch 1/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 1.5433 - val_loss: 0.6180\n",
      "Epoch 2/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5913 - val_loss: 0.5823\n",
      "Epoch 3/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5625 - val_loss: 0.5665\n",
      "Epoch 4/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5520 - val_loss: 0.5575\n",
      "Epoch 5/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5409 - val_loss: 0.5509\n",
      "Epoch 6/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5368 - val_loss: 0.5602\n",
      "Epoch 7/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5322 - val_loss: 0.6650\n",
      "Epoch 8/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5400 - val_loss: 0.5578\n",
      "Epoch 9/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5334 - val_loss: 0.5462\n",
      "Epoch 10/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5273 - val_loss: 0.6477\n",
      "Epoch 11/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5359 - val_loss: 0.5514\n",
      "Epoch 12/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5329 - val_loss: 0.5453\n",
      "Epoch 13/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5406 - val_loss: 0.5536\n",
      "Epoch 14/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5276 - val_loss: 0.5467\n",
      "Epoch 15/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5300 - val_loss: 0.5531\n",
      "Epoch 16/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5322 - val_loss: 0.5559\n",
      "Epoch 17/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5313 - val_loss: 0.5490\n",
      "Epoch 18/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5352 - val_loss: 0.5460\n",
      "Epoch 19/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5318 - val_loss: 0.5765\n",
      "Epoch 20/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5392 - val_loss: 0.5499\n",
      "Epoch 21/50\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5352 - val_loss: 0.5525\n",
      "Epoch 22/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5283 - val_loss: 0.6357\n",
      "121/121 [==============================] - 0s 719us/step - loss: 0.5447\n",
      "Epoch 1/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 1.4802 - val_loss: 0.5856\n",
      "Epoch 2/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5628 - val_loss: 0.5637\n",
      "Epoch 3/50\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5500 - val_loss: 0.5607\n",
      "Epoch 4/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5459 - val_loss: 0.5594\n",
      "Epoch 5/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5361 - val_loss: 0.5520\n",
      "Epoch 6/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5339 - val_loss: 0.5777\n",
      "Epoch 7/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5298 - val_loss: 0.7441\n",
      "Epoch 8/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5428 - val_loss: 0.5681\n",
      "Epoch 9/50\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5354 - val_loss: 0.5597\n",
      "Epoch 10/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5270 - val_loss: 0.7073\n",
      "Epoch 11/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5331 - val_loss: 0.5564\n",
      "Epoch 12/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5384 - val_loss: 0.5571\n",
      "Epoch 13/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5317 - val_loss: 0.5541\n",
      "Epoch 14/50\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5308 - val_loss: 0.5528\n",
      "Epoch 15/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5276 - val_loss: 0.5482\n",
      "Epoch 16/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5323 - val_loss: 0.5627\n",
      "Epoch 17/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5284 - val_loss: 0.5517\n",
      "Epoch 18/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5287 - val_loss: 0.5481\n",
      "Epoch 19/50\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5311 - val_loss: 0.5590\n",
      "Epoch 20/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5400 - val_loss: 0.5640\n",
      "Epoch 21/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5345 - val_loss: 0.5632\n",
      "Epoch 22/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5260 - val_loss: 0.6068\n",
      "Epoch 23/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5260 - val_loss: 0.5595\n",
      "Epoch 24/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5312 - val_loss: 0.6276\n",
      "Epoch 25/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5350 - val_loss: 0.5829\n",
      "Epoch 26/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5323 - val_loss: 0.5861\n",
      "Epoch 27/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5360 - val_loss: 0.5607\n",
      "Epoch 28/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5519 - val_loss: 0.5598\n",
      "121/121 [==============================] - 0s 754us/step - loss: 0.5370\n",
      "Epoch 1/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 1.7594 - val_loss: 0.6705\n",
      "Epoch 2/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6418 - val_loss: 0.6243\n",
      "Epoch 3/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6016 - val_loss: 0.5955\n",
      "Epoch 4/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5750 - val_loss: 0.5767\n",
      "Epoch 5/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5957 - val_loss: 0.5679\n",
      "Epoch 6/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5512 - val_loss: 0.5649\n",
      "Epoch 7/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5459 - val_loss: 0.5566\n",
      "Epoch 8/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5432 - val_loss: 0.5590\n",
      "Epoch 9/50\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5414 - val_loss: 0.5594\n",
      "Epoch 10/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5407 - val_loss: 0.5521\n",
      "Epoch 11/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5424 - val_loss: 0.5606\n",
      "Epoch 12/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5373 - val_loss: 0.5598\n",
      "Epoch 13/50\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5370 - val_loss: 0.5609\n",
      "Epoch 14/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5373 - val_loss: 0.5645\n",
      "Epoch 15/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5362 - val_loss: 0.5500\n",
      "Epoch 16/50\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5430 - val_loss: 0.5548\n",
      "Epoch 17/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5376 - val_loss: 0.5574\n",
      "Epoch 18/50\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5385 - val_loss: 0.5545\n",
      "Epoch 19/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5359 - val_loss: 0.5485\n",
      "Epoch 20/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5364 - val_loss: 0.5548\n",
      "Epoch 21/50\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5367 - val_loss: 0.5562\n",
      "Epoch 22/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5368 - val_loss: 0.5557\n",
      "Epoch 23/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5376 - val_loss: 0.5651\n",
      "Epoch 24/50\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5378 - val_loss: 0.5551\n",
      "Epoch 25/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5356 - val_loss: 0.5596\n",
      "Epoch 26/50\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5362 - val_loss: 0.5571\n",
      "Epoch 27/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5369 - val_loss: 0.5599\n",
      "Epoch 28/50\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5387 - val_loss: 0.5561\n",
      "Epoch 29/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5363 - val_loss: 0.5623\n",
      "121/121 [==============================] - 0s 735us/step - loss: 0.5143\n",
      "Epoch 1/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 1.3951 - val_loss: 0.6646\n",
      "Epoch 2/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5910 - val_loss: 0.5646\n",
      "Epoch 3/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5212 - val_loss: 0.5222\n",
      "Epoch 4/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4937 - val_loss: 0.5063\n",
      "Epoch 5/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4823 - val_loss: 0.4975\n",
      "Epoch 6/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4748 - val_loss: 0.4938\n",
      "Epoch 7/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4701 - val_loss: 0.4902\n",
      "Epoch 8/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4644 - val_loss: 0.4856\n",
      "Epoch 9/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4625 - val_loss: 0.4786\n",
      "Epoch 10/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4582 - val_loss: 0.4796\n",
      "Epoch 11/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4566 - val_loss: 0.4739\n",
      "Epoch 12/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4535 - val_loss: 0.4708\n",
      "Epoch 13/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4514 - val_loss: 0.4715\n",
      "Epoch 14/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4499 - val_loss: 0.4686\n",
      "Epoch 15/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4474 - val_loss: 0.4742\n",
      "Epoch 16/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4477 - val_loss: 0.4680\n",
      "Epoch 17/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4436 - val_loss: 0.4628\n",
      "Epoch 18/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4419 - val_loss: 0.4605\n",
      "Epoch 19/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4417 - val_loss: 0.4642\n",
      "Epoch 20/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4507 - val_loss: 0.4553\n",
      "Epoch 21/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4400 - val_loss: 0.4543\n",
      "Epoch 22/50\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4357 - val_loss: 0.4660\n",
      "Epoch 23/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4415 - val_loss: 0.4546\n",
      "Epoch 24/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4336 - val_loss: 0.5268\n",
      "Epoch 25/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4376 - val_loss: 0.4494\n",
      "Epoch 26/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4307 - val_loss: 0.4500\n",
      "Epoch 27/50\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4292 - val_loss: 0.4509\n",
      "Epoch 28/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4268 - val_loss: 0.4463\n",
      "Epoch 29/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4244 - val_loss: 0.4482\n",
      "Epoch 30/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4228 - val_loss: 0.4456\n",
      "Epoch 31/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4234 - val_loss: 0.4435\n",
      "Epoch 32/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4217 - val_loss: 0.4493\n",
      "Epoch 33/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4199 - val_loss: 0.4775\n",
      "Epoch 34/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4181 - val_loss: 0.4384\n",
      "Epoch 35/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4162 - val_loss: 0.4435\n",
      "Epoch 36/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4167 - val_loss: 0.4373\n",
      "Epoch 37/50\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4132 - val_loss: 0.4367\n",
      "Epoch 38/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4132 - val_loss: 0.4558\n",
      "Epoch 39/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4128 - val_loss: 0.4342\n",
      "Epoch 40/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4122 - val_loss: 0.4328\n",
      "Epoch 41/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4112 - val_loss: 0.4301\n",
      "Epoch 42/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4099 - val_loss: 0.4306\n",
      "Epoch 43/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4083 - val_loss: 0.4377\n",
      "Epoch 44/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4058 - val_loss: 0.4370\n",
      "Epoch 45/50\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4194 - val_loss: 0.4308\n",
      "Epoch 46/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4063 - val_loss: 0.4268\n",
      "Epoch 47/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4039 - val_loss: 0.4276\n",
      "Epoch 48/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4032 - val_loss: 0.4779\n",
      "Epoch 49/50\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4053 - val_loss: 0.4273\n",
      "Epoch 50/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4036 - val_loss: 0.4263\n",
      "121/121 [==============================] - 0s 755us/step - loss: 0.4149\n",
      "Epoch 1/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 1.1649 - val_loss: 0.7161\n",
      "Epoch 2/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6343 - val_loss: 0.5848\n",
      "Epoch 3/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5454 - val_loss: 0.5405\n",
      "Epoch 4/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5184 - val_loss: 0.5171\n",
      "Epoch 5/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4948 - val_loss: 0.5049\n",
      "Epoch 6/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4837 - val_loss: 0.4995\n",
      "Epoch 7/50\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4749 - val_loss: 0.5651\n",
      "Epoch 8/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4706 - val_loss: 0.4863\n",
      "Epoch 9/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4639 - val_loss: 0.4762\n",
      "Epoch 10/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4532 - val_loss: 0.6017\n",
      "Epoch 11/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4499 - val_loss: 0.4645\n",
      "Epoch 12/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4421 - val_loss: 0.4581\n",
      "Epoch 13/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4370 - val_loss: 0.4521\n",
      "Epoch 14/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4303 - val_loss: 0.4468\n",
      "Epoch 15/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4256 - val_loss: 0.4437\n",
      "Epoch 16/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4218 - val_loss: 0.4429\n",
      "Epoch 17/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4217 - val_loss: 0.4414\n",
      "Epoch 18/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4171 - val_loss: 0.4391\n",
      "Epoch 19/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4142 - val_loss: 0.4339\n",
      "Epoch 20/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4138 - val_loss: 0.4354\n",
      "Epoch 21/50\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4113 - val_loss: 0.4316\n",
      "Epoch 22/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4088 - val_loss: 0.4411\n",
      "Epoch 23/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4057 - val_loss: 0.4269\n",
      "Epoch 24/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4037 - val_loss: 0.4328\n",
      "Epoch 25/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4036 - val_loss: 0.4226\n",
      "Epoch 26/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4019 - val_loss: 0.4243\n",
      "Epoch 27/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3993 - val_loss: 0.4221\n",
      "Epoch 28/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3978 - val_loss: 0.4218\n",
      "Epoch 29/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3955 - val_loss: 0.4240\n",
      "Epoch 30/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3953 - val_loss: 0.4168\n",
      "Epoch 31/50\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3934 - val_loss: 0.4157\n",
      "Epoch 32/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3917 - val_loss: 0.4199\n",
      "Epoch 33/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3908 - val_loss: 0.4158\n",
      "Epoch 34/50\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3892 - val_loss: 0.4147\n",
      "Epoch 35/50\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3868 - val_loss: 0.4125\n",
      "Epoch 36/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3867 - val_loss: 0.4111\n",
      "Epoch 37/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3838 - val_loss: 0.4147\n",
      "Epoch 38/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3846 - val_loss: 0.4188\n",
      "Epoch 39/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3821 - val_loss: 0.4071\n",
      "Epoch 40/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3818 - val_loss: 0.4084\n",
      "Epoch 41/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3792 - val_loss: 0.4053\n",
      "Epoch 42/50\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3785 - val_loss: 0.4122\n",
      "Epoch 43/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3779 - val_loss: 0.4363\n",
      "Epoch 44/50\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3800 - val_loss: 0.4032\n",
      "Epoch 45/50\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3755 - val_loss: 0.4012\n",
      "Epoch 46/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3751 - val_loss: 0.3988\n",
      "Epoch 47/50\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3716 - val_loss: 0.4016\n",
      "Epoch 48/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3715 - val_loss: 0.4117\n",
      "Epoch 49/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3708 - val_loss: 0.4006\n",
      "Epoch 50/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3724 - val_loss: 0.3957\n",
      "121/121 [==============================] - 0s 803us/step - loss: 0.4019\n",
      "Epoch 1/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 1.4925 - val_loss: 0.6836\n",
      "Epoch 2/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6352 - val_loss: 0.5950\n",
      "Epoch 3/50\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5772 - val_loss: 0.5627\n",
      "Epoch 4/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5493 - val_loss: 0.5398\n",
      "Epoch 5/50\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5371 - val_loss: 0.5273\n",
      "Epoch 6/50\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5205 - val_loss: 0.5172\n",
      "Epoch 7/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5100 - val_loss: 0.5074\n",
      "Epoch 8/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5015 - val_loss: 0.5043\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4958 - val_loss: 0.4976\n",
      "Epoch 10/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4907 - val_loss: 0.4929\n",
      "Epoch 11/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4853 - val_loss: 0.4900\n",
      "Epoch 12/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4814 - val_loss: 0.4867\n",
      "Epoch 13/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4773 - val_loss: 0.4852\n",
      "Epoch 14/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4734 - val_loss: 0.4831\n",
      "Epoch 15/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4706 - val_loss: 0.4741\n",
      "Epoch 16/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4671 - val_loss: 0.4703\n",
      "Epoch 17/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4655 - val_loss: 0.4714\n",
      "Epoch 18/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4651 - val_loss: 0.4661\n",
      "Epoch 19/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4620 - val_loss: 0.4630\n",
      "Epoch 20/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4578 - val_loss: 0.4660\n",
      "Epoch 21/50\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4558 - val_loss: 0.4613\n",
      "Epoch 22/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4522 - val_loss: 0.4593\n",
      "Epoch 23/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4506 - val_loss: 0.4576\n",
      "Epoch 24/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4494 - val_loss: 0.4517\n",
      "Epoch 25/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4457 - val_loss: 0.4507\n",
      "Epoch 26/50\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4429 - val_loss: 0.4527\n",
      "Epoch 27/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4421 - val_loss: 0.4464\n",
      "Epoch 28/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4422 - val_loss: 0.4544\n",
      "Epoch 29/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4354 - val_loss: 0.4457\n",
      "Epoch 30/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4380 - val_loss: 0.4426\n",
      "Epoch 31/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4301 - val_loss: 0.4398\n",
      "Epoch 32/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4301 - val_loss: 0.4358\n",
      "Epoch 33/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4262 - val_loss: 0.4360\n",
      "Epoch 34/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4240 - val_loss: 0.4310\n",
      "Epoch 35/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4267 - val_loss: 0.4293\n",
      "Epoch 36/50\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4201 - val_loss: 0.4284\n",
      "Epoch 37/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4175 - val_loss: 0.4314\n",
      "Epoch 38/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4163 - val_loss: 0.4245\n",
      "Epoch 39/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4144 - val_loss: 0.4316\n",
      "Epoch 40/50\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4146 - val_loss: 0.4176\n",
      "Epoch 41/50\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4089 - val_loss: 0.4188\n",
      "Epoch 42/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4101 - val_loss: 0.4144\n",
      "Epoch 43/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4066 - val_loss: 0.4148\n",
      "Epoch 44/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4061 - val_loss: 0.4112\n",
      "Epoch 45/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4061 - val_loss: 0.4137\n",
      "Epoch 46/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4038 - val_loss: 0.4107\n",
      "Epoch 47/50\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3993 - val_loss: 0.4091\n",
      "Epoch 48/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3999 - val_loss: 0.4097\n",
      "Epoch 49/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3964 - val_loss: 0.4115\n",
      "Epoch 50/50\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3941 - val_loss: 0.4071\n",
      "121/121 [==============================] - 0s 725us/step - loss: 0.3771\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Cannot clone object <tensorflow.python.keras.wrappers.scikit_learn.KerasRegressor object at 0x7f59111978e0>, as the constructor either does not set or modifies parameter learning_rate",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-747bd833228c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m }\n\u001b[1;32m      9\u001b[0m \u001b[0mrnd_search_cv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomizedSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeras_reg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_distribs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m rnd_search_cv.fit(X_train, y_train, epochs=50,\n\u001b[0m\u001b[1;32m     11\u001b[0m                   \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m                   callbacks=[keras.callbacks.EarlyStopping(patience=10), tensorboard_cb])\n",
      "\u001b[0;32m~/homl/lib/python3.8/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    733\u001b[0m             \u001b[0;31m# we clone again after setting params in case some\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m             \u001b[0;31m# of the params are estimators as well.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 735\u001b[0;31m             self.best_estimator_ = clone(clone(base_estimator).set_params(\n\u001b[0m\u001b[1;32m    736\u001b[0m                 **self.best_params_))\n\u001b[1;32m    737\u001b[0m             \u001b[0mrefit_start_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/homl/lib/python3.8/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36mclone\u001b[0;34m(estimator, safe)\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0mparam2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparams_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mparam1\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mparam2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m             raise RuntimeError('Cannot clone object %s, as the constructor '\n\u001b[0m\u001b[1;32m     81\u001b[0m                                \u001b[0;34m'either does not set or modifies parameter %s'\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m                                (estimator, name))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Cannot clone object <tensorflow.python.keras.wrappers.scikit_learn.KerasRegressor object at 0x7f59111978e0>, as the constructor either does not set or modifies parameter learning_rate"
     ]
    }
   ],
   "source": [
    "from scipy.stats import reciprocal\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "param_distribs = {\n",
    "    \"n_hidden\" : [0, 1, 2, 3],\n",
    "    \"n_neurons\" : np.arange(1,100),\n",
    "    \"learning_rate\" : reciprocal(3e-4, 3e-2)\n",
    "}\n",
    "rnd_search_cv = RandomizedSearchCV(keras_reg, param_distribs, n_iter=10, cv=3)\n",
    "rnd_search_cv.fit(X_train, y_train, epochs=50,\n",
    "                  validation_data=(X_valid, y_valid), \n",
    "                  callbacks=[keras.callbacks.EarlyStopping(patience=10), tensorboard_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd_search_cv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd_search_cv.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Randomized search works well for many fairly simple problems, however when training is slow, this approach only explores a tiny portion of the hyperparameter space.\n",
    "\n",
    "You can first run a quick Random search using wide ranges of hyperparparameter values, then run another search using smaller ranges of values centered on the best ones found during the first ones, and so one. This, however, is a very time consuming approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some Python libraries you can use to optimize hyperparameters\n",
    "\n",
    "[Hyperopt](https://github.com/hyperopt/hyperopt)\n",
    "    - Popular library for optimization over all sorts of complex seach spaces (include real and discrete values)\n",
    "    \n",
    "[Hyperas](https://github.com/maxpumperla/hyperas), [kopt](https://github.com/Avsecz/kopt), [Talos](https://github.com/autonomio/talos)\n",
    "    - Useful for optimizing Keras hyperparameters\n",
    "    \n",
    "[Keras tuner](https://homl.info/kerastuner)\n",
    "    - Library by Google for Keras models with hosted service for visualization and Analysis\n",
    "    \n",
    "[Scikit-Optimize(skopt)](https://scikit-optimize.github.io)\n",
    "    - General purpose optimization library. ```BayesSearchCV``` performs optimization similarly to GridSearchCV\n",
    "    \n",
    "[Spearmint](https://github.com/JasperSnoek/spearmint)\n",
    "    - Bayesian optimization library\n",
    "    \n",
    "[Hyperband](https://github.com/zygmuntz/hyperband)\n",
    "    - Fast hyperparameter tuning library based on the recent Hyperband paper\n",
    "    \n",
    "[Sklearn-Deap](https://github.com/rsteca/sklearn-deap)\n",
    "    - Based on evolutionary algorithms with GridSearchCV-like interface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of Hidden Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For many problems, begin with a single hidden layer. Provided it has enough neurons, an MLP with a single hidden layer can model even the mode complex functions. But for complex problems, deep networks have a much higher *parameter efficiency* than shallow ones: they can model comples functions using exponentially fewer neurons, allowing for better performance with the same amount of training data.\n",
    "\n",
    "Analogy of the forest on pg 324 explains why this happens. It boils down to low layers learning simple patterns, middle layers combine these into intermediate structures and high level layers combine these into high level structures.\n",
    "\n",
    "This also improves theis ability to generalize to new datasets. Re-utilizing the weights of pre-trained models or models trained for similar tasks is called *transfer learning*.\n",
    "\n",
    "In summary, start with one or two hidden layers. For more complex problems, you can ramp up the number of hidden layers until you start overfitting the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of neurons per hidden layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As discussed, the number of neurons in the input and output layers is determined by the type of input and output your task requires.\n",
    "\n",
    "For the hidden layers, it used to be common to stack them as a pyramid, with fewer and fewer neurons at each layer - the rationale being that many low-level features can coalesce into far fewer high-level features. In practice, using the same numbers of neurons in each all hidden layers performs just as well or even better in some cases plus, there's only one hyperparameter to tune. With that said, it can sometimes help to make the first hidden layer bigger than the others, depending on the size of the dataset.\n",
    "\n",
    "In practice, it is a good strategy to pick a model with more layers and neurons than you're going to need and use early stopping and other regularization techniques to prevent it from overfitting. The 'stretch pants' approach. \n",
    "\n",
    "In general increasing the number of layers has more expected value than increasing the number of neurons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Rate, Batch Size, other hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Learning Rate*\n",
    "\n",
    "Arguably the most important parameter. In general, the optimal learning rate is about half the maximum learning rate (i.e. the learning rate above which the training algorithm diverges. One good approach is to train the model for a few hundred iterations with a very low learning rate (e.g. $10^{-5}$) and gradually increase it to a large value (e.g. 10). \n",
    "\n",
    "This is done by multiplying the learning rate by a constant factor at each iteration (e.g. by $\\exp(\\log(10^6)/500)$ to go from $1e-5$ to 10 in 500 iterations). If you plot the loss function of the learning rate (using a log scale for the learning rate), you should see it dropping at first. But after a while the learning rate will be too large, so the loss will shoot back up."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Optimizer*\n",
    "\n",
    "More details on chapter 11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Batch Size*\n",
    "\n",
    "This can have a significant impact on the training time and performacee. large batch sizes benefit from hardware acceleators like GPUs and can be trained more instances per second. Thus many researchers and practictioners recommed using the largest batch size that can fit in GPU RAM.\n",
    "\n",
    "Here's the catch: In practice, large batch sizes often lead to training instabilities, especially at the beginning of training and the resulting model may not generalize as well. There's arguments for both small and large batch sizes. The book recommend suggests using a large batch size using learning rate warm-up and if training is unstable or the final performance is disappointing then try using a small batch instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Activation Function*\n",
    "In general, ReLU is a good default for hidden layers. For the output layer, it depends on your task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Number of iterations*\n",
    "In most cases the number of iterations does not actually need to be tweaked: just use early stopping instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note:* The optimal learning rate depends on other hyperparameters (especially batch size) so if you tweak any hyperparameter, update the learning rate as well!\n",
    "\n",
    "For more best practices regarding tuning hyperparameters, read this [paper by Leslie Smith](https://homl.info/1cycle)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
