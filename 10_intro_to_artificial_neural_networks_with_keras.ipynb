{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *Perceptron* is on of the simplest Artificial neural network architectures, proposed in 1957 by Frank Rosenblatt. It is based on a *threshold logic unit (TLU)* and it computes a weighted sum of its inputs\n",
    "\n",
    "$$ z = w_1x_1 + \\cdots + w_nx_n = \\textbf{x}^{\\intercal}\\textbf{w} $$\n",
    "\n",
    "then applies a step function to that sum and outputs the result: $h_w(\\textbf{x})=\\text{step}(\\textbf{x})$. One of the most common step function used is the *Heaviside step function*\n",
    "\n",
    "$$ \\text{heaviside}(z) = \\begin{cases} 0 & \\text{if } z<0 \\\\ 1 & \\text{if } z\\gt0 \\end{cases}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A single TLU can be used for binary classification; it computes a linear combination of its inputs and if the output reaches a threshold, it outputs a positive class, otherwise outputs the negative class.\n",
    "\n",
    "A perceptron is composed of a single layer of TLUs, with each TLU connected to all the inputs. When all the neurons in a layer are connected to every neuron in the previous layer the layer is called a *fully connected* or *dense* layer. *Input Neurons* are simple inputs that output whatever they are fed and all input neurons form the *input layer*. A bias neuron is generally added, tipycally represented by a *bias neuron*, which outputs 1 all the time. (e.g. architecture pg 286 fig 10-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then write the outputs of a fully connected layer as \n",
    "$$ h_{\\textbf{W, b}}(\\textbf{X}) = \\phi(\\textbf{XW + b})$$\n",
    "Where\n",
    "- $\\textbf{X}$ is the matrix of input features (one row per instance, one col per feature)\n",
    "- $\\textbf{W}$ contains the connection weights, except the ones from the bias neuron (one row per input neuron, one column per artificial neuron in the layer)\n",
    "- $\\phi$ is called the *activation function* (when the neurons are TLU, this is a step function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The perceptron learning rule reinforces connections between neurons tha help reduce the error: the perceptron is fed one training instance at a time, and for each instance it makes its predictions. For every output neuron that produced a wrong predictions, it reinforces the connection weights from the inputs that would have contributed to the correct prediction\n",
    "\n",
    "$$ w_{i,j}^{\\text{next step}} = w_{i,j} +\\eta(y_j - \\hat{y_j})x_i$$\n",
    "\n",
    "where \n",
    "- $w_{i,j}$ is the weight between ith input neuron and jth output neuron\n",
    "- $x_i$ is the ith input value of the current training instance\n",
    "- $\\hat{y_j}$ is the output of the jth output neuron \n",
    "- $y_j$ is the target output of the jth ouptut neuron\n",
    "- $\\eta$ is the learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The multilayer perceptron and backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An MLP consistis of one input layer, one or more layers of TLUs (called *hidden layers*) and one final layer of TLUs called the *output layer*. Every except the output layer includes a bias neuron and is fully connected to the next layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train an MLP, we use [backpropagation](https://homl.info/44). In short, it is Gradient Descent and it is able to compute the gradient of the network's error with regard to every single model parameter, thus it is able to find out how much it should tweak each connection weight and bias in order to reduce the error. This process is called *autodiff*, appendix D has more info on it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's how it works\n",
    "- It handles one mini-batch at a time (e.g. 32 instances) and goes through the training set multiple times, each pass is called an *epoch*\n",
    "- Each mini-batch is passed is passed to the network's input layer, which sends it to the first hidden layer. The algorithm then computes the outputs of this layer and passes it to the next layer, and so on, until we get the output of the output layer. This is called a *forward pass* and the intermediate results are saved\n",
    "- Next we calculate the network's output error (using some loss function)\n",
    "- Then it computs how much each output connection contributed to the error (done using chain rule)\n",
    "- The algorithm then measures how much of these error contributions came from each connection in the layer below until it reaches the input layer\n",
    "- Finally, it performs a gradient descent step to tweak all the connection weights in the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One change that had to be made to the original MLP architecture was replacing the step function with the logistic function $\\sigma(z) = 1 / (1 +\\exp(-z))$, this allows for gradients to be computed as it is a smooth function.\n",
    "\n",
    "Some other choices of function are:\n",
    "- Hyperbolic tan $\\tanh(z) = 2\\sigma(2z) - 1$\n",
    "\n",
    "Another S-shaped function, continues and differentiable. Its outputs are in the range -1 to 1, making each layer's output more or less centered around 0 at the beginning of training, which helps speed up convergence.\n",
    "\n",
    "- Rectified Linear unit $ReLU(z) = \\max(0,z)$\n",
    "\n",
    "Continuous but not differentiable at $z=0$, however it works very well and has become the default."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Activation functions are useful because they can add non-linearity to each layer. Recall that a linear transformation of linear transformations is also linear. Using a non-linear function allows for an MLP to learn more complex patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression MLPs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use MLP for regression we use an output neuron for each value we want to predict. In the univariate case (e.g. predicting house price) only a single output neuron is needed. \n",
    "\n",
    "For multivariate problems, you need one output neuron per output dimension. For example to locate the center of an object in an image, you need to predict 2D coordinates, thus 2 output neurons. If you also want to place a bounding box around the object, you need two more numbers, the width and height of the object. In total, 4 output neurons.\n",
    "\n",
    "In general we do not want to use any activation function for output neurons so they are free to output any range of values. To guarantee the range of values is always positive, use ReLU or *softplus*, which is a smooth variant of ReLU: $\\text{softplus}(z) = \\log(1 + \\exp(z))$. \n",
    "Finally if we want to guarantee the predictions will fall between a range of values we can use the logistic or hyperbolic tangent function, scaling the labels to the appropriate values.\n",
    "\n",
    "The typical loss function used is MSE, however if you have a lot of outliers in your training set you may want to use the mean absolute error instead. Alternatively use [Huber loss](https://en.wikipedia.org/wiki/Huber_loss), which is a combination of both.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typical regression MLP architecture\n",
    "\n",
    "| Hyperparameter | Typical value |\n",
    "|     ---        |      ---      |\n",
    "|# input neurons | One per input feature (e.g. 28x28=784 for MNIST) | \n",
    "|# hidden layers | Variable (typically 1 to 5) |\n",
    "|# neurons per hidden layers | Variable (typically 10 to 100) |\n",
    "|# output layer | 1 per prediction dimension |\n",
    "|Activation function | $\\begin{cases} \n",
    "                        \\text{None} & \\text{ for any range of values } \\\\ \n",
    "                        ReLU/\\text{softplus} & \\text{ positive outputs }\\\\\n",
    "                        \\text{logistic/tanh} & \\text{ bounded outputs}\n",
    "                        \\end{cases}$ |\n",
    "|Loss Function | MSE or MAE/Huber (if outliers)|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification MLPs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For binary classification, we just need a single output neuron using the logistic activation function. The output will be in the range 0 - 1 and we can interpret it as an estimated class probability of the positive class. The estimated probability for the negative class is one minus that number."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLPs can also be used for multilabel binary classification. For example, in an e-mail classification system that tags messages as spam/ham and urgent/non-urgent we would use two output neurons with the logistic function. The first outputs the probability that the e-mail is spam and the second the probability the e-mail is urgent. \n",
    "More generally, we use one output neuron for each positive class.\n",
    "\n",
    "For multiclass calssification (e.g. identifying digit classes 0 through 9), then we need one output neuron per class and should use the softmax activation to ensure estimated probabilities are between 0-1 and they add up to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the loss function, since we're preducting probability distributions, the cross-entropy loss is generally a good choice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typical classification MLP architecture\n",
    "\n",
    "| Hyperparameter | Binary Classification | Multilabel Binary Classification | Multiclass Classification | \n",
    "| --- | --- | --- | --- |\n",
    "|# input neurons | One per input feature | One per input feature | One per input feature |\n",
    "|# hidden layers | Variable (typically 1 to 5) | Variable (typically 1 to 5) | Variable (typically 1 to 5) |\n",
    "|# neurons per hidden layers | Variable (typically 10 to 100) | Variable (typically 10 to 100)| Variable (typically 10 to 100)|\n",
    "|# output neurons | 1 | 1 per label | 1 per class |\n",
    "|Activation Function| Logistic | Logistic | Softmax |\n",
    "|Loss Function | Cross Entropy | Cross Entropy | Cross Entropy |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1\n",
    "Play around in the [Tensorflow Playground](https://playground.tensorflow.org) to get a better feeling for ANNs and explore the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing MLPs with Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building an Image Classifier using the Sequential API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this chapter, we will tackle Fashion MNIST, which is a drop-in replacement of MNIST. The images represent fashion items instead of digits, so each class is more diverse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_full.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data is loaded as ints in the range from 0 to 255. Let's create a validation set and scale the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('uint8')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_full.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid, X_train = X_train_full[:5000]/255.0, X_train_full[5000:]/255.0\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 0, 7, ..., 3, 0, 5], dtype=uint8)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to map the target values to their actual class as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the model using the Sequential API\n",
    "We'll start by creating a classification MLP with two hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten, Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=[28,28])) # Converts inputs to 1D array\n",
    "model.add(Dense(300, activation='relu'))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Sequential API is the simplest kind of model for NNs that are just composed of a single stack of layers connected sequentially. Another way to write the same model could be as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Flatten(input_shape=[28,28]),\n",
    "    Dense(300, activation='relu'),\n",
    "    Dense(100, activation='relu'),\n",
    "    Dense(10, activation='softmax'),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can vew a definition of the model by using summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_2 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 300)               235500    \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 100)               30100     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 266,610\n",
      "Trainable params: 266,610\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the model has a lot of parameters, it has a lot of flexibility to train the data. However this also means that it runs the risk of overfitting, especially when we don't have much training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can view a model's Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.layers.core.Flatten at 0x7f6d2b75eb80>,\n",
       " <keras.layers.core.Dense at 0x7f6d2b75eb20>,\n",
       " <keras.layers.core.Dense at 0x7f6d2b75ebb0>,\n",
       " <keras.layers.core.Dense at 0x7f6d1117f3d0>]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dense_4'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden1 = model.layers[1]\n",
    "hidden1.name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As well as it's weights and biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.01190935, -0.02662365,  0.05324669, ...,  0.0494841 ,\n",
       "        -0.06483872,  0.07305315],\n",
       "       [-0.00893984, -0.06924371, -0.07077597, ..., -0.04943123,\n",
       "         0.03560346,  0.05264443],\n",
       "       [ 0.03196555,  0.02825633, -0.04930341, ..., -0.00858872,\n",
       "         0.01880597,  0.01205157],\n",
       "       ...,\n",
       "       [-0.04694034,  0.06772318,  0.0301396 , ...,  0.06040362,\n",
       "        -0.05731079,  0.05229205],\n",
       "       [-0.05691219, -0.05847605,  0.00148755, ..., -0.07146315,\n",
       "        -0.02082144,  0.01556862],\n",
       "       [-0.03562712, -0.01602892, -0.03188283, ..., -0.01397945,\n",
       "        -0.06196937, -0.03350033]], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights, biases = hidden1.get_weights()\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the weights are initialized randomly (to break simmetry) and the biases set to zero. To use other initialization methods we can set the ```kernel_initializer``` or ```bias_initializer``` when creating the layer.\n",
    "\n",
    "Next we compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss = 'sparse_categorical_crossentropy',\n",
    "              optimizer = 'sgd',\n",
    "              metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use ```sparse_categorical_crossentropy``` because we have sparse labels (i.e. for each instance there is only a single target class) and the classes are exclusive. If instead we had one target probability per class for each instance  (such as one-hot vectors for a single class) we'd use ```categorical_crossentropy``` instead. \n",
    "\n",
    "The optimizer set to ```sgd``` means we'll train the model using simple stochastic gradient descent. \n",
    "\n",
    "Finally, we can fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "RELOAD = True\n",
    "CHAPTER_DIR = 'saved_models/10_intro_to_anns/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 55000 samples, validate on 5000 samples\n",
      "Epoch 1/30\n",
      "55000/55000 [==============================] - 5s 93us/step - loss: 0.7088 - accuracy: 0.7669 - val_loss: 0.5175 - val_accuracy: 0.8220\n",
      "Epoch 2/30\n",
      "55000/55000 [==============================] - 5s 88us/step - loss: 0.4851 - accuracy: 0.8312 - val_loss: 0.4464 - val_accuracy: 0.8480\n",
      "Epoch 3/30\n",
      "55000/55000 [==============================] - 5s 88us/step - loss: 0.4431 - accuracy: 0.8444 - val_loss: 0.4512 - val_accuracy: 0.8406\n",
      "Epoch 4/30\n",
      "55000/55000 [==============================] - 5s 87us/step - loss: 0.4159 - accuracy: 0.8541 - val_loss: 0.4172 - val_accuracy: 0.8556\n",
      "Epoch 5/30\n",
      "55000/55000 [==============================] - 5s 95us/step - loss: 0.3968 - accuracy: 0.8618 - val_loss: 0.3883 - val_accuracy: 0.8690\n",
      "Epoch 6/30\n",
      "55000/55000 [==============================] - 5s 92us/step - loss: 0.3801 - accuracy: 0.8651 - val_loss: 0.3769 - val_accuracy: 0.8692\n",
      "Epoch 7/30\n",
      "55000/55000 [==============================] - 5s 95us/step - loss: 0.3681 - accuracy: 0.8697 - val_loss: 0.3661 - val_accuracy: 0.8738\n",
      "Epoch 8/30\n",
      "55000/55000 [==============================] - 5s 96us/step - loss: 0.3556 - accuracy: 0.8739 - val_loss: 0.3660 - val_accuracy: 0.8742\n",
      "Epoch 9/30\n",
      "55000/55000 [==============================] - 5s 97us/step - loss: 0.3466 - accuracy: 0.8757 - val_loss: 0.3579 - val_accuracy: 0.8768\n",
      "Epoch 10/30\n",
      "55000/55000 [==============================] - 5s 92us/step - loss: 0.3361 - accuracy: 0.8792 - val_loss: 0.3501 - val_accuracy: 0.8774\n",
      "Epoch 11/30\n",
      "55000/55000 [==============================] - 5s 95us/step - loss: 0.3280 - accuracy: 0.8817 - val_loss: 0.3618 - val_accuracy: 0.8692\n",
      "Epoch 12/30\n",
      "55000/55000 [==============================] - 5s 97us/step - loss: 0.3199 - accuracy: 0.8850 - val_loss: 0.3339 - val_accuracy: 0.8798\n",
      "Epoch 13/30\n",
      "55000/55000 [==============================] - 5s 93us/step - loss: 0.3131 - accuracy: 0.8864 - val_loss: 0.3351 - val_accuracy: 0.8802\n",
      "Epoch 14/30\n",
      "55000/55000 [==============================] - 5s 93us/step - loss: 0.3051 - accuracy: 0.8901 - val_loss: 0.3322 - val_accuracy: 0.8836\n",
      "Epoch 15/30\n",
      "55000/55000 [==============================] - 5s 94us/step - loss: 0.2994 - accuracy: 0.8923 - val_loss: 0.3276 - val_accuracy: 0.8824\n",
      "Epoch 16/30\n",
      "55000/55000 [==============================] - 5s 94us/step - loss: 0.2925 - accuracy: 0.8948 - val_loss: 0.3504 - val_accuracy: 0.8750\n",
      "Epoch 17/30\n",
      "55000/55000 [==============================] - 5s 95us/step - loss: 0.2861 - accuracy: 0.8970 - val_loss: 0.3317 - val_accuracy: 0.8812\n",
      "Epoch 18/30\n",
      "55000/55000 [==============================] - 5s 96us/step - loss: 0.2813 - accuracy: 0.8979 - val_loss: 0.3113 - val_accuracy: 0.8892\n",
      "Epoch 19/30\n",
      "55000/55000 [==============================] - 5s 97us/step - loss: 0.2758 - accuracy: 0.9011 - val_loss: 0.3176 - val_accuracy: 0.8872\n",
      "Epoch 20/30\n",
      "55000/55000 [==============================] - 5s 97us/step - loss: 0.2705 - accuracy: 0.9028 - val_loss: 0.3373 - val_accuracy: 0.8792\n",
      "Epoch 21/30\n",
      "55000/55000 [==============================] - 5s 97us/step - loss: 0.2671 - accuracy: 0.9031 - val_loss: 0.3066 - val_accuracy: 0.8870\n",
      "Epoch 22/30\n",
      "55000/55000 [==============================] - 5s 95us/step - loss: 0.2609 - accuracy: 0.9056 - val_loss: 0.3218 - val_accuracy: 0.8832\n",
      "Epoch 23/30\n",
      "55000/55000 [==============================] - 5s 100us/step - loss: 0.2575 - accuracy: 0.9067 - val_loss: 0.3153 - val_accuracy: 0.8816\n",
      "Epoch 24/30\n",
      "55000/55000 [==============================] - 5s 92us/step - loss: 0.2524 - accuracy: 0.9088 - val_loss: 0.3222 - val_accuracy: 0.8802\n",
      "Epoch 25/30\n",
      "55000/55000 [==============================] - 5s 93us/step - loss: 0.2486 - accuracy: 0.9086 - val_loss: 0.3021 - val_accuracy: 0.8922\n",
      "Epoch 26/30\n",
      "55000/55000 [==============================] - 5s 95us/step - loss: 0.2443 - accuracy: 0.9112 - val_loss: 0.3030 - val_accuracy: 0.8900\n",
      "Epoch 27/30\n",
      "55000/55000 [==============================] - 5s 98us/step - loss: 0.2403 - accuracy: 0.9131 - val_loss: 0.3198 - val_accuracy: 0.8862\n",
      "Epoch 28/30\n",
      "55000/55000 [==============================] - 5s 96us/step - loss: 0.2361 - accuracy: 0.9151 - val_loss: 0.3168 - val_accuracy: 0.8882\n",
      "Epoch 29/30\n",
      "55000/55000 [==============================] - 5s 97us/step - loss: 0.2320 - accuracy: 0.9163 - val_loss: 0.3087 - val_accuracy: 0.8912\n",
      "Epoch 30/30\n",
      "55000/55000 [==============================] - 5s 96us/step - loss: 0.2291 - accuracy: 0.9178 - val_loss: 0.3311 - val_accuracy: 0.8840\n"
     ]
    }
   ],
   "source": [
    "if RELOAD:\n",
    "    model = keras.models.load_model(CHAPTER_DIR + 'model1.h5')\n",
    "    history = model.history\n",
    "else:\n",
    "    history = model.fit(X_train, y_train, epochs=30, validation_data=(X_valid, y_valid))\n",
    "    model.save(CHAPTER_DIR + 'model1.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: instead of passing a validation set, you can also set the ```validation_split``` argument of the ```fit``` method to the ratio of the training set you want Keras to use for validation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the training set is very skewed, with some classes being underrepresented, it can be useful to set the ```class_weight``` parameter, this would give larger weight to underrepresented classes and a lower weight to overrepresented classes.\n",
    "\n",
    "If you need per instance weights, the ```sample_weight``` can be used. Per-instance weights can be useful if some instances are labelled by experts while others were labeled using a crowdsourcing platform: we might want to give more weights to the former."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can access the ```history.history``` to access the loss and accuracy of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from jupyterthemes import jtplot\n",
    "jtplot.style()\n",
    "\n",
    "pd.DataFrame(history.history).plot(figsize=(10, 5))\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0,1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the model performed better on the validation set than on the training set at the beginning, but this is not true.\n",
    "\n",
    "The validation error is computed at the *end* of each epoch while the training error is computed using a running mean *during* each epoch. So the training curve should actually be shifted half an epoch to the left. \n",
    "\n",
    "We can also tell that the model has not quite converged yet as the validation is still going down. We can continue training from where we left off by calling the fit method again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 55000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "55000/55000 [==============================] - 4s 78us/step - loss: 0.2244 - accuracy: 0.9194 - val_loss: 0.3026 - val_accuracy: 0.8912\n",
      "Epoch 2/20\n",
      "55000/55000 [==============================] - 4s 80us/step - loss: 0.2214 - accuracy: 0.9203 - val_loss: 0.2965 - val_accuracy: 0.8966\n",
      "Epoch 3/20\n",
      "55000/55000 [==============================] - 4s 80us/step - loss: 0.2179 - accuracy: 0.9216 - val_loss: 0.2981 - val_accuracy: 0.8938\n",
      "Epoch 4/20\n",
      "55000/55000 [==============================] - 5s 83us/step - loss: 0.2142 - accuracy: 0.9229 - val_loss: 0.3025 - val_accuracy: 0.8952\n",
      "Epoch 5/20\n",
      "55000/55000 [==============================] - 4s 76us/step - loss: 0.2112 - accuracy: 0.9236 - val_loss: 0.3011 - val_accuracy: 0.8954\n",
      "Epoch 6/20\n",
      "55000/55000 [==============================] - 4s 81us/step - loss: 0.2084 - accuracy: 0.9249 - val_loss: 0.2922 - val_accuracy: 0.8948\n",
      "Epoch 7/20\n",
      "55000/55000 [==============================] - 4s 81us/step - loss: 0.2041 - accuracy: 0.9277 - val_loss: 0.2922 - val_accuracy: 0.8964\n",
      "Epoch 8/20\n",
      "55000/55000 [==============================] - 4s 77us/step - loss: 0.2017 - accuracy: 0.9280 - val_loss: 0.3005 - val_accuracy: 0.8932\n",
      "Epoch 9/20\n",
      "55000/55000 [==============================] - 4s 79us/step - loss: 0.1979 - accuracy: 0.9291 - val_loss: 0.2999 - val_accuracy: 0.8902\n",
      "Epoch 10/20\n",
      "55000/55000 [==============================] - 4s 79us/step - loss: 0.1945 - accuracy: 0.9304 - val_loss: 0.2991 - val_accuracy: 0.8966\n",
      "Epoch 11/20\n",
      "55000/55000 [==============================] - 4s 80us/step - loss: 0.1922 - accuracy: 0.9327 - val_loss: 0.3234 - val_accuracy: 0.8862\n",
      "Epoch 12/20\n",
      "55000/55000 [==============================] - 4s 79us/step - loss: 0.1885 - accuracy: 0.9324 - val_loss: 0.2942 - val_accuracy: 0.8952\n",
      "Epoch 13/20\n",
      "55000/55000 [==============================] - 4s 82us/step - loss: 0.1862 - accuracy: 0.9339 - val_loss: 0.3026 - val_accuracy: 0.8956\n",
      "Epoch 14/20\n",
      "55000/55000 [==============================] - 4s 80us/step - loss: 0.1833 - accuracy: 0.9341 - val_loss: 0.3026 - val_accuracy: 0.8924\n",
      "Epoch 15/20\n",
      "55000/55000 [==============================] - 4s 79us/step - loss: 0.1794 - accuracy: 0.9358 - val_loss: 0.2962 - val_accuracy: 0.8952\n",
      "Epoch 16/20\n",
      "55000/55000 [==============================] - 4s 78us/step - loss: 0.1781 - accuracy: 0.9376 - val_loss: 0.2960 - val_accuracy: 0.8942\n",
      "Epoch 17/20\n",
      "55000/55000 [==============================] - 4s 77us/step - loss: 0.1747 - accuracy: 0.9387 - val_loss: 0.3024 - val_accuracy: 0.8938\n",
      "Epoch 18/20\n",
      "55000/55000 [==============================] - 4s 79us/step - loss: 0.1719 - accuracy: 0.9390 - val_loss: 0.2927 - val_accuracy: 0.8974\n",
      "Epoch 19/20\n",
      "55000/55000 [==============================] - 4s 79us/step - loss: 0.1684 - accuracy: 0.9406 - val_loss: 0.3012 - val_accuracy: 0.8914\n",
      "Epoch 20/20\n",
      "55000/55000 [==============================] - 5s 86us/step - loss: 0.1660 - accuracy: 0.9413 - val_loss: 0.2997 - val_accuracy: 0.8974\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlYAAAE1CAYAAAAlLa52AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nO3deXwcdeH/8ddn9kiySZrehwXaSilHQUX9IlBaDr9fBBQBqZSr3CIoiAiCiCeKHAIKiHw9gMIXkFNQRED9AS3laLnlLAi0XKVN2jTXZs+Z3x+zszu72SSbdJI2yfv5aLqzM5+Znfns7Mx7PzM7Yz69+34OIiIiIrLRrE09AyIiIiLDhYKViIiISEAUrEREREQComAlIiIiEhAFKxEREZGAKFiJiIiIBETBSkRERCQg4UoK7brrLuz/hX2ZPn0rWtvaOP1bZ3Vb1rIsFh59BPPmzsEYw7Llz3D9DTeRTqcDm2kRERGRzVFFLVYdHR089I9/ctvtd/Va9pCDD2T27O05+5zzOePMc9hi6lSOOnLBRs+oiIiIyOauoharl156BYDPfvbTvZbdZ++9uOXW22hubgbgrrvv4dtnnMaNN92C45S/yHusto50OlXpPIuIiIhsMpFIlHhHe9lhFQWrSsViMcaPH8fKVe/m+739zkpisRomTpjAmrVru45TW8dhC08NcjZEREREBtQd/3dt2XAVaLCqqa4GoKMjnu8Xj7vd1TXVZcfxWqru+L//HdBWq7pRo2lv3TBg0x9KVBcFqguX6qFAdVGguihQXbhUD25r1WELT+k2swQarDoTCQBisRpaWlpy3TEAEp2JHsdNp5OkUwMVrAyZdDo3/ZF+z2nVRYHqwqV6KFBdFKguClQXLtWDq+dlD/RyC/F4nKamdUyfNi3fb8b0acTjnaxtbAzypUREREQ2OxUFK2MMkUiEcCiEIdcdLt/Y9fAjj3LwQV9izJjR1NfXM3/+ISxe8li3J66LiIiIDBcVHQqcN3cO3zj15Pzzm2+6jrWNjZz+rbM46cTjAPjjdYsAuOfe+6ivr+eySy/CsgxPLXuaW269Pej5FhEREdnsVBSsFi9ZyuIlS8sO8wKVx7ZtFt14M4tuvHmjZ05ERERkKNEtbUREREQComAlIiIiEhAFKxEREZGAKFiJiIiIBETBSkRERCQgClYiIiIiAVGwEhEREQmIgpWIiIhIQAK9CbOIiIgIgDEWlrEwxsJgcP8ZjDHkuvL9MMbrU+j2yhnjTs9fzuCbRnG/ptYP2ZQ3iVawEhGREaWwwzfFO3+vGwvLcsOA+xiitmYUVU5NYUfu2/kbvGl5/ayiUFDUv9z4uf5ev1y6KMyv121KnmMK/5viIfimU+iXe+6bTtFyd6mPkudYhMNhHNvpUq64Dt3xBorjODg4kHv07kXsPjo8+OwibCc7YK/fGwUrEZEhweR3WEU7M0p3gL4ymPyOrrCj9abWtavQx+1XE6unmlrcb//lynkdxSGgdGdb+uif1679Sh9NL9Po6bXKB6e+cBwbO7fDdhzb3ak7Dg7+bse3s7e79HPL2b5ub3pdp+OFheJ58J77hjj5/4qHOF3Gxikq62B7zx0HO79Mdq7bfW5j49g2DsX9q6pjxONt2E62/Hj5brswbeyuy5efT/+yUyYw5cqVqZfNlYKViASu6Jt4l2/0VvfDfGHA62dhQW7HaVkh99GEsCz30XTT392ZFj/3ynn9QybkK5MrFwrh2HZ+WbwNftFz3zOgZMdFfkfQpZy/khwn13KR+4ZPuW//pl9hoPASbihwHNvft2gWSvv5+xhj8js3f7HyO7hCXbghwsa23TBh2zaOky1054cVyhTvoG1sJ0vWTpfdaXfdeZcfXm4nX25ct46yZafhMtQ3jKGtpblsXY0cqodKKFiJdMPboYWscG6HHSJkhfLdlhUilO8O57otLBPuWtZY5NsBCl/zKXpa1NxfOqxQxpSM3+VZ2UMTXtN8aT+Ta96PYNtZ3/kK5ccvPe/BCwH4WkY29hBA8bf7wrd528m6f7ad67ax7WxuB1jav9CdyaTcfr2O4z5Wx2pJxDso7DjKvE9FLT/+J+XeN//hmuKWHcexsfPLWbrj97dm+L79ezv+nsbL1d/G0U5UpD8UrEaAouP3JkTYihIN12AMXc4JKN964D/2751g6HVTcqJh8QmGXpniEwy9HXOutFe23E6/SwuGf566n2fvMAC+7kL5Qnc4HMU4ptCqkQtDISucX55yvJ2wbWfJOhn3uZ0l62Rzw3LPvW7HLmrB6LLT69J8nyvVpUXBa8YHsH2D/C0ihcMN5Q5D0KUfVFVX09nZkfuGXjq+3WVa7iwX7/zdwyWFFpKuh0y6hqWiwyCBhIGNZai3FSZEpP9GRLCqqapnXO0UYmaUGyaw3C+dJScMlj/ZsMwOv5vxKQ0FPR3yKDnc0fOhknJBp6dDLF7rwcafPNjtTtl3PNx/qCA/LF+OkmPqheP9ZU8+9O/US3fERTvl4mHuoYbcYYdciPG6u57fUJhetKqaeEdrIRyVBiJ/WMr1y9pZhtdOVy0TIiJBGRHBamLDlmz/sc9h29mib97+kFB8QqHtRgPvG3TZMr5+ZcoUN+FncexuAkLZsFDa3cu3/QpaAQqvB7G6etpbN+CdB1G2ZWEInSjYfwoUIiISrBERrFatfZX1ydXagQJgsKosOlPtqC5ERESCpSuvi4iIiAREwUpEREQkIApWIiIiIgEZEedYDRZjOUSiDhiwswY7C3YWyl2NSEYCx/0Vqnt9S7JZwNG60DcOoTCEwg6ppFH9iVSs8NlxH/3d7qPjQDppSKdyj0lDNgNDZZ9ljEOkCiJV7r43XOUQrXL48K0wm3IZFKxwsEK4b0qU/JvjPQ9H3e5Irtt7ni/rGxbqpjazGTdgZX1hK5spdNtZQzZbCGNu+Vx31h/STH5a3jj+HbdlOfnuwmNh527lHquqm0mnE/lhlr+sb3wrPx23fxFTZrUtvp1VlwLdXRaq0N9/RWiTe6Tw6F0A2yk894Z1W75kvOJpGaJVWTKZVFHd5bstr258zw2YEFi+fsaAFSqUs3z9SnnvbzZrsDPuepD1rRPee5/N4A7PFvfz1pustw75y3jTyhTWNX/5TRdK3M9LtMohUu0QyW38IlWFjWK02nvuDcttLKsdQrl6tG1IdBgSHYbOdotEu6Gzw5Bot+hsz/XrMGQzm89OIRRxqI45VMXs3KP753WHwk7xZ6nkNnH+z4zb7XQpkx9WOm5JmdL1obf1J5sxRKPtjGpLF7Zh/nWwS3l865j7WQiFwAq529hQuNBthSDk7w573U5unK7dxeXc8THkPucmtx0A75Jx/m1Afrh36TdfGbecKXqev+ycb/yq6lYSnUlv8Yq3Kf7uwiXnfP1N1/7lpuNejN+tq7C7/KFIaUCq9LH8Oul/H43l7sf82yrb7hq2MklDOgnplIUxG+hoSRaXSZl82XTKvx70xJ3PcLTwmff2vRH/9sH/PFoSpKLFU3Tn0bBmVTgXEDeNERGsJm6VYetPNIJJFYUj740p3QGmU5BJGTIpk+9OpwyZJKQ6DfEWy32eIvfo6067H6BQDxuDchsTd1jxRigScagKQShklxnHLYvtfuhtu3iD4NjufNi2v5/bPxTOEEravvIGx4Zs2hSV87ptf0gpVXynC19HcdDpMlppf990vB2CKdq5OPmdR75fSRlTsjMyxikq03WakM1YpDpNvs68+rL99ZirAyfrq2tfndpemZJ69k8LCu9fKARW2F0XijaguR2Pt3H01pdIlfe8ZJh/WuHCTqg7/qBe2FGaXKtqknQyk/8CUBTwisbxjZs1hMKFUBT1NnilASpKUTDPb4wT7ucqlXA32sm4oa3ZyvdPJwypJLlv0YaqmENNrU11nUNNrUN1nc2o8TY1tWmq6xzCEXf6qQQkOtyw5YavXHeHF8AMqU7fytRHobA/HNlud61DVU3u0esXK8yTnYVE3JDM/SXiFm3rLTJp34S72xHT3c665ItGmf7+6eTXn5CTW/9y617YDb2hWqfLuhiJZsHKuP288cLdf0mys+68uF80uq/DwpdK/xfOwpeMfHfG399dD9LJwnrrOL4vkd7n2zhFz711z1i+L0DGtw2x/P2c4v4Uph8Kx7Hd9Nh9uM316y7gdilDbn5LpuV9ySr3mOo0+c9oNu3VkyGTdusuk/Z9aSvz6HQJPbnGhdznNewLLoVA4/aritnU1KYYPTFTFHa6BBxfKEsn3f2j9xr+wFQU6LLkAlrXsJbfNnghLwWpfODL/aXZbFq0R0SwsrOQ7AwRb7eKw1KyTHjajN6cgaFrNxUMw7owhYDlD2D54J4PYLlQlysbq6shk45j+QKdFYJwxMGq9gU7/zTCkE0XNqCppKGjxXI3eglTtGH1wlM6ZXDs/n2+4q3QTHfJ0d1YV9fa1NQ51NQ5+e6GCVkmT3eornPDDrjbBLe1yw1eiXyLl0U0GmfcFimqagutTP4QFcntRNztiikEpg5349/0fqhLiEonYagcXino7vORa43yhzTfOmFZTlEQL215H5qHxIfhtiLPfV/c9bX3suXqwT0Nxg1O4WjhS1U4WghTdobilq0hfAiyNyMiWDV9ECbZPlw/FCI+jvft1N1gVcZQ3zCKtpahfEV575tuiLb1PZSy3Falmjo7F77c7upah9ETM1TX2UQiKTo7HDcUdRg6Nlis+9ALSV5gskgl3NcdedwW2kwKMiNy+aWUYxtSCfcLlYyQYCUiAu4OoLPN0NnW3Q+ih3PLhIgMBl1uQURERCQgClYiIiIiAVGwEhEREQmIgpWIiIhIQBSsRERERAKiYCUiIiISEAUrERERkYAoWImIiIgERMFKREREJCAKViIiIiIBUbASERERCYiClYiIiEhAFKxEREREAqJgJSIiIhIQBSsRERGRgChYiYiIiAREwUpEREQkIApWIiIiIgFRsBIREREJSLiSQpZlsfDoI5g3dw7GGJYtf4brb7iJdDrdpezo0Q2ccNwxbL/DdhgMr7++gusX3cT69c2Bz7yIiIjI5qSiFqtDDj6Q2bO35+xzzueMM89hi6lTOerIBWXLnnjCsYTDYU7/1ll847Rvk0wmOeXrJwU60yIiIiKbo4qC1T5778W9995Hc3MzbW1t3HX3Pew5by7GmC5lJ02ayFPLlpNIJEilUix94kmmbbVlBa9iBvBvsF5nKPypLlQXqgfVhepCdaF6CK4uivV6KDAWizF+/DhWrno33+/td1YSi9UwccIE1qxdW1T+/vsf5HOf+y+eefZ5bNtm3h5zePa5F3p7GepGjSZT5tBikOobxgzo9IcS1UWB6sKleihQXRSoLgpUF66RXg/hSKTn4b1NoKa6GoCOjni+XzzudlfXVHcp//qKN9h7r3lc94ffArDq3fe48BeX9jqj7a0bSKdSvZbrr/qGMbS16DwvUF34qS5cqocC1UWB6qJAdeFSPUAkGu1xeK/BqjORACAWq6GlpSXXHQMg0ZkoKmuM4Qfnn8vy5c9y0SWXY9s2B335i/z4R+dx7vd+SDab7eGVnNzfQPA32w3UawwVqosC1YVL9VCguihQXRSoLlyqB1fPy97rOVbxeJympnVMnzYt32/G9GnE452sbWwsKltXV8vECRN48MF/kEwmSafT/O3+B9lyiy2YNGliPxdAREREZGio6OT1hx95lIMP+hJjxoymvr6e+fMPYfGSx3Cc4tTW1tbO6tUfse++/00kEiEUCnHA/vvS3t5OY2PTgCyAiIiIyOaioutY3XPvfdTX13PZpRdhWYanlj3NLbfeDsBJJx4HwB+vWwTALy//NccsPJJrr7kSYwzvvf8+l1x6RdlrXomIiIgMJxUFK9u2WXTjzSy68eYuw7xA5fnggw+56OLLApk5ERERkaFEt7QRERERCYiClYiIiEhAFKxEREREAqJgJSIiIhIQBSsRERGRgChYiYiIiAREwUpEREQkIApWIiIiIgFRsBIREREJiIKViIiISEAUrEREREQComAlIiIiEhAFKxEREZGAKFiJiIiIBETBSkRERCQgClYiIiIiAVGwEhEREQmIgpWIiIhIQBSsRERERAKiYCUiIiISEAUrERERkYAoWImIiIgERMFKREREJCAKViIiIiIBUbASERERCYiClYiIiEhAFKxEREREAqJgJSIiIhIQBSsRERGRgChYiYiIiAREwUpEREQkIApWIiIiIgFRsBIREREJSHhTz4CIiMjmLGRZVFdXkUlVA86mnp1NyIyIekin02Qy2X6Pr2AlIiLSjfHjxmDbNvH2VoZzmKiMMyLqoTYWo6oqSjKZpKW1vc/jK1iJiIiUEbIsbNtmfXMLViiEne1/K8ZwMRLqobMzAbih2hiD4/QtSOocKxERkTKiVVE6E8lNPRuyicTjnVRXV/V5PAUrERERkRL9PeCpYCUiIiISEAUrERERkYAoWImIiIgERMFKRERkBJp/6CGc893vBFZOXApWIiIiIgHRdaxEREQqZiBSOzgvle5guF+McziqKFhZlsXCo49g3tw5GGNYtvwZrr/hJtLpdNnyO+/8SRZ89VCmTJlCItHJ3+5/kPv+9vdAZ1xERGTQRWoJfWZwDotln70C0j1f+fuA/b/ALv/1GX5ywS/y/Xbe+ZOcfNLx/PDHP+Pkr53AjBnTsYzFW2+/zfU33MRHH63ZqPmaNHEixx+/kG1mbk083skjjy7mnnvvw3EcamtrOeXkE9lhh+0wxtDY2MRVv7mWDz74kJ12ms3Co45gwoQJpNNp/v3SS/zmmt9t1LxsjioKVoccfCCzZ2/P2eecTyaT4Zyzz+SoIxew6Mabu5T9xE47cvJJx3PNb3/Pq6+9TlVVlPHjxgU+4yIiIoMu3eEGnkF6rd4sffwJjjpyARMmjKexsQmAeXP34LGlT2Aw/P2Bh3jlldewLIsTTziW0755Cj/44U/7PUuWZXHuOd/h+Rf+zeVXXMX48eM479yzicc7efChf3Lgl/YnFA7zjdO+TSqVZurUj9He7i7HN049mT/ddidLliwlEonw8Y/P6Pd8bM4qOsdqn7334t5776O5uZm2tjbuuvse9pw3F2NMl7KHHXYod9/zF15+5VVs26azM8F7739QwauYAfwbrNcZCn+qC9WF6kF1obroXz0AOG4r0mD8VXAYsLW1jRdffIl5c+cAUFNTw2c/szOLlyylsamJF174N+l0mmQyyZ13/pltZm5NVVW01+l2Z+bWH2fcuLH86bY7SKfTrF79EX+7/wH22nMuAJlMlrq6WiZPnozjOLz//ge0tLTkhmWYNHEio0bVk06nWbHijX7Px+CpdL0o6LXFKhaLMX78OFauejff7+13VhKL1TBxwgTWrF2b719VFWXrj8/ghRf+zRWXXUxtXS3/+c9bLLrx5nyS7k7dqNFkujm0GJT6hjEDOv2hRHVRoLpwqR4KVBcFI7kuqquriLe3YoVCAPnHzc2SpU9w+IL53POXvzFn91157/0PWP3RGhoaGlh49BHssP121MRqIHfPu4aG0TStW4exDMb0vlz+cmPHjqV5Qwu24+THa2xqYuzYsVihEPc/8CDRaJQzvvVN6mprWf70M9x6250kEgmu+NXVHHzQgVxx2SWsX7+e+x94iMeWPjHg9dNflhUiVjeKcLT4tkbhSKTH8XoNVjXV1QB0dMTz/eJxt7u6prqobG1tLZZl8bldPstFF19GS2srxx5zFGd95wy+d94Pe3yd9tYNpFOp3man3+obxtDW0jxg0x9KVBcFqguX6qFAdVEw0usik6oGHOxsdrO++fAzzzzL1048lpkfn8Eee+zO4sWPYWezLDjsUGprY5x3/o9oaWllwvjx/ObqK3BsGzubxbEdHIdel8tfbv369YwZ3YBlDJlMBoBx48ayfv167GyWzngnt/7pdm790+2MGzuWM888nS/u/wXuvOvPvPPOSn7166sxxrDTjrP53rln8frrK1izZm2Pr7+p2HaW9tYN+ZsyeyLRnlv8ej0U2JlwJxiL1eT7xWIxABIlL+a9+AMP/oPGpiZSqRS33X4nM6ZPY1yv51k5A/g3WK8zFP5UF6oL1YPqQnXRv3rYPGUyGZ54chmHHnowW398Bo8/8RTgHhZMJJK0t3dQWxvj8AXzN/q13nr7HdY3b+DwBfOJRCJMmTyZA790AIuXLAXg05/+FFOmTMYYQ2ciQTaTxXFsQqEQ8+bOoba2Fsdx8g00tm1v9DwNrL6vF722WMXjcZqa1jF92jRWr/4IgBnTpxGPd7K2sbGobGen289xhsbKKCIiMhwsXvwYF/78Jyx/+hna291fEt5x591889STuf6P19LcvIE/3/tX9thj9416nWw2yyWXXs4Jxx3D//72SuKdnSxe/BgPPvRPACZPmsSxxxzF6IbRJJNJnnv+hfxVAXbfbVcWLjySaCTC+vXN/O731/V6mtBQVNGvAh9+5FEOPuhLvL5iBZlMlvnzD2HxksfKBqh//esR9t9/X/7975dpbWvjsK8eyltvv8O6desCn3kRERGB/7z1NguOOKao3+rVH/GDH11Q1G9JrmUJ4K6776lo2qXlPvpoDb+4+Jdly/79gYf4+wMPlR128aWXV/R6Q11Fweqee++jvr6eyy69CMsyPLXsaW659XYATjrxOAD+eN0iAP563/3U1sa4+KILMMbi9RVvcPkVVw3EvIuIiIhsVioKVrZts+jGm8tet8oLVB7Hcbj1T3dw65/uCGQGRUREZOBst+0szvve2WWHXXnVb3nu+RcGeY6GNt3SRkREZAR7fcUbHHv8yZt6NoYN3YRZREREJCAKViIiIiIBUbASERERCYiClYiIiEhAFKxERERGoPmHHsI53/3Opp6NYUfBSkRERCQgClYiIiKy2QuFQpt6Fiqi61iJiIgMUQfs/wV2+a/P8JMLfpHvt/POn+Tkk47nhz/+GSd/7QRmzJiOZSzeevttrr/hJj76aE2fXuMTO+3I4Yd/lSmTJ5FKpXnuuedZdNMtJJNJAGpqqjl8wVf5zKd3pra2ltWrV3P5FVexbv36HoddfdXl3HzzbSxb/jQAO2y/Heee8538NbV+9MPzeOedlWyxxVS223YWv/v9dXy4+iOOO/Zotpg6FYBXXn2V62+4iZaWVsANX1855CD22GM3GkaNorGxid9e+3vGjhvL8ccu5PQzzsrfjm/LLaZy4c9/winfOCN/U+ggKFiJiIhUyjhUx7reJ3cgJOIGHNNjmaWPP8FRRy5gwoTx+Rsaz5u7B48tfQKD4e8PPMQrr7yGZVmceMKxnPbNU/jBD3/ap/lIpVL84Q/Xs3LVu4yfMJ6zzvwWh37loPwdVr5xyslEq6L88McXsGFDC9OmbUUqnep1WCXmzZ3Dpb/8FW/+5y0ikQgfmzKZ2267kzf/8xaxWA2nf/NUjjt2IVdedQ0ARx6xgB2235aLLr6Mjz5aw5Qpk0mn06x69z1OOvE4dpy9Ay+9/AoAe+01j6effjbQUAUKViIiIhWrjjnsf2LHoLzWA9fVkujoOVi1trbx4osvMW/uHO7+81+oqanhs5/Zme99/0c0NjXR2NSUL3vnnX/mN1dfQVVVlGSy8nDz+oo38t3r1q3ngQceYt//+TwADQ2j2GWXz3La6d+huXkDACtXrup1WKWeeHIZb/7nLYB8QPK0tbVz71/u49tnnJbv99+f35tLLr083yq3evVH+WFLlixlrz3n8tLLr2BZFnvssTtXX31tn+anEgpWIiIiFUrEDQ9cVztor1WJxUuWcsThX+XuP/+F3Xb7HO+99z4ffPAh9fV1HLPwKHbYYTtiNTHAbWmrr68nmVxX8XzMmDGdIw7/KtOmbUU0GsUyFq2t7qG38ePHk8lkigKcp6dhlVq3rng+J02cyNFHH87MmVtTXVWNMVBTUwPAqFH1VFdX8dGa8oc6H35kMZdefCE1NTXM3mF7UskUL7/yar/nrTsKViIiIpVyTK+tSIPt2eee5+SvHc+sbWYyb+4cFi9ZCsARhx9GXV0t3z//x7S0tDJh/Hh+c/UVGPo2/2ec/g2WPv4kl19xFelMhrlzdmP+/EMAaGpqIhwOM2H8+C4BqqdhAMlEkqqqaP75mDFjupSx7eLDrieddByNjU2c/d3z6OiIs8P22/HjH30fcFvvEokkkydNYv365i7TWrNmLW+99RZzdt+VT33yEyxe8lif6qFS+lWgiIjIEJbJZHjiyWUceujBbP3xGTz+xFOA25KTSCRpb++gtjbG4Qvm92v6NbEaOjo6SCbd0PLFA/bLD2tpaeXpp5/lhBOOYfToBowxTJ8+jbq6uh6HAbzzzkp2331XIpEI48aN44tf3K+7WSjMS00NnZ2dxOOdjBkzmq8cclDR8IcfeZSjjlzApEkTAZgyZTLjx4/zDV/Mfvv9D5/85E48uljBSkRERMpYvPgxPvXJT/DCi/+mvb0dgDvuvJsJ48dx/R+v5cKf/YQXX3q5X9P+wx9uYL/9/ocbb/g9p379pHxw81xz7e9Zt249v/j5T7n+j//LSSceRzQa6XXYbXfcTVVVFX/43W84+zvfYkmupa0nN910C5/YaUcWXf87zvve2Sx/+pmi4bfcejsvvfwKP/j+uSy6/nec+e3T8kEOYNnyZxgzegyvr3iDpqbKD4f2hfn07vsNzs8buhGJRjnqhDO45forSacqP5mubwz1DWNoa2nGO8Y8cqkuClQXLtVDgeqiQHVRU1MNQGdnAisUws5mN/EcbXrDoR4u/+VF3H3PX3iiJCCW8r//fr3lFrVYiYiIyIjw2c9+mtraWpYte3rAXkMnr4uIiIxg2207i/O+d3bZYVde9Vuee/6FQZ6jgfHLSy6koaGB3/3hOrID2OqmYCUiIjKCvb7ijfzVzoez7557/qC8jg4FioiIiAREwUpEREQkIApWIiIiIgFRsBIREREJiIKViIiISEAUrERERIawH/3wPL70xf039WxIjoKViIiISEAUrEREREQCoguEioiIDBM77TSbIw8/jMmTJ9HY1MQdd/6ZZ555DoAZ06dxwvHHssUWU8lkM7z33vtc8LOLADhg/8DAK3YAACAASURBVC9wwP5foK6ulo6OOPf//UH+/sBDm3JRhiwFKxERkQoZoN4yg/JabbbTp9tfT5o0kXPOPpNrrv09y5c/wyc+sSNnnfktfvCjC1i16l2OP/4Ynn/hRX70k59hWRbbztoGgCmTJ3P4gvmc9/0f88GHH1JfX8f4ceMGZqFGAAUrERGRCtVbhp+PrRmU1/rB+k5a7cqj1e67fY7XV7zBU08tB+CFF/7Ns889z7y5e/B/q24lk8kwbtxYxo4dy7p163j1tdcByNpZjDFsscVUGpuaaGtrp62tfUCWaSRQsBIREalQm+3wg/Wdg/ZafTF27FgaG5uK+q1Z08ikSRMAuPZ//8BX53+FC3/2YzoTCf7f/3uEv93/AGvXNvKba37Hvvt+nlO+fhJvvf02f7rtTt566+3AlmUkUbASERGpkAN9akUaTOvXr2eHHbYv6jdxwnjWrWsGoLGxid9e+3sAtv74DH5w/rm8s3Ilr7zyGsuWP82y5U8TiUT48oEH8J1vn843Tz9z0JdhONCvAkVERIaBJ55cxnbbzmKXXT6LMYZPfnInPvvZT/PY0scBmDd3Dg0NDQDE453Yto1t20yZMplP7LQjkUiETCZDIpHEtu1NuShDmlqsREREhoE1a9Zy2RVXcsThh3Hq179GU1MTV//mf1m5chUAO+04m6OOXEB1dTWtbW385a/389prK9hyyy346vyvsMUWUwGH997/gKuu/u2mXZghTMFKRERkCPMumQDw4osv8eKLL5Utd03uMGCp9957nx/++IIBmbeRSIcCRURERAKiYCUiIiISEAUrERERkYAoWImIiIgERMFKREREJCAKViIiIiIBUbASERERCYiClYiIiEhAFKxEREREAlLRldcty2Lh0Ucwb+4cjDEsW/4M199wE+l0uttxIpEIl136C0aPbuDY408ObIZFRERENlcVtVgdcvCBzJ69PWefcz5nnHkOW0ydylFHLuhxnMO+eiiNTU2BzKSIiIjIUFBRsNpn77249977aG5upq2tjbvuvoc9583FGFO2/IwZ0/nUJ3fir3+9vw+zYgbwb7BeZyj8qS5UF6oH1YXqon/1IN0xxnSbCYa2vq8XvR4KjMVijB8/jpWr3s33e/udlcRiNUycMIE1a9cWlbcsi69/7QSuv+GmPlVy3ajRZHo4tBiE+oYxAzr9oUR1UaC6cKkeClQXBSO5Lqqrq4i3t2KFQgC5R0NVpGZQXj+Z7gScisoesP++/Pc+ezNmzGhaW9v4x78e5v6/PwjA5EmTOPqoBWwzcyaWZXj1tRX86srf9Dhs/PhxXP3ryzj5lNNpa28H4NCvHMTHZ0znl5dfCcCfbr6BRTfdwj57zWPKlMmc/u2z2XH2Dnz5wC8yfvw4Ojs7eeLJZfzptjtxHHc5GhpGcfSRhzN79vZEI1Hefe89LrrkchYcdigNo0YV3Sh63//5PHN235Uf//TCgGq0bywrRKxuFOFosqh/OBLpcbxeg1VNdTUAHR3xfL943O2urqnuUv7LBx7AOytX8drrK9hh++16n/Oc9tYNpFOpisv3VX3DGNpamgds+kOJ6qJAdeFSPRSoLgpGel1kUtWAg53NYoVC2Nks1ZEY+35q4aC8/kPPLiKRjvdeEGhc28TPf3EJTU3rmLXNTM773nd57933eO311/n+985m6eNPcOVV15DJZNlu21nY2SxVVdFuhzlZGwDbzmJnswA4thuOvOcAc3b7HBddchktLa3Ytk1baxu/+vXVrF79EVOnfozzzj2bNR+t4Z//ehhjDN/9zhm89/4HnHX29+jsTLDNNjPJZjI8/PCjXPizn1AVjdDZmQBgz7lz+Of/e6To9QaTbWdpb92Qnx9PJBrtcbxeg1Vnwp1gLFZDS0tLrjsGQKLkxSZNmsh/f34fzj3vh5XPeZ5Dpcm87/wtZwP1GkOF6qJAdeFSPRSoLgpUF+WWO5Hu5KFnFw3KqyfSnRWXXbb86Xz3G2/+h+VPP8Ps2dtTU1OD4zjcdvtd+eGvvPoaAJ/eeeduh1Xqvr89wPr1hfD9wov/zne///4HPPLoEmbP3p5//uthtv74DLbYYio/ueAXpHINKStWvJEv++6777Hbbrvy8MOPstVWW/Kxj32MJ554qk/zE7xy2aTnz0OvwSoej9PUtI7p06axevVHAMyYPo14vJO1jY1FZbfbdhYNDaP49a8udSceClFdXc0ffn8NV1xxFa+9vqLyZREREdnsOBW3Ig2m3XfflQO/uD8TJ07AGEM0GuXxx59kwoRxrFmztuw4PQ2rVNO6dUXPd9ppNvO/cggfmzqFcChEOBzmzTf/A8D4CeNpbt6QD1WlHn7kUfbee08efvhR9tpzLsuWP00ikShbdnNW0eUWHn7kUQ4+6Eu8vmIFmUyW+fMPYfGSx/LHTD1PPrWcl15+Jf981jYzOfWUkzn3ez+gtbUt2DkXERERxo0dy+nfPIWLL72cl19+lWw2y6mnfA2MobFxHRMnTig7Xk/DEkk30ESjVYB7jtWYMaO7lPPngFAoxNnf+TY33nQzjy19gnQ6zfxDD2GHHdzTgpoamxgzZjSRSKTs5ZqeeHIZxyw8ii23mMoec3bnV1de3ad62FxU9KvAe+69j9deX8Fll17EVb/+JR988CG33Ho7ACedeBwnnXgcAKlUivXrm/N/bphyWL++mUwmMzBLICIiMoJV586FbmlpJZvNsuPsHdjlvz4DwHPPv0AoHOKwrx5KVVWUUCjE7B2273VYW1s7jY1N7DlvD4wxbDtrG3b93H/1OB/hcJhIJExrWxvpdJoZ06ex997z8sPfevsdPvjgQ0484VhisRiWZbHttrMIh902nmQyyZNPLeOb3zyFzs5OXnttaB7lqqjFyrZtFt14M4tuvLnLsD9et6jb8V597XVdHFRERGQAffDhh9z953s5//vnELIsXnzxJZ56ajlWKEQymeTnF17CMQuP5Jqrf40x8PIrr/LKq6/1OAzg2t/9gZNOOI4vH3gAL738Cksee5zJkyd1Ox/JZJLrrr+RE48/htO+8XVWvPEmjz/+JDNnbg24rVuXXvYrjjn6SH51xSVEwmFWrnqXiy6+LD+Nhx9+lM/vsxe33X7ngNbZQDKf3n2/TXpWYiQa5agTzuCW668cwF8FGt+vW0bqSZge1UWB6sKleihQXRSoLmpyv3zv7EzkfxU40g10PTQ0NPDb3/yK0884q+ik+E3B//779ZZbdK9AERER2eSMMXz5wAN47rkXNnmo2hgKViIiIrJJjRs3jkXX/46dP/VJbr7ltk09OxulonOsRERERAbKunXrNrtzsg39OwiuFisREZEy0uk00V5uXyLDV1VVFalk38/9VrASEREpI5Nxb/siI1N1dZSsbfd5PB0KFBER6UYymWT8uDEkkimy+lUglhXCtodvPRjclqrq6ihrG9f3axpqsRIREelGS2s769a3UFM7iuL7J45Ehljd8K4HB2htbWP1R439DtJqsRIREemB4zgkEsnc9YxG5jW9XIZwVPXQG7VYiYiIiAREwUpEREQkIApWIiIiIgFRsBIREREJiIKViIinehzZ6okM5189icjA0q8CRWTkqpmAGTUNRk3D1E/DROtIOjbWFp04G97EaX4TWt6CbN+vviwiI5OClYiMHLGJboAaNR1GbYWJ1OLE1+C0rsJe+QC0vUddXR0dZhyMmYk144sQqoLWVTgb3nCDVrJ5Uy+FiGzGFKxEZJgyEJuEGTXNbZWq3wrCNZALUs7b90PbKsh0Fo1j7ChOy6uw/hUcDNRNxYyZhZmwM9b0/XA6G3Ga38RpfgPa3kPX8xERPwUrERkmDNROzgWp6W6QCkWh4yOctndxGl+A1nchm+jDNB1ofx+n/X2c9x6GaANmzDZu0Jq8C9hpnA3/geY33cc+TVtEhiMFKxEZmowFtVNyh/ZyLVKhCHSsdluk1jwLbe9CNhnca6ZacNY8g7PmGbAi0DADM3oWZtr/YGYeDG3vua1ZG96AzqbgXldEhoyREawatiY5ZWfM6Fb3G2UmAdkETibhbnRzz/GeD/OmfcdEIFILVtj9Rm9Fc48RjBUp7heKuDsQEwZscGxwHPcRX7f3V1qmqF+hv1NufK9cNgXpdkjHGe7vReBMyH2/ALfuHLeOIffo/fmfDxHGgtqP+VqktnSXNx+kns4FqUE60dxOQ/MbOM1v4LyDG/JGb4MZtz3WtP/GSazPhaw3oXUVOAN841or97nO/Zmw1x2DcMytvz4xJCMRzIQ0+fXEVPBrScd2t6X57Wpnblvr9ev0bWtHGBOCcDWEqt3D0uFqTKi6uJ8VgnQHpNpx0m2Qaod029DcHoaqC+uksYr3Fd3sG7r295cfGss/MoKVncbYaQjHMNVjcitwVX6FNqGqouJOJlkctPIhzL+xSOIUPc89ZpPuCmAswAIr92i6+wsVunsoZ/zP/eVygcj/aCwvEPkfc8EpFKUTCPmX186CnYJsOveYcncauUfHToGdzS1POPfapjBfRfNtSuax9Lnbz5RdZgMmhLHc1dJxHMjEcyGrAyflPrp/7Ti+btIdDJUPXZ8YC3w7SBOO+XaesZKdZy0mXNX7NMuIA5Zju0/8AczB1+3Vr1MoU1S2wsf+DAtFoHaqWx/tH+C0rcJZ/aR7jpOd7tcyB65jNU7HapwPlkCkDjN6pnvIcNZhgAMb3nZPgN/wn9z62gtjuYHIC0qR2uJ1wReiCNdiQm6gdhzb9znpcD8nmc7cjqlvjKmCVB8PbxrL3a5GR+UDgwm7ocGEovlijmPnvth2Fm1HnUxnYZuaC2JOSVAjswnvVReKFkKQtw/xB6NQlbus+X5ecKrJv0f55S9d9mwC7AymeixE6jHROvd99sqn2yHVlgtdue50e27b2JZ7PoDbQhPGDtdDbXVunfTWzzp3/1q0TsYwlrunceyM+8Uit833+veH45SEsTIhzH7xWnAywSxzP4yMYNX2LlGrjWRLM07ZFc7kPgy+D4K3MSjpb6INEK7yDc89biSnXDr3p3c7S9cVyMkHIscLROkOnGyzLxj5h6fBTlMbq6ajZZ27UbNT/drg5ud7o5e8DBPyhYfchiVS5z6P1mNqp+R3NCYSK8xL0c6kmxCWaodMx0Yt88YtW8nOMt8dK94w5fr71y0n05lbnjhk3B2mE1+TC58dOGn3ETtF/jpMxuS6Ta6X1+21PLjdsboG4u1tJWW6L99tv1y36VKuh8cey1juo5PFef8xaH8P7E23waxYuh2n8QX3vC4TglHT3XOzttgTa+uDcNo/cE9+z3QWdkL+nVKkFhOuyU+u8N4XwpLTsboQnPx/gZ3nZYg2jCHZ0kx/PullxzCWL2zU+EJXdVFYMVVj3J13rl9+W+trMXOyyeJ1wSl9xW6eO70M72Z68XA1Vqgq92XSm4dU8ZfrTGfuC3cnTrK5pF9JOOzhC0HRKxvL3f5F693tYe6RaB2mbqr7GKl31xljcgEsngtaxa1eTqoQxki3u8sYrikO7/kAH8N4291cgDKhKhLkvoTltkOk4/l10Olsym+bitbJ7pa1uwYD74t2d40N3rahy/i5/gPdOtyLkRGseuXkWpsSpX3LlSzD+L7FVLvPu2vK7K4pdNAYrPAYSLV2uzSbnJN15y/V6j4tHex/4m/RiZaEsEgtJjbJF8JqC9NIxyETp9OAlS3zIezxiEc/Lh5pQrkWJv/OMuFudPLBqAOnsxFaVxY2VrlhZOIDuJ4YQuEkdPRvB1rOZrpmbRpOFlrewml5C2flg+61s8bMwoye6bYAe+99Yr3bApfuwPHe9wF/7weZY7vLk4kXepUr1t34+S+67mE0rNJdWOln05Tv3Wu5rsNj1VHirY35sEQ2MTjvi2N3uz0suy2MuiHM+MNY7WSIzMyFstp8OHQcu9CdSeaCki+8x9fm+zm5L3V1NWHa138EBLDszmDv/waHglUgnNwhwySkWjb1zIwsjp07/6AN4r19BzVdWsIisXoSnf1oOu/y7bj3+SzsLHM7lk38rUo2kc5G95INHz6+qedk6PG2swz2dtYQahgD7cF9+Qicf1vYsbr7AIbJtUDVu2HMO5+1okNnBhMd02WKUkzBSkYQp9AEzhocDOGGMTj9PNQhIjL0OIXWUBkQulegiIiISEAUrEREREQComAlIiIiEhAFKxEREZGAKFiJiIiIBETBSkRERCQgClYiIiIiAVGwEhEREQmIgpWIiIhIQBSsRERERAKiYCUiIiISEAUrERERkYAoWImIiIgERMFKREREJCAKViIiIiIBCW/qGRgMU0KGbU2GtqoQacch4zhkgLQDGSDjOMXdQMaB7KadbRERERliRkSwmhq22D2UxYqFCQMRY3KPEDam2/HsXADLOOTCllPcTS6clQS1dC6opYGU150LbCmnMDxF+e7MANeHiIiIDIwREayeSWZZUV1FW0sz4BQNM0CIXMjCDVpF3UDYFMJY2JDrZ4q7feVqjGGU5XZHc9OO5KYV9bpz41plgp2dC2FpxxfMcINXqrS7m8DmjZfqMi1DFIesccdRiBMREQnOiAhWPXEg3yrl9nBKhg4sr+WsNIQVdedCWL47F9IiBqLGUA1ELIgay+1XUtabVigf4pIwrgboPsSlfMEtRaFlzjtM6rXMZXxBr6h/mXG8lj6vVW7ga1dERGRwjfhgtal5oa5zEAKdhRvExo4aTbKthahxug1x0VyrWtTX0hY2hircEOdvzcsfXvUFQK8Fr6dDrZmSMJYPbeUOwVIIdP7xSsNe6bByh3L9w0KKdyIiEqCKgpVlWSw8+gjmzZ2DMYZly5/h+htuIp1OF08sHOaE449hx9k7MGpUPc0bWnjooX/y4EP/HJCZl76xgYQDbRjabAc3wA1ssDBQdD5b/tAq5Q+7euX8h11DZQ61hi1vPKsQ8Izpcli393PpkmTGVZPKtdglc4dSk153ruUumXv0yiTpOqzoEfdQq2KbiMjIUlGwOuTgA5k9e3vOPud8MpkM55x9JkcduYBFN95cVC4UstiwoYULL7qUtWsb2WqrLfn+ed9lQ0sLTz21fEAWQDZvDuQPNRYfZvWGDp58KxpekDOMrh9FpqONqHGoyrXYRY2hykAVbnfUQJUx1OcOt1blnkeBKl/5UJnw5g9fWQq/Ns34nmeAbK4FLdvNc6986XP/9LzypefgeSHPHugKFhGRyoLVPnvvxS233kZzczMAd919D98+4zRuvOkWHN/OMplMccedd+efr1r1Ls8++zzbbTurgmBlcn8DbTBeY6gYWXVRfC6d+18Ci7aM7e/ZbyHIhbBc2MIfzNzgFcL90IV8LXMh7zn+Vjf33LmwZfLlQ3jB0OTLFx4L5br7UUTWa3WjELy88GWHUsTro92fZ9flRxGQwvvFa+FQ7vD6VevI+nz0THVRoLpwjeR66HnZew1WsViM8ePHsXLVu/l+b7+zklishokTJrBm7dpuxw2FQmy37Sz+dv8Dvc5m3ajRZEoOLQatvmHMgE5/KFFdFARdF1mgM/c3YJySx5KBIXLnygERnPxh1ghOrh+EcYhaufPiHIiEo0SAagP13ji58b1z78K+8aPdbFtsr1WNXOsZJv9jhjSQweT657rz/d2yhR87GF9/t6zXapfF+zO+7sLfxm709fkoUF0UqC5cI70ewpFIz8N7m0BNdTUAHR3xfL943O2urqnucdwTjltIIpFg8ZKlvc5oe+sG0qlUr+X6q75hTO5yC6K6KFBduPpTD/7z59zQZvLdUV+3/1es5S5BEjFQ7bsESekvX73uUK67EuUOlfoPpbqPTj6kZfJhzcFEonQkU6TxWuyKL3WSb7XDf8i1uAVvuLTa6fNRoLpwqR4gEo32OLzXYNWZSAAQi9XQ0tKS644BkOhMdDvewqOPZJtZM/nZzy8mm63kGuYDeSK1f2M80k8nVl0UqC5c/auH4vPn+jbuxrAoHBZ1H03+cGrhUGvpIVTT7Til5WOOG/Ji+RBY/AvZ0l/LVnotuuIg5p77lg90vl+wlg+CXc/By+ILid7wMtP0xun7u6PPR4HqwqV6cPW87L0Gq3g8TlPTOqZPm8bq1R8BMGP6NOLxTtY2NpYd59hjjmLH2Ttwwc8vpq2tvR8zLSJSnp37S5e99pynvxt9Q304Qlt7e8XT8M6t81+mpOylS+h6LTr/5UpqgJDlBT2rxyBYLij2xh/e0k7XS5iUXu4k7YAJpYnXRnyXLul6O7Byd6PwwmSSQpjULcJkpKjo5PWHH3mUgw/6Eq+vWEEmk2X+/ENYvOSxohPXPccdezQ7zt6Bn/7sItra2gKfYRGRzUkW6HRy59Tlt4mD/20+RPmWPP8vYf3Xl4uUDCu9TEnYGGKOe86d9yOKiLGKLmHi/6Wt/1Zh5X4hW/rjiWSZH0Z4lzTxH3b190v5Lljs/dLWdgph23acQnfpsIGqeJESFQWre+69j/r6ei679CIsy/DUsqe55dbbATjpxOMA+ON1ixg/fhz777cvqVSK31x1eX78115fwcWXXF5myiIiEgTvxP1Uty15fQ17hvpwXZ9a7zzuxYhzv4r1d5f0i+QuVRLx9Stc2qRwPl65ccsdgu1NUfAqCV3eMAe3Hp1cK5vjDbeSJEZFi+40UXqPVy8oei2AXutd/scbvkPA/l/SqjVveKkoWNm2zaIbb+5y3SpwA5WnqWkdC444JrCZExGRoce7GHFiAO8o4bWWGdwWOoPbUmZwg50FWMbfbQrdpcNKh+eGedO2jKG2JoqdTuV/ZOEd8q01hohV/gcXYVP8g41yLXlQfE6e/5Zg+WvUlRxuLb2tmHfeXZe7UFC4tViXu1f4Xs9rIRzJZ00FSbe0ERGRIafounSesufb5QduxKsZ6qvCtHVu3F1OLdydbtR3ODYfxLq5JZj/LhSlh1+ru7sLhVe2zGHgaC+3GfNfx85/jTrv169OKEVnXaQokHmtdv4WvFSZVrvSH2N4P7QYboFOwUpERGQQ2JC/EwID2JrXG+/cOy/I+X9cUfoLWK/1zTt0Wxs2OI5bpta7Dp6xyo7XW0udJ/8r125+Bdvd5VNKy3vDH4ynN+nhVQUrERGREcS7lEcS+hjwDPWRCG0dfTvvzmup8/+wwn/niJ5+9eq/HIo3funzcO5OFyHLfb6pW8AUrERERGTAFLXUwSa/b+xAszb1DIiIiIgMFwpWIiIiIgFRsBIREREJiIKViIiISEAUrEREREQComAlIiIiEhAFKxEREZGAKFiJiIiIBETBSkRERCQgClYiIiIiAVGwEhEREQmIgpWIiIhIQBSsRERERAKiYCUiIiISEAUrERERkYAoWImIiIgERMFKREREJCAKViIiIiIBUbASERERCYiClYiIiEhAFKxEREREAqJgJSIiIhIQBSsRERGRgChYiYiIiAREwUpEREQkIApWIiIiIgFRsBIREREJiIKViIiISEAUrEREREQComAlIiIiEhAFKxEREZGAKFiJiIiIBETBSkRERCQgClYiIiIiAVGwEhEREQmIgpWIiIhIQBSsRERERAKiYCUiIiISEAUrERERkYAoWImIiIgEJFxJIcuyWHj0EcybOwdjDMuWP8P1N9xEOp3eqLIiIiIiw0lFLVaHHHwgs2dvz9nnnM8ZZ57DFlOnctSRCza6rIiIiMhwUlGL1T5778Utt95Gc3MzAHfdfQ/fPuM0brzpFhzH6XdZv0ikCjD9XIzehSMRItHogE1/KFFdFKguXKqHAtVFgeqiQHXhUj1AJNLz8vcarGKxGOPHj2Plqnfz/d5+ZyWxWA0TJ0xgzdq1/SpbOoOHLTyl96URERER2QxEIlHSqVSX/r0Gq5rqagA6OuL5fvG4211dU93vsvnhHe3c8X/Xkk53nTkRERGRzU0kEiXe0V52WK/BqjORACAWq6GlpSXXHQMg0Znod1m/7mZOREREZHNTrqXK0+vJ6/F4nKamdUyfNi3fb8b0acTjnaxtbOx3WREREZHhpqJfBT78yKMcfNCXGDNmNPX19cyffwiLlzxW9mT0vpQVERERGU7Mp3ffr9fEY1kWxyw8krl7zMGyDE8tezp/baqTTjwOgD9et6jXsiIiIiLDWUXBSkRERER6p1vaiIiIiASkoguEDgW67Y4rHA5zwvHHsOPsHRg1qp7mDS089NA/efChf5Ytf+opX2OPObuRyWTy/a749dW8+OJLgzXLA6avyzZc14sbb/h90fNwOMwHH37IOef+oGz54bZO7LrrLuz/hX2ZPn0rWtvaOP1bZ+WH9fU9H8rrSHf10NdtBgz9daSndWKkbTd6qouRvu3or2ETrPy30slkMpxz9pkcdeQCFt1480aVHWpCIYsNG1q48KJLWbu2ka222pLvn/ddNrS08NRTy8uO86//9wg3LPq/QZ7TwdGXZRuu68Wxx59c9PzSS37OE08s63Gc4bROdHR08NA//klDQwMHHPCFomF9fc+H8jrSXT30Z5sBQ3sd6WmdgJG13eipLkb6tqO/hs2hwH323ot7772P5uZm2trauOvue9hz3lyM6XqbnL6UHWqSyRR33Hk3a9asxXEcVq16l2effZ7ttp21qWdtszec1wvP1lt/nC2mTmXxksc29awMmpdeeoUnnlxGY1NTl2F9fc+H8jrSXT2MxG1GT+tEXw3ldQIqr4uRuO3or2HRYjXQt90ZykKhENttO4u/3f9At2X2mLM7e8zZjZaWVh5b+jh/+ev92LY9iHM5cCpdtpGyXuyz1zxeeOHfNDdv6LHccF4nPH19z0fKOlLJNgOG9zqi7UZX2nZUblgEq4G+7c5QdsJxC0kkEixesrTs8Acf/Ae33HobbW3tzJgxnW+ddiqRSJQ77rx7UOdzIPRl2UbCelFVFWX33Xflmt/+vsdyw3md8Ovrez4S1hHofZsBw3sd0XajK207ZwNspAAAAfpJREFU+mZYHAr030rHU8ltd3orO9QtPPpItpk1k4suuYxsNlu2zDsrV9Ha2objOLz99jvcedc97L7b5wZ5TgdGX5ZtJKwXu35uF5LJFM89/0KP5YbzOuHX1/d8JKwjlWwzYHivI9pudKVtR98Mi2Cl2+50dewxR/GJnWbzs59fQltb5fdidBx7yJwb0Fc9LdtIWC/22XsvFi9Z2udm+eG6TvT1PR/u60h/txkwfNcR0HYDtO3oq2ERrEC33fE77tij2WnH2Vzw84tpa2vrsexuu36Omhr329ZWW23J/EMP4all3f8SaCjp67IN5/ViypTJzJo1k0ceXdxr2eG2ThhjiEQihEMhDLnusHsWRF/f86G8jvRUD33ZZsDQX0d6qouRtt3oqS5gZG87+mvYXHldt91xjR8/jmuu/hWpVKro28Vrr6/g4ksu71IXP/7R99lqyy0Jh0M0N2/gsaWPc+9f/tbjYYChordlG0nrxVFHLmDmzK356QW/6DJsuK8Te87bg2+cWvyz8bWNjZz+rbN6fc+H0zrSXT389IJf9LjNgOG3jvS0Toy07UZPdQEje9vRX8MmWImIiIhsasPmUKCIiIjIpqZgJSIiIhIQBSsRERGRgChYiYiIiAREwUpEREQkIApWIiIiIgFRsBIREREJiIKViIiISED+Pzxk5NMaeN42AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if RELOAD:\n",
    "    model = keras.models.load_model(CHAPTER_DIR + 'model1_hist2.h5')\n",
    "    history2 = model.history\n",
    "else:\n",
    "    history2 = model.fit(X_train, y_train, epochs=20, validation_data=(X_valid, y_valid))\n",
    "    model.save(CHAPTER_DIR + 'model1_hist2.h5')\n",
    "\n",
    "pd.DataFrame(history2.history).plot(figsize=(10,5))\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0,1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To improve the model's performance, we perform our usual task: tuning hyperparameters.\n",
    "\n",
    "The first one to check is the learning rate. If that doesn't help we should check the optimizer (always re-tuning the learning rate after changing any other hyperparameter). If performance is still not great, we can tune the architecture, by changing the number of layers, neurons in each hidden layer and the activation function in the hidden layers.\n",
    "\n",
    "To estimate the generalisation error, we use the ```evaluation()``` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 29us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[72.8416064743042, 0.8514999747276306]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use this model to make predictions on the first 3 instances of the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new = X_test[:3]\n",
    "y_proba = model.predict(X_new)\n",
    "y_proba.round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each instance, the model estimated the probability per class, from class 0 to class 9. If we don't care about probabilities and only want the class that has the highest probability we can use ```predict_classes()``` instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9, 2, 1])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model.predict_classes(X_new)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Ankle boot', 'Pullover', 'Trouser'], dtype='<U11')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.array(class_names)[y_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 518.4x172.8 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(7.2, 2.4))\n",
    "for index, image in enumerate(X_new):\n",
    "    plt.subplot(1, 3, index + 1)\n",
    "    plt.imshow(image, cmap=\"binary\", interpolation=\"nearest\")\n",
    "    plt.axis('off')\n",
    "    plt.title(class_names[y_test[index]], fontsize=12)\n",
    "plt.subplots_adjust(wspace=0.2, hspace=0.5)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a regression MLP using the Sequential API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use scikit-learn to import a simplified version of the California housing dataset (only numerical features and no missing values). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(housing.data, housing.target)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_valid = scaler.transform(X_valid)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building the model is pretty similar to the steps taken for the example above. Since we're predicting house prices we only need 1 output and the output layer won't use an activation function and, we'll also use mean squared error as the error function. \n",
    "\n",
    "Since the dataset is noisy, we'll use a single layer with a few neurons to avoid overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11610 samples, validate on 3870 samples\n",
      "Epoch 1/20\n",
      "11610/11610 [==============================] - 0s 33us/step - loss: 1.3492 - val_loss: 10.4417\n",
      "Epoch 2/20\n",
      "11610/11610 [==============================] - 0s 27us/step - loss: 25.6096 - val_loss: 0.6001\n",
      "Epoch 3/20\n",
      "11610/11610 [==============================] - 0s 28us/step - loss: 0.4792 - val_loss: 0.4679\n",
      "Epoch 4/20\n",
      "11610/11610 [==============================] - 0s 31us/step - loss: 0.4213 - val_loss: 0.4998\n",
      "Epoch 5/20\n",
      "11610/11610 [==============================] - 0s 29us/step - loss: 0.4235 - val_loss: 0.4189\n",
      "Epoch 6/20\n",
      "11610/11610 [==============================] - 0s 29us/step - loss: 0.3933 - val_loss: 0.4144\n",
      "Epoch 7/20\n",
      "11610/11610 [==============================] - 0s 30us/step - loss: 0.3837 - val_loss: 0.4018\n",
      "Epoch 8/20\n",
      "11610/11610 [==============================] - 0s 31us/step - loss: 0.3775 - val_loss: 0.4015\n",
      "Epoch 9/20\n",
      "11610/11610 [==============================] - 0s 31us/step - loss: 0.3718 - val_loss: 0.3950\n",
      "Epoch 10/20\n",
      "11610/11610 [==============================] - 0s 31us/step - loss: 0.3753 - val_loss: 0.4115\n",
      "Epoch 11/20\n",
      "11610/11610 [==============================] - 0s 28us/step - loss: 0.3674 - val_loss: 0.3934\n",
      "Epoch 12/20\n",
      "11610/11610 [==============================] - 0s 31us/step - loss: 0.3658 - val_loss: 0.3938\n",
      "Epoch 13/20\n",
      "11610/11610 [==============================] - 0s 29us/step - loss: 0.3633 - val_loss: 0.3840\n",
      "Epoch 14/20\n",
      "11610/11610 [==============================] - 0s 28us/step - loss: 0.3582 - val_loss: 0.3864\n",
      "Epoch 15/20\n",
      "11610/11610 [==============================] - 0s 29us/step - loss: 0.3579 - val_loss: 0.3851\n",
      "Epoch 16/20\n",
      "11610/11610 [==============================] - 0s 30us/step - loss: 0.3540 - val_loss: 0.3801\n",
      "Epoch 17/20\n",
      "11610/11610 [==============================] - 0s 30us/step - loss: 0.3581 - val_loss: 0.3783\n",
      "Epoch 18/20\n",
      "11610/11610 [==============================] - 0s 30us/step - loss: 0.3742 - val_loss: 0.3767\n",
      "Epoch 19/20\n",
      "11610/11610 [==============================] - 0s 31us/step - loss: 0.4079 - val_loss: 0.3850\n",
      "Epoch 20/20\n",
      "11610/11610 [==============================] - 0s 31us/step - loss: 0.3496 - val_loss: 0.3810\n",
      "5160/5160 [==============================] - 0s 12us/step\n"
     ]
    }
   ],
   "source": [
    "if RELOAD:\n",
    "    model = keras.models.load_model(CHAPTER_DIR + 'model2.h5')\n",
    "else:\n",
    "    model = Sequential([\n",
    "        Dense(30, activation='relu', input_shape=X_train.shape[1:]),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    model.compile(loss='mean_squared_error', optimizer='sgd')\n",
    "    history = model.fit(X_train, y_train, epochs=20, validation_data=(X_valid, y_valid))\n",
    "    model.save(CHAPTER_DIR + 'model2.h5')\n",
    "\n",
    "mse_test = model.evaluate(X_test, y_test)\n",
    "X_new = X_test[:3]\n",
    "y_pred = model.predict(X_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 460.8x403.2 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "width = 0.35\n",
    "actual = y_test[:X_new.shape[0]]\n",
    "preds = y_pred.reshape(1,-1)[0]\n",
    "x_pos = np.arange(X_new.shape[0])\n",
    "\n",
    "ax.bar(x= x_pos-width/2.0, height=actual, width=width, label='actual')\n",
    "ax.bar(x= x_pos+width/2.0, height=preds, width=width, label='predicted')\n",
    "ax.legend()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sequential API is quite easy to use, however for more complex models Keras offers the Functional API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building complex models with the Functional API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One example of a non-sequential networs is a *Wide and Deep* NN, which was first introduced in a [2016 paper](https://homl.info/widedeep). This architecture allows for the Neural Net to learn both deep pattens (using the deep path) and simple rules (through the short path) (example image on page 309).\n",
    "\n",
    "In contrast, a regular MLP forces the data to go through the full stack of layers; thus, simple patterns in the data may end up being distorted by this sequence of transformations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One example network for the California housing dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input\n",
    "\n",
    "input_ = keras.layers.Input(shape=X_train.shape[1:])\n",
    "hidden1 = keras.layers.Dense(30, activation=\"relu\")(input_)\n",
    "hidden2 = keras.layers.Dense(30, activation=\"relu\")(hidden1)\n",
    "concat = keras.layers.concatenate([input_, hidden2])\n",
    "output = keras.layers.Dense(1)(concat)\n",
    "model = keras.models.Model(inputs=[input_], outputs=[output])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a breakdown of what's happening in the code above:\n",
    "- We create an Input object, containint a specification of what type of input the model will get (a model may have multiple inputs)\n",
    "- Next create a Dense layer, with 30 neurons using ReLU. We call it like a function passing it the inputs (hence functional api)\n",
    "- Create a second hidden layer as above and use it as a function. We pass it the outputs of the first hidden layer\n",
    "- Next we use the ```concatenate``` function, which creates a Concatenate layer and pass calls it with the given inputs\n",
    "- Create output layer with single neuron, passing it the result of concatenation\n",
    "- Finally create the model, specifying the inputs and outputs\n",
    "\n",
    "Now that's done we do exactly as above, comple, train, evaluate the model and use it to make predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we wanted to send some features through the wide path and a different subset (possibly overlapping) through the deep path (see fig on pg 310)? One solution is to use multiple inputs. For example we could send 5 features through the wide path (feats 0 to 4) and six through the deep path (feats 2 to 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_A = keras.layers.Input(shape=[5], name=\"wide_input\")\n",
    "input_B = keras.layers.Input(shape=[6], name=\"deep_input\")\n",
    "hidden1 = keras.layers.Dense(30, activation=\"relu\")(input_B)\n",
    "hidden2 = keras.layers.Dense(30, activation=\"relu\")(hidden1)\n",
    "concat = keras.layers.concatenate([input_A, hidden2])\n",
    "output = keras.layers.Dense(1, name=\"output\")(concat)\n",
    "model = keras.models.Model(inputs=[input_A, input_B], outputs=[output])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A good tip is to name the most important layers, especially as the model gets more complex. Also note that we specified ```inputs=[input_A, input_B]``` so when we're calling the fit method we need to pass a pair of matrices (X_train_A, X_train_B)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 2.1468 - val_loss: 0.8962\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.8029 - val_loss: 0.6787\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.6703 - val_loss: 0.6205\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.6175 - val_loss: 0.5889\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5835 - val_loss: 0.5646\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5578 - val_loss: 0.5448\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5373 - val_loss: 0.5306\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5200 - val_loss: 0.5163\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5055 - val_loss: 0.5043\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4937 - val_loss: 0.4960\n",
      "Epoch 11/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4849 - val_loss: 0.4967\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4766 - val_loss: 0.4868\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4699 - val_loss: 0.4806\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4640 - val_loss: 0.4805\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4581 - val_loss: 0.4750\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4530 - val_loss: 0.4651\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4484 - val_loss: 0.4663\n",
      "Epoch 18/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4447 - val_loss: 0.4625\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4407 - val_loss: 0.4577\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4371 - val_loss: 0.4546\n",
      "162/162 [==============================] - 0s 903us/step - loss: 0.4467\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='mse', optimizer=keras.optimizers.SGD(lr=1e-3))\n",
    "\n",
    "X_train_A, X_train_B = X_train[:, :5], X_train[:, 2:]\n",
    "X_valid_A, X_valid_B = X_valid[:, :5], X_valid[:, 2:]\n",
    "X_test_A, X_test_B = X_test[:, :5], X_test[:, 2:]\n",
    "X_new_A, X_new_B = X_test_A[:3], X_test_B[:3]\n",
    "\n",
    "if RELOAD:\n",
    "    model = keras.models.load_model(CHAPTER_DIR + 'model3.h5')\n",
    "else:    \n",
    "    history = model.fit((X_train_A, X_train_B), y_train, epochs=20,\n",
    "                         validation_data=((X_valid_A, X_valid_B), y_valid))\n",
    "    model.save(CHAPTER_DIR + 'model3.h5')\n",
    "    \n",
    "mse_test = model.evaluate((X_test_A, X_test_B), y_test)\n",
    "y_pred = model.predict((X_new_A, X_new_B))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are also cases where you may want to have multiple outputs:\n",
    "- The task may demand it; e.g. locate and classify the object in a picture. This is both regression task (finding the coordinates of the object center) and classification task\n",
    "- There may be multiple independent tasks based on the same data. For example you could perform *multitask classification* on pictures of faces to classify the person's facial expression and another to identify whether they're wearing sunglasses or not\n",
    "- Another use case is as a regularization technique. For example you may want to add some auxiliary outputs in a neural network architecture to ensure that the underlying part of the network learns something useful on its own, without relying on the rest of the network (see fig 10-16 pg 312)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To add extra outputs, simply connect them to the appropriate layers and add them to the model's list of outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continuing from the architecture above\n",
    "# ...\n",
    "output = keras.layers.Dense(1, name='main_output')(concat)\n",
    "aux_output = keras.layers.Dense(1, name='aux_output')(hidden2)\n",
    "model = keras.models.Model(inputs=[input_A, input_B], outputs=[output, aux_output])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each output needs its own loss function, when compiling the model we need to passa list of losses (or a single loss, Keras will assume that the same loss must be used for all). By default, Keras will compute all these losses and simply add them to arrive at a final loss for training. If we care more about the main output's loss, we can specify loss weights when compiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=['mse', 'mse'], loss_weights=[0.9, 0.1], optimizer='sgd')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now when training the model we also need to pass the labels to the auxiliary output. In this case they're trying to predict the same output so we just pass y_train again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.8676 - main_output_loss: 0.7595 - aux_output_loss: 1.8407 - val_loss: 1.5737 - val_main_output_loss: 1.6217 - val_aux_output_loss: 1.1420\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.5154 - main_output_loss: 0.4582 - aux_output_loss: 1.0293 - val_loss: 1.0300 - val_main_output_loss: 1.0367 - val_aux_output_loss: 0.9694\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.5154 - main_output_loss: 0.4728 - aux_output_loss: 0.8989 - val_loss: 0.6058 - val_main_output_loss: 0.5834 - val_aux_output_loss: 0.8075\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4747 - main_output_loss: 0.4376 - aux_output_loss: 0.8085 - val_loss: 0.4632 - val_main_output_loss: 0.4328 - val_aux_output_loss: 0.7375\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4390 - main_output_loss: 0.4065 - aux_output_loss: 0.7308 - val_loss: 0.4455 - val_main_output_loss: 0.4189 - val_aux_output_loss: 0.6844\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4232 - main_output_loss: 0.3946 - aux_output_loss: 0.6811 - val_loss: 0.4407 - val_main_output_loss: 0.4182 - val_aux_output_loss: 0.6428\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4115 - main_output_loss: 0.3865 - aux_output_loss: 0.6366 - val_loss: 0.4390 - val_main_output_loss: 0.4192 - val_aux_output_loss: 0.6169\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4055 - main_output_loss: 0.3835 - aux_output_loss: 0.6040 - val_loss: 0.4358 - val_main_output_loss: 0.4186 - val_aux_output_loss: 0.5901\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3955 - main_output_loss: 0.3747 - aux_output_loss: 0.5830 - val_loss: 0.4281 - val_main_output_loss: 0.4118 - val_aux_output_loss: 0.5740\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3890 - main_output_loss: 0.3695 - aux_output_loss: 0.5639 - val_loss: 0.4173 - val_main_output_loss: 0.4015 - val_aux_output_loss: 0.5598\n",
      "Epoch 11/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3844 - main_output_loss: 0.3658 - aux_output_loss: 0.5514 - val_loss: 0.4167 - val_main_output_loss: 0.4019 - val_aux_output_loss: 0.5497\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3794 - main_output_loss: 0.3617 - aux_output_loss: 0.5380 - val_loss: 0.4125 - val_main_output_loss: 0.3979 - val_aux_output_loss: 0.5437\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3759 - main_output_loss: 0.3590 - aux_output_loss: 0.5282 - val_loss: 0.4074 - val_main_output_loss: 0.3932 - val_aux_output_loss: 0.5354\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3710 - main_output_loss: 0.3545 - aux_output_loss: 0.5191 - val_loss: 0.4034 - val_main_output_loss: 0.3900 - val_aux_output_loss: 0.5241\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3663 - main_output_loss: 0.3503 - aux_output_loss: 0.5096 - val_loss: 0.4068 - val_main_output_loss: 0.3943 - val_aux_output_loss: 0.5201\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3650 - main_output_loss: 0.3494 - aux_output_loss: 0.5057 - val_loss: 0.3990 - val_main_output_loss: 0.3861 - val_aux_output_loss: 0.5151\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3578 - main_output_loss: 0.3428 - aux_output_loss: 0.4926 - val_loss: 0.3914 - val_main_output_loss: 0.3784 - val_aux_output_loss: 0.5078\n",
      "Epoch 18/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3546 - main_output_loss: 0.3401 - aux_output_loss: 0.4853 - val_loss: 0.3974 - val_main_output_loss: 0.3844 - val_aux_output_loss: 0.5144\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3544 - main_output_loss: 0.3403 - aux_output_loss: 0.4819 - val_loss: 0.3897 - val_main_output_loss: 0.3771 - val_aux_output_loss: 0.5031\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3500 - main_output_loss: 0.3363 - aux_output_loss: 0.4738 - val_loss: 0.3871 - val_main_output_loss: 0.3751 - val_aux_output_loss: 0.4951\n"
     ]
    }
   ],
   "source": [
    "if RELOAD:\n",
    "    model = keras.models.load_model(CHAPTER_DIR + 'model4.h5')\n",
    "else:\n",
    "    history = model.fit([X_train_A, X_train_B], [y_train, y_train], epochs=20,\n",
    "                        validation_data=([X_valid_A, X_valid_B], [y_valid, y_valid]))\n",
    "    model.save(CHAPTER_DIR + 'model4.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we evalute the model, keras returns the total loss as well as the individual losses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162/162 [==============================] - 0s 856us/step - loss: 0.3638 - main_output_loss: 0.3525 - aux_output_loss: 0.4661\n"
     ]
    }
   ],
   "source": [
    "total_loss, main_loss, aux_loss = model.evaluate([X_test_A, X_test_B], [y_test, y_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly for the ```predict()``` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_main, y_pred_aux = model.predict([X_new_A, X_new_B])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subclassing to build Dynamic Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sequential and functional API are delcarative; you specify which layers you want to use and how are they connected. This means they're easily saved, used, cloned and shared among other advantages. The flip side is that they're static.\n",
    "\n",
    "Some models may have loops, branches, varying shapes and other dynamic behaviors. For these, you can use the subclassing API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simply sublclass the Model class and create the layers you want in the ```call()``` method. The example below builds a class for the ```WideAndDeepModel``` discussed above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WideAndDeepModel(keras.models.Model):\n",
    "    def __init__(self, units=30, activation=\"relu\", **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.hidden1 = keras.layers.Dense(units, activation=activation)\n",
    "        self.hidden2 = keras.layers.Dense(units, activation=activation)\n",
    "        self.main_output = keras.layers.Dense(1)\n",
    "        self.aux_output = keras.layers.Dense(1)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        input_A, input_B = inputs\n",
    "        hidden1 = self.hidden1(input_B)\n",
    "        hidden2 = self.hidden2(hidden1)\n",
    "        concat = keras.layers.concatenate([input_A, hidden2])\n",
    "        main_output = self.main_output(concat)\n",
    "        aux_output = self.aux_output(hidden2)\n",
    "        return main_output, aux_output\n",
    "\n",
    "model = WideAndDeepModel(30, activation=\"relu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usage is similar to the functional API, except we do not need to create the inputs. We just use the input argument to the ```call()``` method and separaate the creation of layers in the constructor from their usage in the ```call()``` method. This enables you to use loops, if statements, low-level tensorflow operations, etc...\n",
    "\n",
    "The cost is that the model architecture is hidden behind the call method. Keras cannot check types, and shapes ahead of time and it is easier to make mistakes. \n",
    "\n",
    "**Note:** Keras models can be used like regular layers so you can easily combine them to build complex architectures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and restoring a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras uses HDF5 to save the model's architecture and the values of the model parameters for every layer (weights and biases). It also saves the optimizer. You would have seen the save function scattered through this notebook and the load function as well to re-store the models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this works when using the Sequential or Functionla APIs but not when using model subclassing. You can use ```save_weights()``` and ```load_weights()``` to at least save and restore the model parameters, but you will need to save and restore everything else yourself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if training lasts several hours? Which is quite common, especially on large datasets. In this case we should not only save the final model but save at regular checkpoints during training to avoid losing everything due to a crash. \n",
    "\n",
    "The ```fit()``` method accepts a ```callbacks``` argument that lets you specify a list of objects that Keras will call at the start and end of training, at the start and end of each epoch, and even before and after processing each batch.\n",
    "\n",
    "The ```ModelCheckpoint``` callback saves checkpoints at regular intervals during training, at the end of each epoch. Moreover, if we use a validation set while training, you can set ```save_best_only=True``` when creating the model checkpoint. Then it only saves the model when its performance on the validation set is the best so far. This way we don't need to save the train the model for too long and overfitting the training set: simply restore the last model saved after training and this will be the best model on the validation set. \n",
    "The following is a simple way of implementing early stopping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 2.2844 - val_loss: 0.9851\n",
      "Epoch 2/10\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.7927 - val_loss: 0.6585\n",
      "Epoch 3/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.6355 - val_loss: 0.6066\n",
      "Epoch 4/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5908 - val_loss: 0.5773\n",
      "Epoch 5/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5598 - val_loss: 0.5530\n",
      "Epoch 6/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5331 - val_loss: 0.5317\n",
      "Epoch 7/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5116 - val_loss: 0.5163\n",
      "Epoch 8/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4925 - val_loss: 0.4974\n",
      "Epoch 9/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4774 - val_loss: 0.4845\n",
      "Epoch 10/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4631 - val_loss: 0.4756\n",
      "162/162 [==============================] - 0s 754us/step - loss: 0.4639\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", input_shape=[8]),\n",
    "    keras.layers.Dense(30, activation=\"relu\"),\n",
    "    keras.layers.Dense(1)\n",
    "])\n",
    "model.compile(loss=\"mse\", optimizer=keras.optimizers.SGD(lr=1e-3))\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(CHAPTER_DIR + \"my_keras_model.h5\", save_best_only=True)\n",
    "history = model.fit(X_train, y_train, epochs=10,\n",
    "                    validation_data=(X_valid, y_valid),\n",
    "                    callbacks=[checkpoint_cb])\n",
    "model = keras.models.load_model(CHAPTER_DIR + \"my_keras_model.h5\") # rollback to best model\n",
    "mse_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way of implementing early stopping is to simply use the ```EarlyStopping``` callback. It interrupts training when it measures no progress on the validation set for a number of epochs (defined by the ```patience``` arg). You can combine both callbacks to save checkpoints of your model and interrupt training early when there is no more progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4505 - val_loss: 0.4663\n",
      "Epoch 2/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4395 - val_loss: 0.4586\n",
      "Epoch 3/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4331 - val_loss: 0.4521\n",
      "Epoch 4/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4257 - val_loss: 0.4469\n",
      "Epoch 5/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4199 - val_loss: 0.4429\n",
      "Epoch 6/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4142 - val_loss: 0.4419\n",
      "Epoch 7/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4097 - val_loss: 0.4410\n",
      "Epoch 8/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4060 - val_loss: 0.4341\n",
      "Epoch 9/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4026 - val_loss: 0.4314\n",
      "Epoch 10/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3983 - val_loss: 0.4293\n",
      "Epoch 11/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3948 - val_loss: 0.4291\n",
      "Epoch 12/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3924 - val_loss: 0.4212\n",
      "Epoch 13/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3907 - val_loss: 0.4220\n",
      "Epoch 14/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3876 - val_loss: 0.4215\n",
      "Epoch 15/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3851 - val_loss: 0.4218\n",
      "Epoch 16/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3841 - val_loss: 0.4163\n",
      "Epoch 17/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3810 - val_loss: 0.4152\n",
      "Epoch 18/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3796 - val_loss: 0.4175\n",
      "Epoch 19/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3790 - val_loss: 0.4089\n",
      "Epoch 20/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3760 - val_loss: 0.4087\n",
      "Epoch 21/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3748 - val_loss: 0.4079\n",
      "Epoch 22/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3728 - val_loss: 0.4067\n",
      "Epoch 23/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3715 - val_loss: 0.4063\n",
      "Epoch 24/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3703 - val_loss: 0.4052\n",
      "Epoch 25/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3698 - val_loss: 0.4054\n",
      "Epoch 26/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3680 - val_loss: 0.4017\n",
      "Epoch 27/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3665 - val_loss: 0.4014\n",
      "Epoch 28/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3655 - val_loss: 0.3984\n",
      "Epoch 29/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3643 - val_loss: 0.3977\n",
      "Epoch 30/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3633 - val_loss: 0.3962\n",
      "Epoch 31/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3627 - val_loss: 0.3968\n",
      "Epoch 32/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3619 - val_loss: 0.3962\n",
      "Epoch 33/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3603 - val_loss: 0.3944\n",
      "Epoch 34/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3600 - val_loss: 0.3958\n",
      "Epoch 35/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3592 - val_loss: 0.3922\n",
      "Epoch 36/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3578 - val_loss: 0.3930\n",
      "Epoch 37/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3572 - val_loss: 0.3941\n",
      "Epoch 38/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3563 - val_loss: 0.3910\n",
      "Epoch 39/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3553 - val_loss: 0.3899\n",
      "Epoch 40/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3549 - val_loss: 0.3914\n",
      "Epoch 41/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3544 - val_loss: 0.3880\n",
      "Epoch 42/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3535 - val_loss: 0.3881\n",
      "Epoch 43/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3526 - val_loss: 0.3892\n",
      "Epoch 44/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3520 - val_loss: 0.3886\n",
      "Epoch 45/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3515 - val_loss: 0.3845\n",
      "Epoch 46/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3507 - val_loss: 0.3861\n",
      "Epoch 47/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3503 - val_loss: 0.3854\n",
      "Epoch 48/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3496 - val_loss: 0.3862\n",
      "Epoch 49/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3490 - val_loss: 0.3838\n",
      "Epoch 50/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3483 - val_loss: 0.3831\n",
      "Epoch 51/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3481 - val_loss: 0.3824\n",
      "Epoch 52/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3473 - val_loss: 0.3818\n",
      "Epoch 53/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3468 - val_loss: 0.3833\n",
      "Epoch 54/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3463 - val_loss: 0.3841\n",
      "Epoch 55/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3458 - val_loss: 0.3854\n",
      "Epoch 56/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3454 - val_loss: 0.3796\n",
      "Epoch 57/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3444 - val_loss: 0.3795\n",
      "Epoch 58/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3439 - val_loss: 0.3839\n",
      "Epoch 59/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3438 - val_loss: 0.3776\n",
      "Epoch 60/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3431 - val_loss: 0.3788\n",
      "Epoch 61/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3425 - val_loss: 0.3795\n",
      "Epoch 62/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3421 - val_loss: 0.3792\n",
      "Epoch 63/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3420 - val_loss: 0.3761\n",
      "Epoch 64/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3409 - val_loss: 0.3754\n",
      "Epoch 65/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3406 - val_loss: 0.3753\n",
      "Epoch 66/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3404 - val_loss: 0.3755\n",
      "Epoch 67/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3398 - val_loss: 0.3774\n",
      "Epoch 68/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3393 - val_loss: 0.3773\n",
      "Epoch 69/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3389 - val_loss: 0.3738\n",
      "Epoch 70/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3387 - val_loss: 0.3732\n",
      "Epoch 71/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3382 - val_loss: 0.3776\n",
      "Epoch 72/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3375 - val_loss: 0.3736\n",
      "Epoch 73/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3374 - val_loss: 0.3735\n",
      "Epoch 74/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3370 - val_loss: 0.3722\n",
      "Epoch 75/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3365 - val_loss: 0.3731\n",
      "Epoch 76/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3361 - val_loss: 0.3725\n",
      "Epoch 77/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3355 - val_loss: 0.3705\n",
      "Epoch 78/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3353 - val_loss: 0.3725\n",
      "Epoch 79/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3350 - val_loss: 0.3692\n",
      "Epoch 80/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3347 - val_loss: 0.3703\n",
      "Epoch 81/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3339 - val_loss: 0.3736\n",
      "Epoch 82/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3339 - val_loss: 0.3692\n",
      "Epoch 83/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3336 - val_loss: 0.3727\n",
      "Epoch 84/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3332 - val_loss: 0.3701\n",
      "Epoch 85/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3327 - val_loss: 0.3710\n",
      "Epoch 86/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3324 - val_loss: 0.3680\n",
      "Epoch 87/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3319 - val_loss: 0.3675\n",
      "Epoch 88/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3317 - val_loss: 0.3682\n",
      "Epoch 89/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3315 - val_loss: 0.3696\n",
      "Epoch 90/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3309 - val_loss: 0.3671\n",
      "Epoch 91/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3307 - val_loss: 0.3660\n",
      "Epoch 92/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3305 - val_loss: 0.3676\n",
      "Epoch 93/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3298 - val_loss: 0.3700\n",
      "Epoch 94/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3297 - val_loss: 0.3668\n",
      "Epoch 95/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3295 - val_loss: 0.3655\n",
      "Epoch 96/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3291 - val_loss: 0.3679\n",
      "Epoch 97/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3287 - val_loss: 0.3645\n",
      "Epoch 98/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3283 - val_loss: 0.3643\n",
      "Epoch 99/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3281 - val_loss: 0.3672\n",
      "Epoch 100/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3275 - val_loss: 0.3651\n",
      "162/162 [==============================] - 0s 722us/step - loss: 0.3475\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=\"mse\", optimizer=keras.optimizers.SGD(lr=1e-3))\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n",
    "history = model.fit(X_train, y_train, epochs=100,\n",
    "                    validation_data=(X_valid, y_valid),\n",
    "                    callbacks=[checkpoint_cb, early_stopping_cb])\n",
    "mse_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This way we can also set the number of epochs to a large value since training will stop when there's no more progress."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also write your own custom callbacks. Below is an example of a callback that displays the ratio between validation loss and the training loss during training (e.g. to detect overfitting)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "345/363 [===========================>..] - ETA: 0s - loss: 0.3268\n",
      "val/train: 1.11\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3274 - val_loss: 0.3635\n"
     ]
    }
   ],
   "source": [
    "class PrintValTrainRatioCallback(keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        print(\"\\nval/train: {:.2f}\".format(logs[\"val_loss\"] / logs[\"loss\"]))\n",
    "        \n",
    "val_train_ratio_cb = PrintValTrainRatioCallback()\n",
    "history = model.fit(X_train, y_train, epochs=1,\n",
    "                    validation_data=(X_valid, y_valid),\n",
    "                    callbacks=[val_train_ratio_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also implement ```on_train_begin(), on_train_end(), on_epoch_begin(), on_epoch_end(), on_batch_begin(), on_batch_end()```. They can also be used during evaluation and predictions (e.g. for debugging), see notes on pg 316 for more on that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
