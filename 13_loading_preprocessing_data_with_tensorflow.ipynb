{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Data API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main object used in the data API are ```datasets```. Usually you will use datasets that gradually read data from disk, but let's start with an example that creates a dataset entirely in RAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TensorSliceDataset shapes: (), types: tf.int32>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = tf.range(10)\n",
    "dataset = tf.data.Dataset.from_tensor_slices(X)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```from_tensor_slices()``` takes a tensor and creates a Dataset whose elements are all slices of X (along the first dimension). This dataset contains 10 items: tensors 0, 1, 2, ..., 9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0, shape=(), dtype=int32)\n",
      "tf.Tensor(1, shape=(), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "tf.Tensor(3, shape=(), dtype=int32)\n",
      "tf.Tensor(4, shape=(), dtype=int32)\n",
      "tf.Tensor(5, shape=(), dtype=int32)\n",
      "tf.Tensor(6, shape=(), dtype=int32)\n",
      "tf.Tensor(7, shape=(), dtype=int32)\n",
      "tf.Tensor(8, shape=(), dtype=int32)\n",
      "tf.Tensor(9, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chaining Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset methods reeturn a new Dataset, so we can chain transformations like so"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0 1 2 3 4 5 6], shape=(7,), dtype=int32)\n",
      "tf.Tensor([7 8 9 0 1 2 3], shape=(7,), dtype=int32)\n",
      "tf.Tensor([4 5 6 7 8 9 0], shape=(7,), dtype=int32)\n",
      "tf.Tensor([1 2 3 4 5 6 7], shape=(7,), dtype=int32)\n",
      "tf.Tensor([8 9], shape=(2,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.repeat(3).batch(7)\n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that calling the repeat method, it returns a new dataset that repeats the items of the original dataset 3 times, however it does not copy data in memory 3 times!\n",
    "\n",
    "batch() returned 2 remaining items in the last tensor, if we wanted to keep the tensors with the same size we could pass ```drop_remainder=True```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also apply transformations to each item with the ```map``` method. Sometimes we will perform expensive computations with this method, such as rotating or reshaping an image. To spawn multiple threads to speed things up, set the ```num_parallel_calls``` argument. Note that the function you pass to ```map``` must be convertible to a TF function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function <lambda> at 0x7f77c443f430> and will run it as-is.\n",
      "Cause: could not parse the source code:\n",
      "\n",
      "for item in dataset.map(lambda x: x**2):\n",
      "\n",
      "This error may be avoided by creating the lambda in a standalone statement.\n",
      "\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function <lambda> at 0x7f77c443f430> and will run it as-is.\n",
      "Cause: could not parse the source code:\n",
      "\n",
      "for item in dataset.map(lambda x: x**2):\n",
      "\n",
      "This error may be avoided by creating the lambda in a standalone statement.\n",
      "\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "tf.Tensor([ 0  1  4  9 16 25 36], shape=(7,), dtype=int32)\n",
      "tf.Tensor([49 64 81  0  1  4  9], shape=(7,), dtype=int32)\n",
      "tf.Tensor([16 25 36 49 64 81  0], shape=(7,), dtype=int32)\n",
      "tf.Tensor([ 1  4  9 16 25 36 49], shape=(7,), dtype=int32)\n",
      "tf.Tensor([64 81], shape=(2,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for item in dataset.map(lambda x: x**2):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ```apply()```  methods applies a transformation to the dataset as whole. Using the ```unbatch``` function, each item in the new dataset will be a single-integer instead of a batch of seven items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0, shape=(), dtype=int32)\n",
      "tf.Tensor(1, shape=(), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "tf.Tensor(3, shape=(), dtype=int32)\n",
      "tf.Tensor(4, shape=(), dtype=int32)\n",
      "tf.Tensor(5, shape=(), dtype=int32)\n",
      "tf.Tensor(6, shape=(), dtype=int32)\n",
      "tf.Tensor(7, shape=(), dtype=int32)\n",
      "tf.Tensor(8, shape=(), dtype=int32)\n",
      "tf.Tensor(9, shape=(), dtype=int32)\n",
      "tf.Tensor(0, shape=(), dtype=int32)\n",
      "tf.Tensor(1, shape=(), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "tf.Tensor(3, shape=(), dtype=int32)\n",
      "tf.Tensor(4, shape=(), dtype=int32)\n",
      "tf.Tensor(5, shape=(), dtype=int32)\n",
      "tf.Tensor(6, shape=(), dtype=int32)\n",
      "tf.Tensor(7, shape=(), dtype=int32)\n",
      "tf.Tensor(8, shape=(), dtype=int32)\n",
      "tf.Tensor(9, shape=(), dtype=int32)\n",
      "tf.Tensor(0, shape=(), dtype=int32)\n",
      "tf.Tensor(1, shape=(), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "tf.Tensor(3, shape=(), dtype=int32)\n",
      "tf.Tensor(4, shape=(), dtype=int32)\n",
      "tf.Tensor(5, shape=(), dtype=int32)\n",
      "tf.Tensor(6, shape=(), dtype=int32)\n",
      "tf.Tensor(7, shape=(), dtype=int32)\n",
      "tf.Tensor(8, shape=(), dtype=int32)\n",
      "tf.Tensor(9, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.apply(tf.data.Dataset.unbatch)\n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also filter data with ```filter```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.filter(lambda x: x < 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And to look at a t few items from the dataset use ```take```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0, shape=(), dtype=int32)\n",
      "tf.Tensor(1, shape=(), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for item in dataset.take(3):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shuffling the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ```shuffle()``` method creates a new dataset that fills up a buffer with the first items of the source dataset. Whenever it is asked for an item, it will pull one out randomly from the buffer and replace it with a fresh new one from the source dataset, until it has iterated thorugh the source dataset. At this point it continues to  pull out items randomly from the buffer until it is empty.\n",
    "\n",
    "You must specify the buffer size and it is important to make it large enough else shuffling will not be very effective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0 2 3 6 7 9 4], shape=(7,), dtype=int64)\n",
      "tf.Tensor([5 0 1 1 8 6 5], shape=(7,), dtype=int64)\n",
      "tf.Tensor([4 8 7 1 2 3 0], shape=(7,), dtype=int64)\n",
      "tf.Tensor([5 4 2 7 8 9 9], shape=(7,), dtype=int64)\n",
      "tf.Tensor([3 6], shape=(2,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.range(10).repeat(3)\n",
    "dataset = dataset.shuffle(buffer_size=5, seed=42).batch(7)\n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For large datasets tha do not fit im memory, this simple shuffling-buffer approach may not be sufficient, since the buffer will be small compared to the size of the dataset.\n",
    "\n",
    "One solution to this is to shuffle the source data itself, for example on Linux you can use ```shuf``` to shuffle text files. \n",
    "Even if the source data is shuffled, you migh want to shuffle it more or else the same order will be repeated at each epoch and the model might end up biased. To shuffle some more, a common approach is to split the source data into multiple files, then read them in a random order during training. With this, instances located in the same file will still end up close to each other. To avoid this you can pick multiple files randomly and read them simutaneously, interleaving their records.\n",
    "Then on top of that we can add a shuffling buffer with ```shuffle```.\n",
    "\n",
    "The best part about this: the Data API makes it easy for you to do all this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interleaving lines from multiple files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by loading the California Housing dataset, shuffling it, split into a training and validation set and a test. Finally we split each set into many csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    "    housing.data, housing.target.reshape(-1, 1), random_state=42)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_train_full, y_train_full, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_mean = scaler.mean_\n",
    "X_std = scaler.scale_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def save_to_multiple_csv_files(data, name_prefix, header=None, n_parts=10):\n",
    "    housing_dir = os.path.join(\"datasets\", \"housing\")\n",
    "    os.makedirs(housing_dir, exist_ok=True)\n",
    "    path_format = os.path.join(housing_dir, \"my_{}_{:02d}.csv\")\n",
    "\n",
    "    filepaths = []\n",
    "    m = len(data)\n",
    "    for file_idx, row_indices in enumerate(np.array_split(np.arange(m), n_parts)):\n",
    "        part_csv = path_format.format(name_prefix, file_idx)\n",
    "        filepaths.append(part_csv)\n",
    "        with open(part_csv, \"wt\", encoding=\"utf-8\") as f:\n",
    "            if header is not None:\n",
    "                f.write(header)\n",
    "                f.write(\"\\n\")\n",
    "            for row_idx in row_indices:\n",
    "                f.write(\",\".join([repr(col) for col in data[row_idx]]))\n",
    "                f.write(\"\\n\")\n",
    "    return filepaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "train_data = np.c_[X_train, y_train]\n",
    "valid_data = np.c_[X_valid, y_valid]\n",
    "test_data = np.c_[X_test, y_test]\n",
    "header_cols = housing.feature_names + ['MedianHouseValue']\n",
    "header = \",\".join(header_cols)\n",
    "\n",
    "train_filepaths = save_to_multiple_csv_files(train_data, 'train', header, n_parts=20)\n",
    "valid_filepaths = save_to_multiple_csv_files(valid_data, 'valid', header, n_parts=10)\n",
    "test_filepaths = save_to_multiple_csv_files(test_data, 'test', header, n_parts=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MedInc</th>\n",
       "      <th>HouseAge</th>\n",
       "      <th>AveRooms</th>\n",
       "      <th>AveBedrms</th>\n",
       "      <th>Population</th>\n",
       "      <th>AveOccup</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>MedianHouseValue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.5214</td>\n",
       "      <td>15.0</td>\n",
       "      <td>3.049945</td>\n",
       "      <td>1.106548</td>\n",
       "      <td>1447.0</td>\n",
       "      <td>1.605993</td>\n",
       "      <td>37.63</td>\n",
       "      <td>-122.43</td>\n",
       "      <td>1.442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.3275</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.490060</td>\n",
       "      <td>0.991054</td>\n",
       "      <td>3464.0</td>\n",
       "      <td>3.443340</td>\n",
       "      <td>33.69</td>\n",
       "      <td>-117.39</td>\n",
       "      <td>1.687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.1000</td>\n",
       "      <td>29.0</td>\n",
       "      <td>7.542373</td>\n",
       "      <td>1.591525</td>\n",
       "      <td>1328.0</td>\n",
       "      <td>2.250847</td>\n",
       "      <td>38.44</td>\n",
       "      <td>-122.98</td>\n",
       "      <td>1.621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.1736</td>\n",
       "      <td>12.0</td>\n",
       "      <td>6.289003</td>\n",
       "      <td>0.997442</td>\n",
       "      <td>1054.0</td>\n",
       "      <td>2.695652</td>\n",
       "      <td>33.55</td>\n",
       "      <td>-117.70</td>\n",
       "      <td>2.621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0549</td>\n",
       "      <td>13.0</td>\n",
       "      <td>5.312457</td>\n",
       "      <td>1.085092</td>\n",
       "      <td>3297.0</td>\n",
       "      <td>2.244384</td>\n",
       "      <td>33.93</td>\n",
       "      <td>-116.93</td>\n",
       "      <td>0.956</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  \\\n",
       "0  3.5214      15.0  3.049945   1.106548      1447.0  1.605993     37.63   \n",
       "1  5.3275       5.0  6.490060   0.991054      3464.0  3.443340     33.69   \n",
       "2  3.1000      29.0  7.542373   1.591525      1328.0  2.250847     38.44   \n",
       "3  7.1736      12.0  6.289003   0.997442      1054.0  2.695652     33.55   \n",
       "4  2.0549      13.0  5.312457   1.085092      3297.0  2.244384     33.93   \n",
       "\n",
       "   Longitude  MedianHouseValue  \n",
       "0    -122.43             1.442  \n",
       "1    -117.39             1.687  \n",
       "2    -122.98             1.621  \n",
       "3    -117.70             2.621  \n",
       "4    -116.93             0.956  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.read_csv(train_filepaths[0]).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create a dataset using only these filepaths. By default, ```list_files``` retuns a dataset that shuffles the file paths. You can set shuffle=False if you don't want it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath_dataset = tf.data.Dataset.list_files(train_filepaths, seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we use the ```interleave``` method to read from five files at a time and interleave their lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_readers = 5\n",
    "dataset = filepath_dataset.interleave(\n",
    "    lambda filepath: tf.data.TextLineDataset(filepath).skip(1),\n",
    "    cycle_length=n_readers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This pulls five files at a time, skipping the first line (containing headers) then constructs a dataset by reading one line from each file. It then pulls the next five file paths, interleaves them in the same way and so on until it runs out of file paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'4.2083,44.0,5.323204419889502,0.9171270718232044,846.0,2.3370165745856353,37.47,-122.2,2.782'\n",
      "b'4.1812,52.0,5.701388888888889,0.9965277777777778,692.0,2.4027777777777777,33.73,-118.31,3.215'\n",
      "b'3.6875,44.0,4.524475524475524,0.993006993006993,457.0,3.195804195804196,34.04,-118.15,1.625'\n",
      "b'3.3456,37.0,4.514084507042254,0.9084507042253521,458.0,3.2253521126760565,36.67,-121.7,2.526'\n",
      "b'3.5214,15.0,3.0499445061043287,1.106548279689234,1447.0,1.6059933407325193,37.63,-122.43,1.442'\n"
     ]
    }
   ],
   "source": [
    "for line in dataset.take(5):\n",
    "    print(line.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the first 5 lines of the dataset, but they are byte strings, we now need to parse them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using X_mean, X_std from above...\n",
    "n_inputs = 8\n",
    "\n",
    "def preprocess(line):\n",
    "    defs = [0.] * n_inputs + [tf.constant([], dtype=tf.float32)]\n",
    "    fields = tf.io.decode_csv(line, record_defaults=defs)\n",
    "    x = tf.stack(fields[:-1])  # Convert features to 1D tensor array\n",
    "    y = tf.stack(fields[-1:])  # Convert target to 1D tensor array\n",
    "    return (x - X_mean) / X_std, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(8,), dtype=float32, numpy=\n",
      "array([ 0.36618188, -0.998705  ,  0.00781878, -0.00675364, -0.06140145,\n",
      "        0.0072037 , -0.94465536,  0.9367464 ], dtype=float32)>, <tf.Tensor: shape=(1,), dtype=float32, numpy=array([2.418], dtype=float32)>)\n"
     ]
    }
   ],
   "source": [
    "for line in dataset.take(1):\n",
    "    print(preprocess(line.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting everything together / Prefetching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_reader_dataset(filepaths, repeat=1, n_readers=5,\n",
    "                       n_read_threads=None, shuffle_buffer_size=10000,\n",
    "                       n_parse_threads=5, batch_size=32):\n",
    "    dataset = tf.data.Dataset.list_files(filepaths)\n",
    "    dataset = dataset.interleave(\n",
    "        lambda filepath: tf.data.TextLineDataset(filepath).skip(1),\n",
    "        cycle_length=n_readers, num_parallel_calls=n_read_threads)\n",
    "    dataset = dataset.map(preprocess, num_parallel_calls=n_read_threads)\n",
    "    dataset = dataset.shuffle(shuffle_buffer_size).repeat(repeat)\n",
    "    return dataset.batch(batch_size).prefetch(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last line in the function above uses the ```prefetch``` method. This creates a dataset that tries to be one batch ahead. For example if we are training a model on this dataset, while the training is happening, the dataset will already be working in parallel on getting the next batch ready. This can dramatically improve performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the Dataset with tf.keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the ```csv_reader_dataset``` function above to create a dataset for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = csv_reader_dataset(train_filepaths, repeat=None)\n",
    "valid_set = csv_reader_dataset(valid_filepaths)\n",
    "test_set = csv_reader_dataset(test_filepaths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then build a model. We use the steps per epoch to specify the number of training steps as the dataset has not been loaded yet, so it is deemed 'infinite'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "362/362 [==============================] - 1s 2ms/step - loss: 1.2432 - val_loss: 0.4941\n",
      "Epoch 2/10\n",
      "362/362 [==============================] - 1s 2ms/step - loss: 0.4980 - val_loss: 0.4342\n",
      "Epoch 3/10\n",
      "362/362 [==============================] - 1s 3ms/step - loss: 0.4494 - val_loss: 0.4017\n",
      "Epoch 4/10\n",
      "362/362 [==============================] - 1s 3ms/step - loss: 0.4409 - val_loss: 0.3974\n",
      "Epoch 5/10\n",
      "362/362 [==============================] - 1s 4ms/step - loss: 0.4292 - val_loss: 0.3897\n",
      "Epoch 6/10\n",
      "362/362 [==============================] - 1s 3ms/step - loss: 0.4101 - val_loss: 0.3807\n",
      "Epoch 7/10\n",
      "362/362 [==============================] - 1s 3ms/step - loss: 0.4076 - val_loss: 0.3852\n",
      "Epoch 8/10\n",
      "362/362 [==============================] - 1s 3ms/step - loss: 0.4091 - val_loss: 0.3723\n",
      "Epoch 9/10\n",
      "362/362 [==============================] - 1s 3ms/step - loss: 0.3845 - val_loss: 0.4028\n",
      "Epoch 10/10\n",
      "362/362 [==============================] - 1s 4ms/step - loss: 0.3808 - val_loss: 0.4506\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f777599be20>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(100, activation='elu', kernel_initializer='he_normal', \n",
    "                       input_shape=X_train.shape[1:]),\n",
    "    keras.layers.Dense(1, activation = 'relu')\n",
    "])\n",
    "model.compile(optimizer='nadam', loss='mse')\n",
    "batch_size=32\n",
    "model.fit(train_set, epochs=10, validation_data=valid_set, steps_per_epoch=len(X_train)//batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "161/161 [==============================] - 0s 724us/step - loss: 0.3754\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.37539780139923096"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_set, steps=len(X_test)//batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.9838567 ],\n",
       "       [2.162112  ],\n",
       "       [0.9305432 ],\n",
       "       [2.865344  ],\n",
       "       [0.9184288 ],\n",
       "       [1.7495688 ],\n",
       "       [1.753883  ],\n",
       "       [1.5726929 ],\n",
       "       [3.2011614 ],\n",
       "       [1.3680485 ],\n",
       "       [3.3565168 ],\n",
       "       [1.5491389 ],\n",
       "       [2.347571  ],\n",
       "       [2.6110601 ],\n",
       "       [1.9164817 ],\n",
       "       [2.6606073 ],\n",
       "       [0.8301281 ],\n",
       "       [0.48761353],\n",
       "       [2.5339346 ],\n",
       "       [1.9821576 ],\n",
       "       [1.1782722 ],\n",
       "       [2.9220848 ],\n",
       "       [2.7169647 ],\n",
       "       [3.6323676 ],\n",
       "       [2.7950797 ],\n",
       "       [3.7449813 ],\n",
       "       [3.7057714 ],\n",
       "       [1.3309131 ],\n",
       "       [2.0386727 ],\n",
       "       [0.88035214],\n",
       "       [1.4802783 ],\n",
       "       [3.1646976 ],\n",
       "       [0.6698898 ],\n",
       "       [1.2645179 ],\n",
       "       [2.0718355 ],\n",
       "       [1.8404658 ],\n",
       "       [1.6014103 ],\n",
       "       [1.9512341 ],\n",
       "       [2.2154174 ],\n",
       "       [1.3453177 ],\n",
       "       [0.48185408],\n",
       "       [2.3247495 ],\n",
       "       [0.9696155 ],\n",
       "       [1.926871  ],\n",
       "       [1.4848202 ],\n",
       "       [2.8618097 ],\n",
       "       [2.9666567 ],\n",
       "       [3.292745  ],\n",
       "       [2.328271  ],\n",
       "       [1.8333491 ],\n",
       "       [3.0680962 ],\n",
       "       [3.0937133 ],\n",
       "       [1.1360495 ],\n",
       "       [1.8845817 ],\n",
       "       [3.5490012 ],\n",
       "       [1.8706812 ],\n",
       "       [2.5031567 ],\n",
       "       [1.425507  ],\n",
       "       [3.7579622 ],\n",
       "       [2.8899956 ],\n",
       "       [5.8863335 ],\n",
       "       [0.73520124],\n",
       "       [2.7705731 ],\n",
       "       [1.9544796 ],\n",
       "       [1.1312921 ],\n",
       "       [3.0877342 ],\n",
       "       [1.3941703 ],\n",
       "       [0.97826207],\n",
       "       [2.629797  ],\n",
       "       [3.4424691 ],\n",
       "       [1.1652842 ],\n",
       "       [1.8751901 ],\n",
       "       [2.1553965 ],\n",
       "       [1.3619283 ],\n",
       "       [1.4961007 ],\n",
       "       [4.0103426 ],\n",
       "       [1.469119  ],\n",
       "       [2.39119   ],\n",
       "       [2.8227549 ],\n",
       "       [3.017169  ],\n",
       "       [2.9798212 ],\n",
       "       [0.70321304],\n",
       "       [1.3851694 ],\n",
       "       [1.9628501 ],\n",
       "       [1.970571  ],\n",
       "       [2.4320002 ],\n",
       "       [3.2049417 ],\n",
       "       [0.93751323],\n",
       "       [1.25754   ],\n",
       "       [4.5153666 ],\n",
       "       [2.1581388 ],\n",
       "       [0.76056415],\n",
       "       [2.3803687 ],\n",
       "       [3.5090122 ],\n",
       "       [1.1907126 ],\n",
       "       [2.5014868 ]], dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_set = test_set.take(3).map(lambda X, y: X)\n",
    "model.predict(new_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The TFRecord Format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TFRecord is TensorFlow's preferred format for storing large amounts of data and reading it efficiently. We can use ```tf.io.TFRecordWriter``` for creating these objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.io.TFRecordWriter('my_data.tfrecord') as f:\n",
    "    f.write(b'This is the first record')\n",
    "    f.write(b'And this is the second record')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then we use ```tf.data.TFRecordDataset``` to red one or more TFRecord files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'This is the first record', shape=(), dtype=string)\n",
      "tf.Tensor(b'And this is the second record', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "filepaths = ['my_data.tfrecord']\n",
    "dataset = tf.data.TFRecordDataset(filepaths)\n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also set ```num_parallel_reads```, and the ```interleave``` methods as we did earlier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TFRecords can be compressed, which can be useful especially if they need to be loaded via a network connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = tf.io.TFRecordOptions(compression_type='GZIP')\n",
    "with tf.io.TFRecordWriter('my_compressed.tfrecord', options) as f:\n",
    "        f.write(b'This is the first compressed record')\n",
    "        f.write(b'And this is the second compressed record')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When reading a compressed file we need to specify the compression type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.TFRecordDataset(['my_compressed.tfrecord'],\n",
    "                                  compression_type='GZIP')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A brief introduction to protocol buffers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Protocol buffers (*protobufs*) is a portable, extensible and efficient binary format developed at Google. They are defined like so"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting person.proto\n"
     ]
    }
   ],
   "source": [
    "%%writefile person.proto\n",
    "syntax = \"proto3\";\n",
    "message Person {\n",
    "    string name = 1;\n",
    "    int32 id = 2;\n",
    "    repeated string email = 3;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each Person object may (optionally) have a name of type string, an id of type int32 and zero or more email fields, each of type string. The numbers 1, 2 and 3 are the field identifiers.\n",
    "\n",
    "Once our definition is ready in a *.proto* file, we can compile it using **protoc**, the protobuf compiler. See book pg 426 for example and explanations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorFlow Protobufs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main protobuf typically used in a TFRecord file is the Example protobuf, which represents one instance in a dataset. It contains a list of named features, where each feature can either be a list of byte strings, a list of floats or a list of integers. Here is the definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting example.proto\n"
     ]
    }
   ],
   "source": [
    "%%writefile example.proto\n",
    "syntax = \"proto3\";\n",
    "message BytesList { repeated bytes value = 1; }\n",
    "message FloatList { repeated float value = 1 [packed = true]; }\n",
    "message Int64List { repeated int64 value = 1 [packed = true]; }\n",
    "message Feature{\n",
    "    oneof kind {\n",
    "        BytesList bytes_list = 1;\n",
    "        FloatList float_list = 2;\n",
    "        Int64List int64_list = 3;\n",
    "    }\n",
    "};\n",
    "message Features { map<string, Feature> feature = 1};\n",
    "message Example { Features features = 1; };"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```[packed = true]``` is used for repeated numerical fields for a more efficient encoding. A ```Feature``` contains either a BytesList, a FloatList or an Int64List. a ```Features``` contains a dictionary that maps a feature name to the corresponding feature value. Finally, an ```Example``` contains a Features object.\n",
    "\n",
    "Here's how you could write a tf.train.Example representing the same person and write it to a TFRecord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.train import BytesList, FloatList, Int64List\n",
    "from tensorflow.train import Feature, Features, Example\n",
    "\n",
    "person_example = Example(\n",
    "    features = Features(\n",
    "        feature = {\n",
    "            \"name\": Feature(bytes_list=BytesList(value=[b\"Alice\"])),\n",
    "            \"id\": Feature(int64_list=Int64List(value=[123])),\n",
    "            \"emails\": Feature(bytes_list=BytesList(value=[b\"a@b.com\",\n",
    "                                                          b\"c@d.com\"])),\n",
    "        }))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You could also wrap this code inside a small helper function. Now that we have an example protobuf, we can serialize it by calling its ```SerializeToString()``` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.io.TFRecordWriter(\"my_contacts.tfrecord\") as f:\n",
    "    f.write(person_example.SerializeToString())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typically we would create a conversion script that reads from a current format (e.g. CSV), creates an example protobuf for each instance, serializes them, and saves them to several TFRecord files, ideally shuffling them in the process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and parsing examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To load the serialized Example protobuf we will use a tf.data.TFRecordDataset again, parsing each example using ```tf.io.parse_single_example()```, as it is a TF operation it can be included in a TF function. \n",
    "\n",
    "The second argument to the function is a dictionary that maps each feature name to either a ```tf.io.FixedLenFeature``` descriptor, indicating the feature's shape, type and default value. or a ```tf.io.VarLenFeature``` descriptor indicating only the type. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_description = {\n",
    "    \"name\": tf.io.FixedLenFeature([], tf.string, default_value=\"\"),\n",
    "    \"id\": tf.io.FixedLenFeature([], tf.int64, default_value=0),\n",
    "    \"email\": tf.io.VarLenFeature(tf.string),\n",
    "}\n",
    "for serialized_example in tf.data.TFRecordDataset([\"my_contacts.tfrecord\"]):\n",
    "    parsed_example = tf.io.parse_single_example(serialized_example,\n",
    "                                                feature_description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fixed lenght features are parsed as tensors, while the variable length feature is parsed as a sparse tensor. \n",
    "\n",
    "We can convert sparse tensors with ```tf.sparse.to_dense()``` but in this case it is simpler to access it's values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(0,), dtype=string, numpy=array([], dtype=object)>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.sparse.to_dense(parsed_example['email'], default_value=b\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(0,), dtype=string, numpy=array([], dtype=object)>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed_example['email'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could have also parsed the examples in batch using ```tf.io.parse_example()```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.TFRecordDataset(['my_contacts.tfrecord']).batch(10)\n",
    "for serialized_examples in dataset:\n",
    "    parsed_examples = tf.io.parse_example(serialized_examples, feature_description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Example protobuf will probably be sufficient for most cases. It might be a bit cumbersome when dealing with lists of lists. For this case we can use ```SequenceExample```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Lists of Lists using the SequenceExample Protobuf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the definition of the SequenceExample protobuf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting sequence_example.protobuf\n"
     ]
    }
   ],
   "source": [
    "%%writefile sequence_example.protobuf\n",
    "message FeatureList { repeated Feature feature = 1; };\n",
    "message FeatureLists { map<string, FeatureList> feature_list = 1; };\n",
    "message SequenceExample {\n",
    "    Features context = 1;\n",
    "    FeatureLists feature_lists = 2;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A SequenceExample has a Features object for the contextual data and a FeatureLists object that contains one or more named FeatureLists object. Each FeatureList contains a list of Feature objects, each of which may be a list of byte strings, a list of 64-bit integers or a list of floats.\n",
    "\n",
    "To parse it, we use ```tf.io.parse_single_sequence_example``` or ```tf.io.parse_sequence_example``` to parse a batch. If the feature lists contain sequences of varyin sizes you may want to convert them to ragged tensors using ```tf.RaggedTensor.from_sparse()```. See notebook for full code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the Input Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section looks at a including a preprocessing layer in a model, to handle data type conversion, one-hot-encoding, etc...\n",
    "\n",
    "Here's an example of how to implement a standardization layer using a Lambda layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "means = np.mean(X_train, axis=0, keepdims=True)\n",
    "stds = np.std(X_train, axis=0, keepdims=True)\n",
    "eps = keras.backend.epsilon()\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Lambda(lambda inputs: (inputs - means)/ (stds + eps))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make it neater, we can sublclass the Layer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Standardization(keras.layers.Layer):\n",
    "    def adapt(self, data_sample):\n",
    "        self.means_ = np.mean(data_sample, axis=0, keepdims=True)\n",
    "        self.std_ = np.std(data_sample, axis=0, keepdims=True)\n",
    "    def call(self, inputs):\n",
    "        return (inputs - self.mean_) / (self.stds_ + keras.backend.epsilon())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before using it, we need to call the adapt method passing a data sample, allowing it to use the appropriate mean and standard deviation for each feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ellipsis"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "std_layer = Standardization()\n",
    "std_layer.adapt(X_train)\n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(std_layer)\n",
    "... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding categorical features using One-Hot Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall the California housing dataset has the ocean proximity feature with five possible values. We need to encode this feature before we feed it to a neural network. We can do this using a lookup table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = [\"<1H OCEAN\", \"INLAND\", \"NEAR OCEAN\", \"NEAR BAY\", \"ISLAND\"]\n",
    "indices = tf.range(len(vocab), dtype=tf.int64)\n",
    "table_init = tf.lookup.KeyValueTensorInitializer(vocab, indices)\n",
    "num_oov_buckets = 2\n",
    "table = tf.lookup.StaticVocabularyTable(table_init, num_oov_buckets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of *out of vocabulary (oov)* buckets is the number of extra values added if it doesn't exist in our vocabulary. In this case, when a new value is found, the table  computes its hash and adds them to categories 5 and 6.\n",
    "\n",
    "Why use oov buckets? If the number of categories is large (zip codes, cities, words, ...) or the dataset keeps changing, getting the full list of categories might not be convenient. One solution is to base the vocabulary on a sample of the data, using oov buckets for the unknowns. If there are not enough oov buckets, ther will be collisions with different categories ending up in the same bucket so the neural net is not able to distinguish them.\n",
    "\n",
    "Let's put this into practice to one-hot encode some samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4,), dtype=int64, numpy=array([3, 5, 1, 1])>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories = tf.constant([\"NEAR BAY\", \"DESERT\", \"INLAND\", \"INLAND\"])\n",
    "cat_indices = table.lookup(categories)\n",
    "cat_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 7), dtype=float32, numpy=\n",
       "array([[0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_one_hot = tf.one_hot(cat_indices, depth=len(vocab) + num_oov_buckets)\n",
    "cat_one_hot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the DESERT category was added to bucket 5 and how we had to specify the number of indices with the depth parameter.\n",
    "\n",
    "Keras has the ```keras.layers.TextVectorization``` layer to create the lookup table with distinct values for you. You can add it to your model followed by the tf.one_hot() function if you want to convert it to one-hot vectors.\n",
    "\n",
    "This might not be the best solution; the size of each one-hot vector is the vocabulary lenght plus the number of oov buckets. If the vocabulary is large, it is more efficient to use *embeddings* instead.\n",
    "\n",
    "*Tip*: As a rule of thumb, if the number of categories is less than 10, one-hot encoding is the way to go. If it is greater than 50, embeddings are better. In between you can experiment with both and see which works best."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding Categorical Features using Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.14572394467959282, 0.8999561513490448)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(np.random.random(), np.random.random())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An embedding is a trainable dense vector that represents a category. By default they're initialized randomly, for example the category \"NEAR BAY\" could be initialized as [0.1339, 0.6078]. This example shows a 2D embedding, but the number of dimensions is a hyperparameter you can tweak. \n",
    "\n",
    "As the embeddings are trainable, they will improve over training and Gradient Descent will push similar categories together, for example INLAND will end up being far from all the other categories above (NEAR OCEAN, <1H OCEAN, etc...)\n",
    "\n",
    "[Word2Vec Reference](https://arxiv.org/abs/1310.4546)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll implement an embedding manually to understand how they work. Start with an *embedding matrix* containing each category's embedding, initialized randomly. The matrix contains one row per category and oov bucket and one column per embedding dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 2\n",
    "embed_init = tf.random.uniform([len(vocab) + num_oov_buckets, embedding_dim])\n",
    "embedding_matrix = tf.Variable(embed_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable:0' shape=(7, 2) dtype=float32, numpy=\n",
       "array([[0.01089716, 0.93421566],\n",
       "       [0.39437556, 0.2496841 ],\n",
       "       [0.77807224, 0.40052128],\n",
       "       [0.8927587 , 0.35086238],\n",
       "       [0.506053  , 0.8094338 ],\n",
       "       [0.387424  , 0.14231634],\n",
       "       [0.39908934, 0.49101698]], dtype=float32)>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll encode the same batch of features but use embeddings instead of one_hot_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4,), dtype=string, numpy=array([b'NEAR BAY', b'DESERT', b'INLAND', b'INLAND'], dtype=object)>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4,), dtype=int64, numpy=array([3, 5, 1, 1])>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_indices = table.lookup(categories)\n",
    "cat_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 2), dtype=float32, numpy=\n",
       "array([[0.8927587 , 0.35086238],\n",
       "       [0.387424  , 0.14231634],\n",
       "       [0.39437556, 0.2496841 ],\n",
       "       [0.39437556, 0.2496841 ]], dtype=float32)>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.nn.embedding_lookup(embedding_matrix, cat_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```tf.nn.embedding_lookup``` looks up rows in the embedding matrix, at the given indices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With keras, we can use ```keras.layers.Embedding``` to handle the embedding matrix. When created, it initialized the matrix randomly and when it is called with category indices it returns the corresponding rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 2), dtype=float32, numpy=\n",
       "array([[-0.03869525, -0.02267827],\n",
       "       [ 0.00806278,  0.0328465 ],\n",
       "       [ 0.00392421,  0.03095348],\n",
       "       [ 0.00392421,  0.03095348]], dtype=float32)>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding = keras.layers.Embedding(input_dim=len(vocab) + num_oov_buckets, output_dim=embedding_dim)\n",
    "embedding(cat_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can put everything together and create a Keras model to process categorical features (along with numerical ones) and learn an embedding for each category (and oov bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ellipsis"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regular_inputs = keras.layers.Input(shape=[8])\n",
    "categories = keras.layers.Input(shape=[], dtype=tf.string)\n",
    "\n",
    "cat_indices = keras.layers.Lambda(lambda cats: table.lookup(cats))(categories)\n",
    "cat_embed = keras.layers.Embedding(input_dim=6, output_dim=2)(cat_indices)\n",
    "encoded_inputs = keras.layers.concatenate([regular_inputs, cat_embed])\n",
    "\n",
    "output = keras.layers.Dense(1)(encoded_inputs)\n",
    "\n",
    "model = keras.models.Model(inputs=[regular_inputs, categories], outputs=[output])\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras Preprocessing Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already discussed two examples ```keras.layers.Normalization``` and ```keras.layers.TextVectorization```, which perform feature standardization and word encoding respectively. In both cases the pattern is the same: create the layer call it's ```adapt()``` method with a data sample and then use the layer in the model. Other preprocessing layers follow the same pattern."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ```keras.layers.Discretization``` layer chops continuous data into different bins and encodes each bin as an one hot vector. For example we can discretize prices in three categories (low, medium, high) with encodings [1, 0, 0], [0, 1, 0], [0, 0, 1]. This compresses a lot of information but in some cases it can help the model detect patterns that would not be obvious when looking at the continuous values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Warning:** The preprocessing layers are frozen during training, so their parameters are not affected by Gradient Descent, hence they do not need to be differentiable. This also means you should not use an ```Embedding``` layer in a custom pre-processing layer as their weights will not be trainable. Instead, add it separately to the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also chain preprocessing steps using the ```PreprocessingStage``` class. For example, you can create a pipeline that normalizes the inputs then discretizes them. After adapting this layer to your data sample, you can use it as a regular layer (but again, only at the start since it contains a nondifferentiable preprocessing layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: at the time of writing, the discretization and preprocessingStage layers are still experimental. I'm omitting this snippet for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalization = keras.layers.LayerNormalization()\n",
    "# discretization = keras.layers.Discretization([...])\n",
    "# pipelin = keras.layers.PreprocessingStage([normalization, ...])\n",
    "# pipeline.adapt(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See note about ```TextVectorization``` bag of words and TF-IDF on page 438"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF Transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing can be computationally expensive and in such cases, handling data before training rather than on the fly can give a significant speedup. If your dataset is small enough to fit in memory you can use it's cache method. But if it is larger Apache Beam or Spark can help. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is fine for training but what if once the model is trained you want to deploy it to a mobile app. You'll need to write some code in the app to take care of the preprocessing before it is fed to the model. If you also want to deploy to TensorFlow.js so that it runs on a browser you'll have more processing code and a maintenance nightmare. If the preprocessing changes you'll have to update the logic in all of these platforms which is time consuming and error prone. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One solution is to take the trained model and before deploying it on your app, add extra preprocessing layers to take care of preprocessing on the fly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF Transform was designed so that you only have to define your preprocessing once. It is part of the TensorFlow Extended package, an end-to-end platform for productionizing TF models, and does not come bundled with tensorflow so we have to install it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: getting error on the installation below, might be due to running under WSL?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow-transform\n",
      "  Using cached tensorflow_transform-0.22.0-py3-none-any.whl (326 kB)\n",
      "Requirement already satisfied: tensorflow!=2.0.*,<2.3,>=1.15 in ./lib/python3.8/site-packages (from tensorflow-transform) (2.2.0)\n",
      "Collecting apache-beam[gcp]<3,>=2.20\n",
      "  Using cached apache_beam-2.23.0-cp38-cp38-manylinux2010_x86_64.whl (9.8 MB)\n",
      "Requirement already satisfied: six<2,>=1.12 in ./lib/python3.8/site-packages (from tensorflow-transform) (1.14.0)\n",
      "Requirement already satisfied: protobuf<4,>=3.7 in ./lib/python3.8/site-packages (from tensorflow-transform) (3.11.3)\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement tfx-bsl<0.23,>=0.22 (from tensorflow-transform) (from versions: none)\u001b[0m\n",
      "\u001b[31mERROR: No matching distribution found for tfx-bsl<0.23,>=0.22 (from tensorflow-transform)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install tensorflow-transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF we can then define the function just once in Python and using the respective TF Transform functions for scaling, bucketizing, etc..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow_transform as tft\n",
    "\n",
    "# def preprocess(inputs):\n",
    "#     median_age = inputs['hosing_median_age']\n",
    "#     ocean_proximity = inputs['ocean_proximity']\n",
    "#     standardized_age = tft.scale_to_z_score(median_age)\n",
    "#     ocean_proximity_id =  tft.compute_and_apply_vocabulary(ocean_proximity)\n",
    "#     return {\n",
    "#         \"standardized_median_age\": standardized_age,\n",
    "#         \"ocean_proximity_id\": ocean_proximity_id\n",
    "#     }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read more information on book.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow Datasets (TFDS) Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The TFDS projects makes it easy to download common datasets, small ones like MNIST or Fashion MNIST to huge ones like ImageNet. [Link to full list of datasets](https://homl.info/tfds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow-datasets in ./lib/python3.8/site-packages (3.2.1)\n",
      "Requirement already satisfied: future in ./lib/python3.8/site-packages (from tensorflow-datasets) (0.18.2)\n",
      "Requirement already satisfied: six in ./lib/python3.8/site-packages (from tensorflow-datasets) (1.14.0)\n",
      "Requirement already satisfied: tensorflow-metadata in ./lib/python3.8/site-packages (from tensorflow-datasets) (0.22.2)\n",
      "Requirement already satisfied: attrs>=18.1.0 in ./lib/python3.8/site-packages (from tensorflow-datasets) (19.3.0)\n",
      "Requirement already satisfied: protobuf>=3.6.1 in ./lib/python3.8/site-packages (from tensorflow-datasets) (3.11.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in ./lib/python3.8/site-packages (from tensorflow-datasets) (2.23.0)\n",
      "Requirement already satisfied: tqdm in ./lib/python3.8/site-packages (from tensorflow-datasets) (4.48.2)\n",
      "Requirement already satisfied: dill in ./lib/python3.8/site-packages (from tensorflow-datasets) (0.3.2)\n",
      "Requirement already satisfied: wrapt in ./lib/python3.8/site-packages (from tensorflow-datasets) (1.12.1)\n",
      "Requirement already satisfied: absl-py in ./lib/python3.8/site-packages (from tensorflow-datasets) (0.9.0)\n",
      "Requirement already satisfied: numpy in ./lib/python3.8/site-packages (from tensorflow-datasets) (1.18.4)\n",
      "Requirement already satisfied: termcolor in ./lib/python3.8/site-packages (from tensorflow-datasets) (1.1.0)\n",
      "Requirement already satisfied: promise in ./lib/python3.8/site-packages (from tensorflow-datasets) (2.3)\n",
      "Requirement already satisfied: googleapis-common-protos in ./lib/python3.8/site-packages (from tensorflow-metadata->tensorflow-datasets) (1.52.0)\n",
      "Requirement already satisfied: setuptools in ./lib/python3.8/site-packages (from protobuf>=3.6.1->tensorflow-datasets) (46.1.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in ./lib/python3.8/site-packages (from requests>=2.19.0->tensorflow-datasets) (2.9)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in ./lib/python3.8/site-packages (from requests>=2.19.0->tensorflow-datasets) (1.25.9)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in ./lib/python3.8/site-packages (from requests>=2.19.0->tensorflow-datasets) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./lib/python3.8/site-packages (from requests>=2.19.0->tensorflow-datasets) (2020.4.5.1)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install tensorflow-datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset mnist/3.0.1 (download: 11.06 MiB, generated: 21.00 MiB, total: 32.06 MiB) to /home/carlos/tensorflow_datasets/mnist/3.0.1...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Dataset mnist is hosted on GCS. It will automatically be downloaded to your\n",
      "local data directory. If you'd instead prefer to read directly from our public\n",
      "GCS bucket (recommended if you're running on GCP), you can instead pass\n",
      "`try_gcs=True` to `tfds.load` or set `data_dir=gs://tfds-data/datasets`.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e35485a4c14d4ad1a0049515cd367bd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Dl Completed...', max=4.0, style=ProgressStyle(descriptio…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1mDataset mnist downloaded and prepared to /home/carlos/tensorflow_datasets/mnist/3.0.1. Subsequent calls will reuse this data.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "dataset = tfds.load(name='mnist')\n",
    "mnist_train, mnist_test = dataset['train'], dataset['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then transform it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset shapes: {image: (28, 28, 1), label: ()}, types: {image: tf.uint8, label: tf.int64}>"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train = mnist_train.shuffle(10000).batch(32).prefetch(1)\n",
    "for item in mnist_train:\n",
    "    images = item['image']\n",
    "    labels = item['label']\n",
    "    ... # do things"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each item in the dataset is a dictionary containing features and labels, but Keras expects each item to be a tuple containing two elements (features and labels). Let's transform the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train = mnist_train.shuffle(10000).batch(32)\n",
    "mnist_train = mnist_train.map(lambda items: (items[\"image\"], items[\"label\"]))\n",
    "mnist_train = mnist_train.prefetch(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can conveniently do this by passing ```as_supervised=True``` to the load function. You can also specify the batch size and shuffle the datset with ```shuffle_files=True```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 316.2357 - accuracy: 0.8418\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 2s 936us/step - loss: 260.2413 - accuracy: 0.8686\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 2s 956us/step - loss: 244.4567 - accuracy: 0.8735\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 2s 968us/step - loss: 245.1133 - accuracy: 0.8751\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 2s 922us/step - loss: 239.7527 - accuracy: 0.8771\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f7767f9c5e0>"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = tfds.load(name='mnist', batch_size=32, as_supervised=True)\n",
    "mnist_train = dataset['train'].prefetch(1)\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"sgd\", metrics=['accuracy'])\n",
    "model.fit(mnist_train, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
